\part{Built-In Functions}\label{built-in-functions.part}


\chapter{Void Functions}


Stan does not technically support functions that do not return values. It does support two types of statements that look like functions, one for printing and one for rejecting outputs.  Documentation on these functions is included here for completeness.  The special keyword \code{void} is used for the return type of void functions, because they behave like variadic functions with void return type, even though they are special kinds of statements.


Although \code{print} and \code{reject} appear to have the syntax of functions, they are actually special kinds of statements with slightly different form and behavior than other functions.  First, they are the constructs that allow a variable number of arguments.  Second, they are the the only constructs to accept string literals (e.g., \code{"hello world"}) as arguments.  Third, they have no effect on the log density function and operate solely through side effects.


\section{Print Statement}


Printing has no effect on the model's log probability function.  Its sole purpose is the side effect (i.e., an effect not represented in a return value) of arguments being printed to whatever the standard output stream is connected to (e.g., the terminal in command-line Stan or the R console in RStan).


\begin{description}   \fitem{void}{print}{T1 \farg{x1},..., TN \farg{xN}}{Print the values     denoted by the arguments \farg{x1} through \farg{xN} on the     output message stream.  There are no spaces between items in the     print, but a line feed (LF; Unicode U+000A;  C++ literal     \code{'\{n}'}) is inserted at the end of the printed     line.  The types \code{T1} through \code{TN} can be any of Stan's     built-in numerical types or double quoted strings of ASCII     characters.} \end{description}


\section{Reject Statement}


The reject statement has the same syntax as the print statement, accepting an arbitrary number of arguments of any type (including string literals).   The effect of executing a reject statement is to throw an exception internally that terminates the current iteration with a rejection (the behavior of which will depend on the algorithmic context in which it occurs).


\begin{description}   \fitem{void}{reject}{T1 \farg{x1},..., TN \farg{xN}}{Reject the     current iteration and print the values denoted by the arguments     \farg{x1} through \farg{xN} on the output message stream.  There     are no spaces between items in the print, but a line feed (LF;     Unicode U+000A;  C++ literal \code{'\{n}'}) is     inserted at the end of the printed line.  The types \code{T1}     through \code{TN} can be any of Stan's built-in numerical types or     double quoted strings of ASCII characters.} \end{description}


\chapter{Integer-Valued Basic Functions}


This chapter describes Stan's built-in function that take various types of arguments and return results of type integer.


\section{Integer-Valued Arithmetic Operators}\label{int-arithmetic.section}


Stan's arithmetic is based on standard double-precision  C++ integer and floating-point arithmetic.  If the arguments to an arithmetic operator are both integers, as in \code{2 + 2}, integer arithmetic is used.  If one argument is an integer and the other a floating-point value, as in \code{2.0 + 2} and \code{2 + 2.0}, then the integer is promoted to a floating point value and floating-point arithmetic is used.


Integer arithmetic behaves slightly differently than floating point arithmetic.  The first difference is how overflow is treated.  If the sum or product of two integers overflows the maximum integer representable, the result is an undesirable wraparound behavior at the bit level.  If the integers were first promoted to real numbers, they would not overflow a floating-point representation.  There are no extra checks in Stan to flag overflows, so it is up to the user to make sure it does not occur.


Secondly, because the set of integers is not closed under division and there is no special infinite value for integers, integer division implicitly rounds the result.  If both arguments are positive, the result is rounded down.  For example, \code{1 / 2} evaluates to 0 and \code{5 / 3} evaluates to 1.


If one of the integer arguments to division is negative, the latest  C++ specification ( C++11), requires rounding toward zero.  This would have \code{1 / 2} and \code{-1 / 2} evaluate to 0, \code{-7 / 2} evaluate to -3, and \code{7 / 2} evaluate to 3.  Before the  C++11 specification, the behavior was platform dependent, allowing rounding up or down.  All compilers recent enough to be able to deal with Stan's templating should follow the  C++11 specification, but it may be worth testing if you are not sure and plan to use integer division with negative values.


Unlike floating point division, where \code{1.0 / 0.0} produces the special positive infinite value, integer division by zero, as in \code{1 / 0}, has undefined behavior in the  C++ standard.  For example, the clang++ compiler on Mac OS X returns 3764, whereas the g++ compiler throws an exception and aborts the program with a warning.  As with overflow, it is up to the user to make sure integer divide-by-zero does not occur.


\subsection{Binary Infix Operators}


Operators are described using the  C++ syntax.  For instance, the binary operator of addition, written \code{X + Y}, would have the Stan signature \code{int operator+(int,int)} indicating it takes two real arguments and returns a real value.  As noted previously, the value of integer division is platform-dependent when rounding is platform dependent before C++11;  the descriptions below provide the C++11 definition.


\begin{description} \fitem{int}{operator+}{int \farg{x}, int \farg{y}}{ The sum of the addends \farg{x} and \farg{y} \[ \text{operator+}(x,y) = (x + y) \] } \fitem{int}{operator-}{int \farg{x}, int \farg{y}}{ The difference between the minuend \farg{x} and subtrahend \farg{y} \[ \text{operator-}(x,y) = (x - y) \] } \fitem{int}{operator*}{int \farg{x}, int \farg{y}}{ The product of the factors \farg{x} and \farg{y} \[ \text{operator*}(x,y) = (x \times y) \] } \fitem{int}{operator/}{int \farg{x}, int \farg{y}}{ The integer quotient of the dividend \farg{x} and divisor \farg{y} \[ \text{operator/}(x,y) = \begin{cases} \lfloor x / y \rfloor & \text{if } x / y \geq 0 \\ - \lfloor \text{floor}(-x / y) \rfloor & \text{if } x / y < 0. \end{cases} \] } \fitem{int}{operator%}{int \farg{x}, int \farg{y}}{\farg{x} modulo   \farg{y}, which is the positive remainder after dividing \farg{x} by   \farg{y}.  If both \farg{x} and \farg{y} are non-negative, so is the   result; otherwise, the sign of the result is platform dependent. \[ \mathrm{operator\%}(x, y) \ = \ x \ \text{mod} \ y \ = \ x - y * \lfloor x / y \rfloor \] } \end{description}


\subsection{Unary Prefix Operators}


\begin{description} \fitem{int}{operator-}{int \farg{x}}{ The negation of the subtrahend \farg{x} \[ \text{operator-}(x) = -x } \] \fitem{int}{operator+}{int \farg{x}}{ This is a no-op. \[ \text{operator+}(x) = x \] } \end{description}


\section{Absolute Functions}


\begin{description}   \fitemUnaryVec{abs}{absolute value of \farg{x}}{   \[\text{abs}(x) = |x|\]} \fitemnobody{int}{int_step}{int \farg{x}} \fitem{int}{int_step}{real \farg{x}}{Return the step function of \farg{x} as an integer,   \[   \mathrm{int\_step}(x) =   \begin{cases}     1 & \text{if } x > 0 \\     0 & \text{if } x \leq 0 \text{ or } x \text{ is } NaN   \end{cases}   \]  _**Warning:**_  \code{int_step(0)} and \code{int_step(NaN)} return 0 whereas \code{step(0)} and \code{step(NaN)} return 1.} \end{description}


See the warning in section \@ref(step-functions) about the dangers of step functions applied to anything other than data.


\section{Bound Functions}


\begin{description}   \fitem{int}{min}{int \farg{x}, int \farg{y}}{     Return the minimum of \farg{x} and \farg{y}.     \[     \text{min}(x, y) =     \begin{cases}       x & \text{if } x < y\\       y & \text{otherwise}     \end{cases}     \]}   \fitem{int}{max}{int \farg{x}, int \farg{y}}{     Return the maximum of \farg{x} and \farg{y}.     \[     \text{max}(x, y) =     \begin{cases}       x & \text{if } x > y\\       y & \text{otherwise}     \end{cases}     \]} \end{description}


\chapter{Real-Valued Basic Functions}


This chapter describes built-in functions that take zero or more real or integer arguments and return real values.


\section{Vectorization of Real-Valued Functions}\label{fun-vectorization.section}


Although listed in this chapter, many of Stan's built-in functions are vectorized so that they may be applied to any argument type.  The vectorized form of these functions is not any faster than writing an explicit loop that iterates over the elements applying the function---it's just easier to read and write and less error prone.


\subsection{Unary Function Vectorization}


Many of Stan's unary functions can be applied to any argument type. For example, the exponential function, \code{exp}, can be applied to \code{real} arguments or arrays of \code{real} arguments.  Other than for integer arguments, the result type is the same as the argument type, including dimensionality and size.  Integer arguments are first promoted to real values, but the result will still have the same dimensionality and size as the argument.


\subsubsection{Real and real array arguments}


When applied to a simple real value, the result is a real value.  When applied to arrays, vectorized functions like \code{exp()} are defined elementwise.  For example,


```\n // declare some variables for arguments\n real x0;\n real x1[5];\n real x2[4, 7];\n ...\n // declare some variables for results\n real y0;\n real y1[5];\n real y2[4, 7];\n ...\n // calculate and assign results\n y0 = exp(x0);\n y1 = exp(x1);\n y2 = exp(x2);\n ```


When \code{exp} is applied to an array, it applies elementwise.  For example, the statement above,


```\n y2 = exp(x2);\n ```


produces the same result for \code{y2} as the explicit loop


```\n for (i in 1:4)\n   for (j in 1:7)\n     y2[i, j] = exp(x2[i, j]);\n ```


\subsubsection{Vector and matrix arguments}


Vectorized functions also apply elementwise to vectors and matrices. For example,


```\n vector[5] xv;\n row_vector[7] xrv;\n matrix[10, 20] xm;\n \n vector[5] yv;\n row_vector[7] yrv;\n matrix[10, 20] ym;\n \n yv = exp(xv);\n yrv = exp(xrv);\n ym = exp(xm);\n ```


Arrays of vectors and matrices work the same way.  For example,


```\n matrix[17, 93] u[12];\n \n matrix[17, 93] z[12];\n \n z = exp(u);\n ```


After this has been executed, \code{z[i, j, k]} will be equal to \code{exp(u[i, j, k])}.


\subsubsection{Integer and integer array arguments}


Integer arguments are promoted to real values in vectorized unary functions.  Thus if \code{n} is of type \code{int}, \code{exp(n)} is of type \code{real}.  Arrays work the same way, so that if \code{n2} is a one dimensional array of integers, then \code{exp(n2)} will be a one-dimensional array of reals with the same number of elements as \code{n2}.  For example,


```\n int n1[23];\n real z1[23];\n z1 = exp(n1);\n ```


It would be illegal to try to assign \code{exp(n1)} to an array of integers; the return type is a real array.


\section{Mathematical Constants}\label{built-in-constants.section}


Constants are represented as functions with no arguments and must be called as such.  For instance, the mathematical constant $\pi$ must be written in a Stan program as \code{pi()}.


\begin{description} \fitem{real}{pi}{}{   $\pi$, the ratio of a circle's circumference to its diameter} \fitem{real}{e}{}{  $e$, the base of the natural logarithm} \fitem{real}{sqrt2}{}{ The square root of 2} \fitem{real}{log2}{}{ The natural logarithm of 2} \fitem{real}{log10}{}{ The natural logarithm of 10} \end{description}


\section{Special Values}


\begin{description} \fitem{real}{not_a_number}{}{ Not-a-number, a special non-finite real value returned to signal an error} \fitem{real}{positive_infinity}{}{  Positive infinity, a special non-finite real value larger than all   finite numbers} \fitem{real}{negative_infinity}{}{  Negative infinity, a special non-finite real value smaller than all   finite numbers} \fitem{real}{machine_precision}{}{ The smallest number $x$ such that $(x + 1) \neq 1$ in floating-point arithmetic on the current hardware platform} \end{description}


\section{Log Probability Function}\label{get-log-prob.section}


The basic purpose of a Stan program is to compute a log probability function and its derivatives.  The log probability function in a Stan model outputs the log density on the unconstrained scale.  A log probability accumulator starts at zero and is then incremented in various ways by a Stan program.  The variables are first transformed from unconstrained to constrained, and the log Jacobian determinant added to the log probability accumulator.  Then the model block is executed on the constrained parameters, with each sampling statement (`~`) and log probability increment statement (\code{increment_log_prob}) adding to the accumulator.  At the end of the model block execution, the value of the log probability accumulator is the log probability value returned by the Stan program.


Stan provides a special built-in function \code{target()} that takes no arguments and returns the current value of the log probability accumulator.[^fn_lp]  This function is primarily useful for debugging purposes, where for instance, it may be used with a print statement to display the log probability accumulator at various stages of execution to see where it becomes ill defined.

[^fn_lp]: This function used to be called \code{get_lp()}, but that   name has been deprecated; using it will print a warning.  The   function \code{get_lp()} will be removed in a future release.


\begin{description}   \fitem{real}{target}{}{     Return the current value of the log probability     accumulator.}   \fitem{real}{get_lp}{}{     Return the current value of the log probability accumulator; **deprecated;** - use \code{target()} instead.} \end{description}


Both \code{target} and the deprecated \code{get_lp} act like other functions ending in \code{_lp}, meaning that they may only may only be used in the model block.


\section{Logical Functions}


Like C++, BUGS, and R, Stan uses 0 to encode false, and 1 to encode true.  Stan supports the usual boolean comparison operations and boolean operators.  These all have the same syntax and precedence as in  C++; for the full list of operators and precedences, see the reference manual.


\subsection{Comparison Operators}


All comparison operators return boolean values, either 0 or 1.  Each operator has two signatures, one for integer comparisons and one for floating-point comparisons.  Comparing an integer and real value is carried out by first promoting the integer value.


\begin{description}   \fitemnobody{int}{operator<}{int \farg{x}, int \farg{y}}   \fitem{int}{operator<}{real \farg{x}, real \farg{y}}{     Return 1 if \farg{x} is less than \farg{y} and 0 otherwise.     \[     \text{operator<}(x,y)     =     \begin{cases}       1 & \text{if $x < y$} \\       0 & \text{otherwise}     \end{cases}     \]}   \fitemnobody{int}{operator<=}{int \farg{x}, int \farg{y}}   \fitem{int}{operator<=}{real \farg{x}, real \farg{y}}{     Return 1 if \farg{x} is less than or equal \farg{y} and 0 otherwise.     \[     \text{operator<=}(x,y)     = \begin{cases}         1 & \text{if $x \leq y$} \\         0 & \text{otherwise}       \end{cases}     \]}   \fitemnobody{int}{operator>}{int \farg{x}, int \farg{y}}   \fitem{int}{operator>}{real \farg{x}, real \farg{y}}{     Return 1 if \farg{x} is greater than \farg{y} and 0 otherwise.     \[     \text{operator>}     =     \begin{cases}       1 & \text{if $x > y$} \\       0 & \text{otherwise}     \end{cases}     \]}   \fitemnobody{int}{operator>=}{int \farg{x}, int \farg{y}}   \fitem{int}{operator>=}{real \farg{x}, real \farg{y}}{     Return 1 if \farg{x} is greater than or equal to \farg{y}     and 0 otherwise.     \[     \text{operator>=}     =     \begin{cases}       1 & \text{if $x \geq y$} \\       0 & \text{otherwise}     \end{cases}     \]}   \fitemnobody{int}{operator==}{int \farg{x}, int \farg{y}}   \fitem{int}{operator==}{real \farg{x}, real \farg{y}}{     Return 1 if \farg{x} is equal to \farg{y} and 0 otherwise.     \[     \text{operator==}(x,y) =     \begin{cases}       1 & \text{if $x = y$} \\       0 & \text{otherwise}     \end{cases}     \]}   \fitemindexnobody{int}{operator!=}{int \farg{x}, int \farg{y}}{}   \fitemindex{int}{operator!=}{real \farg{x}, real \farg{y}}{     Return 1 if \farg{x} is not equal to \farg{y} and 0 otherwise.     \[     \text{operator!=}(x,y)     =     \begin{cases}       1 & \text{if $x \neq y$} \\       0 & \text{otherwise}     \end{cases}     \]} \end{description}


\subsection{Boolean Operators}


Boolean operators return either 0 for false or 1 for true.  Inputs may be any real or integer values, with non-zero values being treated as true and zero values treated as false.  These operators have the usual precedences, with negation (not) binding the most tightly, conjunction the next and disjunction the weakest; all of the operators bind more tightly than the comparisons.  Thus an expression such as \code{!a && b} is interpreted as \code{(!a) && b}, and \code{a < b || c >= d && e != f} as \code{(a < b) || (((c >= d) && (e != f)))}.


\begin{description}   \fitemindexnobody{int}{operator!}{int \farg{x}}{}{   \fitemindex{int}{operator!}{real \farg{x}}{     Return 1 if \farg{x} is zero and 0 otherwise.     \[     \text{operator!}(x)     =     \begin{cases}       0 & \text{if $x \neq 0$} \\       1 & \text{if $x = 0$}     \end{cases}     \]}  \fitemnobody{int}{operator&&}{int \farg{x}, int \farg{y}}{}   \fitem{int}{operator&&}{real \farg{x}, real \farg{y}}{     Return 1 if \farg{x} is unequal to 0 and \farg{y} is unequal to 0.     \[     \mathrm{operator\&\&}(x,y)     =     \begin{cases}       1 & \text{if $x \neq 0$} \text{ and } y \neq 0\\       0 & \text{otherwise}     \end{cases}     \]}   \fitemindexnobody{int}{operator||}{int \farg{x}, int \farg{y}}{}   \fitemindex{int}{operator||}{real \farg{x}, real \farg{y}}{     Return 1 if \farg{x} is unequal to 0 or \farg{y} is unequal to 0.     \[     \text{operator||}(x,y) =     \begin{cases}       1 & \text{if $x \neq 0$} \textrm{ or } y \neq 0\\       0 & \text{otherwise}     \end{cases}     \]   }\end{description}


\subsubsection{Boolean Operator Short Circuiting}


Like in  C++, the boolean operators `&&` and `||` and are implemented to short circuit directly to a return value after evaluating the first argument if it is sufficient to resolve the result.  In evaluating \code{a || b}, if \code{a} evaluates to a value other than zero, the expression returns the value 1 without evaluating the expression \code{b}.  Similarly, evaluating \code{a && b} first evaluates \code{a}, and if the result is zero, returns 0 without evaluating \code{b}.


\subsection{Logical Functions}


The logical functions introduce conditional behavior functionally and are primarily provided for compatibility with BUGS and JAGS.


\begin{description}   \fitem{real}{step}{real \farg{x}}{     Return 1 if \farg{x} is positive and 0 otherwise.     \[     \text{step}(x) =     \begin{cases}       0 & \text{if } x < 0   \\       1 & \text{otherwise}     \end{cases}     \]  _**Warning:**_  \code{int_step(0)} and \code{int_step(NaN)} return 0 whereas \code{step(0)} and \code{step(NaN)} return 1.} \end{description}


The step function is often used in BUGS to perform conditional operations.  For instance, \code{step(a-b)} evaluates to 1 if \code{a} is greater than \code{b} and evaluates to 0 otherwise. \code{step} is a step-like functions; see the warning in section \@ref(step-functions) applied to expressions dependent on parameters.


\begin{description}   \fitem{int}{is_inf}{real \farg{x}}{     Return \farg{1} if \farg{x} is infinite (positive or negative) and \farg{0} otherwise.   } \end{description}


\begin{description}   \fitem{int}{is_nan}{real \farg{x}}{     Return \farg{1} if \farg{x} is NaN and \farg{0} otherwise.   } \end{description}


Care must be taken because both of these indicator functions are step-like and thus can cause discontinuities in gradients when applied to parameters; see section \@ref(step-functions) for details.


\section{Real-Valued Arithmetic Operators}\label{real-valued-arithmetic-operators.section}


The arithmetic operators are presented using  C++ notation.  For instance \code{operator+(x,y)} refers to the binary addition operator and \code{operator-(x)} to the unary negation operator.  In Stan programs, these are written using the usual infix and prefix notations as \code{x + y} and \code{-x}, respectively.


\subsection{Binary Infix Operators}


\begin{description}   \fitem{real}{operator+}{real \farg{x}, real \farg{y}}{     Return the sum of \farg{x} and \farg{y}.     \[     (x + y) = \text{operator+}(x,y) = x+y     \]}   \fitem{real}{operator-}{real \farg{x}, real \farg{y}}{     Return the difference between \farg{x} and \farg{y}.     \[     (x - y) = \text{operator-}(x,y) = x - y     \]}   \fitem{real}{operator*}{real \farg{x}, real \farg{y}}{     Return the product of \farg{x} and \farg{y}.     \[     (x * y) = \text{operator*}(x,y) = xy     \]}   \fitem{real}{operator/}{real \farg{x}, real \farg{y}}{     Return the quotient of \farg{x} and \farg{y}.     \[     (x / y) = \text{operator/}(x,y) = \frac{x}{y}     \]}   \fitem{real}{operator^}{real \farg{x}, real \farg{y}}{     Return \farg{x} raised to the power of \farg{y}.   \[ (x^\mathrm{\wedge}y)   = \text{operator}^\mathrm{\wedge}(x,y) = x^y   \]} \end{description}


\subsection{Unary Prefix Operators}


\begin{description}   \fitem{real}{operator-}{real \farg{x}}{     Return the negation of the subtrahend \farg{x}.     \[     \text{operator-}(x) = (-x)     \]}   \fitem{real}{operator+}{real \farg{x}}{     Return the value of \farg{x}.     \[     \text{operator+}(x) = x     \]} \end{description}


\section{Step-like Functions}\label{step-functions.section}


 _**Warning:**_  *These functions can seriously hinder sampling and   optimization efficiency for gradient-based methods (e.g., NUTS, HMC,   BFGS) if applied to parameters (including transformed parameters and   local variables in the transformed parameters or model block).  The   problem is that they break gradients due to discontinuities coupled with   zero gradients elsewhere.  They do not hinder sampling when used in the   data, transformed data, or generated quantities blocks.*


\subsection{Absolute Value Functions}


\begin{description}   \fitemUnaryVec{fabs}{absolute value of \farg{x}}{     \[     \text{abs}(x) = |x|     \]     See the warning at start of section \@ref(step-functions) for     application to parameters.   }   \fitem{real}{abs}{real \farg{x}}{ Return the absolute value of     \farg{x}, defined by     \[     \text{abs}(x) = |x|     \]     See the warning at start of section \@ref(step-functions) for     application to parameters.   }   \fitem{int}{abs}{int \farg{x}}{ Return the absolute value of     \farg{x}, defined by     \[     \text{abs}(x) = |x|     \]   }   \fitem{real}{fdim}{real \farg{x}, real \farg{y}}{     Return the positive difference between \farg{x} and \farg{y},     which is \farg{x} - \farg{y} if \farg{x} is greater than \farg{y}     and 0 otherwise;     see warning at start of section \@ref(step-functions).     \[     \text{fdim}(x,y) =     \begin{cases}       x-y & \text{if } x \geq y \\       0 & \text{otherwise}     \end{cases}     \]} \end{description}


\subsection{Bounds Functions}


\begin{description}   \fitem{real}{fmin}{real \farg{x}, real \farg{y}}{     Return the minimum of \farg{x} and \farg{y};     see warning at start of section \@ref(step-functions).     \[     \text{fmin}(x,y) =     \begin{cases}       x & \text{if } x \leq y \\       y & \text{otherwise}     \end{cases}     \]}   \fitem{real}{fmax}{real \farg{x}, real \farg{y}}{     Return the maximum of \farg{x} and \farg{y};     see warning at start of section \@ref(step-functions).     \[     \text{fmax}(x,y) =     \begin{cases}       x & \text{if } x \geq y \\       y & \text{otherwise}     \end{cases}     \]} \end{description}


\subsection{Arithmetic Functions}


\begin{description}   \fitem{real}{fmod}{real \farg{x}, real \farg{y}}{     Return the real value remainder after dividing \farg{x} by \farg{y};     see warning at start of section \@ref(step-functions).     \[     \text{fmod}(x,y) = x - \left\lfloor \frac{x}{y} \right\rfloor \, y     \]   The operator $\lfloor u \rfloor$ is the floor operation; see below.} \end{description}


\subsection{Rounding Functions}


 _**Warning:**_  Rounding functions convert real values to integers. Because the output is an integer, any gradient information resulting from functions applied to the integer is not passed to the real value it was derived from.  With MCMC sampling using HMC or NUTS, the MCMC acceptance procedure will correct for any error due to poor gradient calculations, but the result is likely to be reduced acceptance probabilities and less efficient sampling.


The rounding functions cannot be used as indices to arrays because they return real values.  Stan may introduce integer-valued versions of these in the future, but as of now, there is no good workaround.


\begin{description}   \fitemUnaryVec{floor}{floor of \farg{x}, which is the largest integer less than or equal to \farg{x}, converted to a real value; see warning at start of section \@ref(step-functions)}{     \[     \text{floor}(x) = \lfloor x \rfloor     \]}   \fitemUnaryVec{ceil}{ceiling of \farg{x}, which is the smallest integer greater than or equal to \farg{x}, converted to a real value; see warning at start of section \@ref(step-functions)}{     \[     \text{ceil}(x) = \lceil x\rceil     \]}   \fitemUnaryVec{round}{nearest integer to \farg{x}, converted to a real value; see warning at start of section \@ref(step-functions)}{     \[     \text{round}(x) =     \begin{cases}       \lceil x \rceil & \text{if } x-\lfloor x\rfloor \geq 0.5 \\       \lfloor x \rfloor & \text{otherwise}     \end{cases}     \]}   \fitemUnaryVec{trunc}{integer nearest to but no larger in magnitude than \farg{x}, converted to a double value; see warning at start of section \@ref(step-functions)}{     \[     \text{trunc}(x) = \lfloor x \rfloor     \]   Note that this function is redundant with \code{floor}.} \end{description}


\section{Power and Logarithm Functions}


\begin{description}   \fitemUnaryVec{sqrt}{square root of \farg{x}}{     \[     \text{sqrt}(x) =     \begin{cases}       \sqrt{x} & \text{if } x\geq 0\\       \textrm{NaN} & \text{otherwise}     \end{cases}     \]}   \fitemUnaryVec{cbrt}{cube root of \farg{x}}{     \[     \text{cbrt}(x) = \sqrt[3]{x}     \]}   \fitemUnaryVec{square}{square of \farg{x}}{     \[     \text{square}(x) = x^2     \]}   \fitemUnaryVec{exp}{natural exponential of \farg{x}}{     \[     \text{exp}(x) \ = \ \exp(x)     \ = \ e^x     \]}   \fitemUnaryVec{exp2}{base-2 exponential of \farg{x}}{     \[     \text{exp2}(x) = 2^x     \]}   \fitemUnaryVec{log}{natural logarithm of \farg{x}}{     \[     \text{log}(x) = \log x =     \begin{cases}       \log_{e}(x) & \text{if } x \geq 0 \\       \textrm{NaN} & \text{otherwise}     \end{cases}     \]}   \fitemUnaryVec{log2}{base-2 logarithm of \farg{x}}{     \[     \text{log2}(x) =     \begin{cases}       \log_2(x) & \text{if } x\geq 0 \\       \textrm{NaN} & \text{otherwise}     \end{cases}     \]}   \fitemUnaryVec{log10}{base-10 logarithm of \farg{x}}{     \[     \text{log10}(x) =     \begin{cases}       \log_{10}(x) & \text{if } x \geq 0 \\       \textrm{NaN} & \text{otherwise}     \end{cases}     \]}   \fitem{real}{pow}{real \farg{x}, real \farg{y}}{     Return \farg{x} raised to the power of \farg{y}.     \[     \text{pow}(x,y) = x^y     \]}   \fitemUnaryVec{inv}{inverse of \farg{x}}{     \[     \text{inv}(x) = \frac{1}{x}     \]}   \fitemUnaryVec{inv_sqrt}{inverse of the square root of \farg{x}}{     \[     \mathrm{inv\_sqrt}(x) =     \begin{cases}       \frac{1}{\sqrt{x}} & \text{if } x \geq 0 \\       \textrm{NaN} & \text{otherwise}     \end{cases}     \]}   \fitemUnaryVec{inv_square}{inverse of the square of \farg{x}}{     \[     \mathrm{inv\_square}(x) = \frac{1}{x^2}     \]} \end{description}


\section{Trigonometric Functions}


\begin{description}   \fitem{real}{hypot}{real \farg{x}, real \farg{y}}{     Return the length of the hypotenuse of a right triangle with sides of     length \farg{x} and \farg{y}.     \[     \text{hypot}(x,y) =     \begin{cases}       \sqrt{x^2+y^2} & \text{if } x,y\geq 0 \\       \textrm{NaN} & \text{otherwise}     \end{cases}     \]}   \fitemUnaryVec{cos}{cosine of the angle \farg{x} (in radians)}{     \[     \text{cos}(x) = \cos(x)     \]}   \fitemUnaryVec{sin}{sine of the angle \farg{x} (in radians)}{     \[     \text{sin}(x) = \sin(x)     \]}   \fitemUnaryVec{tan}{tangent of the angle \farg{x} (in radians)}{     \[     \text{tan}(x) = \tan(x)     \]}   \fitemUnaryVec{acos}{principal arc (inverse) cosine (in radians) of \farg{x}}{     \[     \text{acos}(x) =     \begin{cases}       \arccos(x) & \text{if } -1\leq x\leq 1 \\       \textrm{NaN} & \text{otherwise}     \end{cases}     \]}   \fitemUnaryVec{asin}{principal arc (inverse) sine (in radians) of \farg{x}}{     \[     \text{asin}(x) =     \begin{cases}       \arcsin(x) & \text{if } -1\leq x\leq 1 \\       \textrm{NaN} & \text{otherwise}     \end{cases}     \]}   \fitemUnaryVec{atan}{principal arc (inverse) tangent (in radians) of     \farg{x}, with values from $-\pi$ to $\pi$}{     \[     \text{atan}(x) = \arctan(x)     \]}   \fitem{real}{atan2}{real \farg{y}, real \farg{x}}{     Return the principal arc (inverse) tangent (in radians) of \farg{y} divided     by \farg{x},     \[     \text{atan2}(y, x) = \arctan\left(\frac{y}{x}\right)     \]} \end{description}


\section{Hyperbolic Trigonometric Functions}


\begin{description}   \fitemUnaryVec{cosh}{hyperbolic cosine of \farg{x} (in radians)}{     \[     \text{cosh}(x) = \cosh(x)     \]}   \fitemUnaryVec{sinh}{hyperbolic sine of \farg{x} (in radians)}{     \[     \text{sinh}(x) = \sinh(x)     \]}   \fitemUnaryVec{tanh}{hyperbolic tangent of \farg{x} (in radians)}{     \[     \text{tanh}(x) = \tanh(x)     \]}   \fitemUnaryVec{acosh}{inverse hyperbolic cosine (in radians)}{     \[     \text{acosh}(x) =     \begin{cases}       \cosh^{-1}(x) & \text{if } x \geq 1 \\       \textrm{NaN} & \text{otherwise}     \end{cases}     \]}   \fitemUnaryVec{asinh}{inverse hyperbolic cosine (in radians)}{     \[     \text{asinh}(x) = \sinh^{-1}(x)     \]}   \fitemUnaryVec{atanh}{inverse hyperbolic tangent (in radians) of \farg{x}}{     \[     \text{atanh}(x) =     \begin{cases}       \tanh^{-1}(x) & \text{if } -1\leq x \leq 1 \\       \textrm{NaN} & \text{otherwise}     \end{cases}     \]} \end{description}


\section{Link Functions}\label{link-functions.section}


The following functions are commonly used as link functions in generalized linear models.  The function $\Phi$ is also commonly used as a link function (see section \@ref(Phi-function)).


\begin{description}   \fitemUnaryVec{logit}{log odds, or logit, function applied to \farg{x}}{     \[     \text{logit}(x) =     \begin{cases}       \log\frac{x}{1-x} & \text{if } 0\leq x \leq 1 \\       \textrm{NaN} & \text{otherwise}     \end{cases}     \]   }   \fitemUnaryVec{inv_logit}{logistic sigmoid function applied to \farg{x}}{     \[     \mathrm{inv\_logit}(x) = \frac{1}{1 + \exp(-x)}     \]}   \fitemUnaryVec{inv_cloglog}{inverse of the complementary log-log     function applied to \farg{x}}{     \[     \mathrm{inv\_cloglog}(x) = 1 - \exp \! \left( - \exp(x) \right)     \]} \end{description}


\section{Probability-Related Functions}\label{Phi-function.section}


\subsection{Normal Cumulative Distribution Functions}


The error function \code{erf} is related to the standard normal cumulative distribution function $\Phi$ by scaling.  See section \@ref(normal-distribution) for the general normal cumulative distribution function (and its complement).


\begin{description}   \fitemUnaryVec{erf}{error function, also known as the Gauss error function, of \farg{x}}{     \[     \text{erf}(x) = \frac{2}{\sqrt{\pi}}\int_0^x e^{-t^2}dt     \]}   \fitemUnaryVec{erfc}{complementary error function of \farg{x}}{     \[     \text{erfc}(x) = \frac{2}{\sqrt{\pi}}\int_x^\infty e^{-t^2}dt     \]}   \fitemUnaryVecSort{Phi}{standard normal cumulative distribution function of \farg{x}}{     \[     \text{Phi}(x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x} e^{-t^2/2} dt     \]}{phi}   \fitemUnaryVec{inv_Phi}{standard normal inverse cumulative distribution function of \farg{p}, otherwise known as the quantile function}{     \[     \mathrm{Phi(inv\_Phi(p))} = p     \]}   \fitemUnaryVecSort{Phi_approx}{fast approximation of the unit (may     replace \code{Phi} for probit regression with maximum absolute error     of 0.00014, see [@BowlingEtAl:2009] for details)}{     \[     \mathrm{Phi\_approx}(x) = \text{logit}^{-1}(0.07056 \, x^3 + 1.5976 \, x)     \]}{phi_approx} \end{description}


\subsection{Other Probability-Related Functions}


\begin{description}   \fitem{real}{binary_log_loss}{int \farg{y}, real \farg{y_hat}}{     Return the log loss function for for predicting $\hat{y} \in [0,1]$ for     boolean outcome $y \in \{0,1\}$.     \[     \mathrm{binary\_log\_loss}(y,\hat{y}) =     \begin{cases}       -\log \hat{y}       & \text{if } y = 0\\       -\log (1 - \hat{y}) & \text{otherwise}     \end{cases}     \]}   \fitem{real}{owens_t}{real \farg{h}, real \farg{a}}{     Return the Owen's T function for the probability of the event $X > h$     and $0<Y<aX$ where \farg{X} and \farg{Y} are independent standard     normal random variables.     \[     \mathrm{owens\_t}(h,a)     = \frac{1}{2\pi} \int_0^a \frac{\exp(-\frac{1}{2}h^2(1+x^2))}{1+x^2}dx     \]} \end{description}


\section{Combinatorial Functions}\label{betafun.section}


\begin{description}   \fitem{real}{inc_beta}{real \farg{alpha}, real \farg{beta}, real     \farg{x}}{ Return the incomplete beta function up to \farg{x}     applied to \farg{alpha} and \farg{beta}.  See  section   \@ref(inc-beta-appendix) for a definition.  }   \fitem{real}{lbeta}{real \farg{alpha}, real \farg{beta}}{     Return the natural logarithm of the beta function applied to     \farg{alpha} and \farg{beta}.  The beta function, $\text{B}(\alpha,\beta)$,     computes the normalizing constant for the beta distribution, and     is defined for $\alpha > 0$ and $\beta > 0$.     \[     \text{lbeta}(\alpha,\beta)     = \log \Gamma(a) + \log \Gamma(b) - \log \Gamma(a+b)     \]     See section \@ref(beta-appendix) for definition of $\text{B}(\alpha, \beta)$.   }   \fitemUnaryVec{tgamma}{gamma function applied to \farg{x}.  The     gamma function is the generalization of the factorial function to     continuous variables, defined so that $\Gamma(n+1) = n!$.  See   for a full definition of $\Gamma(x)$.     The function is defined for positive numbers and non-integral     negative numbers,}{     \[     \text{tgamma}(x)     =     \begin{cases}       \Gamma(x) & \text{if } x\not\in \{\dots,-3,-2,-1,0\}\\       \textrm{error} & \text{otherwise}     \end{cases}     \]     See section \@ref(gamma-appendix) for a definition of $\Gamma(x)$.   }   \fitemUnaryVec{lgamma}{natural logarithm of the gamma function     applied to \farg{x},}{     \[     \text{lgamma}(x)     =     \begin{cases}       \log\Gamma(x)   & \text{if } x\not\in \{\dots,-3,-2,-1,0\}\\       \textrm{error} & \text{otherwise}     \end{cases}     \]   }   \fitemUnaryVec{digamma}{digamma function applied to \farg{x}.  The digamma function is the derivative of the natural logarithm of the Gamma function.  The function is defined  for positive numbers and non-integral negative numbers}{     \[     \text{digamma}(x)     =     \begin{cases}       \frac{\Gamma'(x)}{\Gamma(x)}  & \text{if } x\not\in \{\dots,-3,-2,-1,0\}\\       \textrm{error}                & \text{otherwise}     \end{cases}     \]     See section \@ref(gamma-appendix) for definition of $\Gamma(x)$.}   \fitemUnaryVec{trigamma}{trigamma function applied to \farg{x}.  The trigamma function is the second derivative of the natural logarithm of the Gamma function}{     \[     \text{trigamma}(x)     =     \begin{cases}       \sum_{n=0}^\infty \frac{1}{(x+n)^2}        & \text{if } x\not\in       \{\dots,-3,-2,-1,0\}       \\[6pt]       \textrm{error}                           & \text{otherwise}     \end{cases}     \]}   \fitem{real}{lmgamma}{int \farg{n}, real \farg{x}}{     Return the natural logarithm of the multivariate gamma function     $\Gamma_n$ with \farg{n} dimensions applied to \farg{x}.     \[     \text{lmgamma}(n,x)     =     \begin{cases}       \frac{n(n-1)}{4} \log \pi + \sum_{j=1}^n \log \Gamma\left(x + \frac{1 - j}{2}\right)                      & \text{if } x\not\in \{\dots,-3,-2,-1,0\}\\       \textrm{error} & \text{otherwise}     \end{cases}     \]}   \fitem{real}{gamma_p}{real \farg{a}, real \farg{z}}{ Return the     normalized lower incomplete gamma function of \farg{a} and     \farg{z} defined for positive \farg{a} and nonnegative \farg{z}.     \[     \mathrm{gamma\_p}(a,z)     =     \begin{cases}       \frac{1}{\Gamma(a)}\int_0^zt^{a-1}e^{-t}dt & \text{if } a > 0, z \geq 0 \\       \textrm{error} & \text{otherwise}     \end{cases}     \]}   \fitem{real}{gamma_q}{real \farg{a}, real \farg{z}}{ Return the     normalized upper incomplete gamma function of \farg{a} and     \farg{z} defined for positive \farg{a} and nonnegative \farg{z}.     \[     \mathrm{gamma\_q}(a,z) =     \begin{cases}       \frac{1}{\Gamma(a)}\int_z^\infty t^{a-1}e^{-t}dt & \text{if } a > 0, z \geq 0 \\[6pt]       \textrm{error} & \text{otherwise}     \end{cases}     \]}   \fitem{real}{binomial_coefficient_log}{real \farg{x}, real     \farg{y}}{      _**Warning:**_  This function is deprecated and should     be replaced with \code{lchoose}.     Return the natural logarithm of the binomial coefficient     of \farg{x} and \farg{y}. For non-negative integer inputs, the     binomial coefficient function is written as $\binom{x}{y}$ and     pronounced "\farg{x} choose \farg{y}."  This function     generalizes to real numbers using the gamma function.     For $0 \leq y \leq x$,     \[     \mathrm{binomial\_coefficient\_log}(x,y)     = \log\Gamma(x+1) - \log\Gamma(y+1) - \log\Gamma(x-y+1).     \]}   \fitem{int}{choose}{int \farg{x}, int \farg{y}}{     Return the binomial coefficient of \farg{x} and \farg{y}. For     non-negative integer inputs, the binomial coefficient function     is written as $\binom{x}{y}$ and pronounced "\farg{x} choose     \farg{y}." In its the antilog of the \code{lchoose} function     but returns an integer rather than a real number with no     non-zero decimal places.     For $0 \leq y \leq x$, the binomial coefficient function can be     defined via the factorial function     \[     \text{choose}(x,y)     = \frac{x!}{\left(y!\right)\left(x - y\right)!}.     \]}   \fitem{real}{bessel_first_kind}{int \farg{v}, real \farg{x}}{     Return the Bessel function of the first kind with order \farg{v}     applied to \farg{x}.     \[     \mathrm{bessel\_first\_kind}(v,x) =  J_v(x),     \]     where     \[     J_v(x)=\left(\frac{1}{2}x\right)^v     \sum_{k=0}^\infty \frac{\left(-\frac{1}{4}x^2\right)^k}{k!\, \Gamma(v+k+1)}     \]}   \fitem{real}{bessel_second_kind}{int \farg{v}, real \farg{x}}{     Return the Bessel function of the second kind with order \farg{v}     applied to \farg{x} defined for positive \farg{x} and \farg{v}.     For $x,v > 0$,     \[     \mathrm{bessel\_second\_kind}(v,x) =     \begin{cases}       Y_v(x) & \text{if } x > 0 \\       \textrm{error} & \text{otherwise}     \end{cases}     \]     where     \[     Y_v(x)=\frac{J_v(x)\cos(v\pi)-J_{-v}(x)}{\sin(v\pi)}     \]}   \fitem{real}{modified_bessel_first_kind}{int \farg{v}, real     \farg{z}}{ Return the modified Bessel function of the first kind     with order \farg{v} applied to \farg{z} defined for all \farg{z}     and \farg{v}.     \[     \mathrm{modified\_bessel\_first\_kind}(v,z) = I_v(z)     \]     where     \[       {I_v}(z)         = \left(\frac{1}{2}z\right)^v\sum_{k=0}^\infty            \frac{\left(\frac{1}{4}z^2\right)^k}{k!\Gamma(v+k+1)}     \]}   \fitem{real}{modified_bessel_second_kind}{int \farg{v}, real \farg{z}}{     Return the modified Bessel function of the second kind with order \farg{v}     applied to \farg{z} defined for  positive \farg{z} and \farg{v}.     \[     \mathrm{modified\_bessel\_second\_kind}(v,z) =     \begin{cases}       K_v(z) & \text{if } z > 0 \\       \textrm{error} & \text{if } z \leq 0     \end{cases}     \]     where     \[       {K_v}(z)       =       \frac{\pi}{2}\cdot\frac{I_{-v}(z) - I_{v}(z)}{\sin(v\pi)}     \]}   \fitem{real}{falling_factorial}{real \farg{x}, real \farg{n}}{     Return the falling factorial of \farg{x} with power \farg{n}     defined for positive \farg{x} and real \farg{n}.     \[     \mathrm{falling\_factorial}(x,n) =     \begin{cases}       (x)_n          & \text{if } x > 0 \\       \textrm{error} & \text{if } x \leq 0     \end{cases}     \]     where     \[     (x)_n=\frac{\Gamma(x+1)}{\Gamma(x-n+1)}     \]}   \fitem{real}{lchoose}{real \farg{x}, real \farg{y}}{ Return the     natural logarithm of the generalized binomial coefficient of     \farg{x} and \farg{y}. For non-negative integer inputs, the     binomial coefficient function is written as $\binom{x}{y}$ and     pronounced "\farg{x} choose \farg{y}."  This function     generalizes to real numbers using the gamma function.     For $0 \leq y \leq x$,     \[     \mathrm{binomial\_coefficient\_log}(x,y)     = \log\Gamma(x+1) - \log\Gamma(y+1) - \log\Gamma(x-y+1).     \]}   \fitem{real}{log_falling_factorial}{real \farg{x}, real \farg{n}}{     Return the log of the falling factorial of \farg{x} with power     \farg{n} defined for positive \farg{x} and real \farg{n}.     \[     \mathrm{log\_falling\_factorial}(x,n) =     \begin{cases}       \log (x)_n      & \text{if } x > 0 \\       \textrm{error} & \text{if } x \leq 0     \end{cases}     \]}   \fitem{real}{rising_factorial}{real \farg{x}, real \farg{n}}{     Return the rising factorial of \farg{x} with power \farg{n}     defined for positive \farg{x} and real \farg{n}.     \[     \mathrm{rising\_factorial}(x,n) =     \begin{cases}       x^{(n)}         & \text{if } x > 0 \\       \textrm{error} & \text{if } x \leq 0     \end{cases}     \]     where     \[     x^{(n)}=\frac{\Gamma(x+n)}{\Gamma(x)}     \]}   \fitem{real}{log_rising_factorial}{real \farg{x}, real \farg{n}}{     Return the log of the rising factorial of \farg{x} with power     \farg{n} defined for positive \farg{x} and real \farg{n}.     \[     \mathrm{log\_rising\_factorial}(x,n) =     \begin{cases}       \log x^{(n)}     & \text{if } x > 0 \\       \textrm{error} & \text{if } x \leq 0     \end{cases}     \]} \end{description}


\section{Composed Functions}\label{composed-functions.section}


The functions in this section are equivalent in theory to combinations of other functions.  In practice, they are implemented to be more efficient and more numerically stable than defining them directly using more basic Stan functions.


\begin{description}   \fitemUnaryVec{expm1}{natural exponential of \farg{x} minus 1}{     \[     \text{expm1}(x) = e^x - 1     \]}   \fitem{real}{fma}{real \farg{x}, real \farg{y}, real \farg{z}}{     Return \farg{z} plus the result of \farg{x} multiplied by \farg{y}.     \[     \text{fma}(x,y,z) = (x \times y) + z     \]}   \fitem{real}{multiply_log}{real \farg{x}, real \farg{y}}{      _**Warning:**_  This function is deprecated and should be replaced     with \code{lmultiply}.    Return the product of \farg{x} and the natural logarithm     of \farg{y}.     \[     \mathrm{multiply\_log}(x,y) =     \begin{cases}       0 & \text{if } x = y = 0 \\       x \log y & \text{if } x, y \neq 0 \\       \text{NaN} & \text{otherwise}     \end{cases}     \]}   \fitem{real}{lmultiply}{real \farg{x}, real \farg{y}}{     Return the product of \farg{x} and the natural logarithm     of \farg{y}.     \[     \text{lmultiply}(x,y) =     \begin{cases}       0 & \text{if } x = y = 0 \\       x \log y & \text{if } x, y \neq 0 \\       \text{NaN} & \text{otherwise}     \end{cases}     \]}   \fitemUnaryVec{log1p}{natural logarithm of 1 plus \farg{x}}{     \[     \text{log1p}(x) =     \begin{cases}       \log(1+x)& \text{if } x \geq -1 \\       \textrm{NaN} & \text{otherwise}     \end{cases}     \]}   \fitemUnaryVec{log1m}{natural logarithm of 1 minus \farg{x}}{     \[     \text{log1m}(x) =     \begin{cases}       \log(1-x) & \text{if } x \leq 1 \\       \textrm{NaN} & \text{otherwise}     \end{cases}     \]}   \fitemUnaryVec{log1p_exp}{natural logarithm of one plus the natural exponentiation of \farg{x}}{     \[     \mathrm{log1p\_exp}(x) = \log(1+\exp(x))     \]}   \fitemUnaryVec{log1m_exp}{logarithm of one minus the natural exponentiation of \farg{x}}{     \[     \mathrm{log1m\_exp}(x) =     \begin{cases}       \log(1-\exp(x)) & \text{if } x < 0 \\       \textrm{NaN} & \text{if } x \geq 0     \end{cases}     \]}   \fitem{real}{log_diff_exp}{real \farg{x}, real \farg{y}}{     Return the natural logarithm of the difference of the natural exponentiation     of \farg{x} and the natural exponentiation of \farg{y}.     \[     \mathrm{log\_diff\_exp}(x,y) =     \begin{cases}       \log(\exp(x)-\exp(y)) & \text{if } x > y \\[6pt]       \textrm{NaN} & \text{otherwise}     \end{cases}     \]}   \fitem{real}{log_mix}{real \farg{theta}, real \farg{lp1}, real     \farg{lp2}}{     Return the log mixture of the log densities \farg{lp1} and     \farg{lp2} with mixing proportion \farg{theta}, defined by         \begin{eqnarray*}     \mathrm{log\_mix}(\theta, \lambda_1, \lambda_2)       & = & \log \!\left( \theta \exp(\lambda_1)       + \left( 1 - \theta \right) \exp(\lambda_2)     \right)     \\[3pt]     & = & \mathrm{log\_sum\_exp}\!\left(\log(\theta) + \lambda_1, \                                \log(1 - \theta) + \lambda_2\right).   \end{eqnarray*} }   \fitem{real}{log_sum_exp}{real \farg{x}, real \farg{y}}{     Return the natural logarithm of the sum of the natural exponentiation     of \farg{x} and the natural exponentiation of \farg{y}.     \[     \mathrm{log\_sum\_exp}(x,y) = \log(\exp(x)+\exp(y))     \]}   \fitemUnaryVec{log_inv_logit}{natural logarithm of the inverse logit function of \farg{x}}{     \[     \mathrm{log\_inv\_logit}(x) = \log \, \text{logit}^{-1}(x)     \]   See section \@ref(link-functions) for a definition of inverse logit.}   \fitemUnaryVec{log1m_inv_logit}{natural logarithm of 1 minus the inverse logit function of \farg{x}}{     \[     \mathrm{log1m\_inv\_logit}(x) = \log(1 - \text{logit}^{-1}(x))     \]   See section \@ref(link-functions) for a definition of inverse logit.}  \end{description}


\chapter{Array Operations}


\section{Reductions}\label{array-reductions.section}


The following operations take arrays as input and produce single output values.  The boundary values for size 0 arrays are the unit with respect to the combination operation (min, max, sum, or product).


\subsection{Minimum and Maximum}


\begin{description} \fitem{real}{min}{real[] \farg{x}}{ The minimum value in \farg{x}, or $+\infty$ if \farg{x} is size 0.} \fitem{int}{min}{int[] \farg{x}}{ The minimum value in \farg{x}, or error if \farg{x} is size 0.} \fitem{real}{max}{real[] \farg{x}}{ The maximum value in \farg{x}, or $-\infty$ if \farg{x} is size 0.} \fitem{int}{max}{int[] \farg{x}}{ The maximum value in \farg{x}, or error if \farg{x} is size 0.} \end{description}


\subsection{Sum, Product, and Log Sum of Exp}


\begin{description} \fitem{int}{sum}{int[] \farg{x}}{ The sum of the elements in \farg{x}, defined for $x$ of size $N$ by \[ \text{sum}(x) = \begin{cases} \sum_{n=1}^N x_n & \text{if}  N > 0 \\[4pt] 0 & \text{if} N = 0 \end{cases} \] } \fitem{real}{sum}{real[] \farg{x}}{ The sum of the elements in \farg{x}; see definition above.} \fitem{real}{prod}{real[] \farg{x}}{ The product of the elements in \farg{x}, or 1 if \farg{x} is size 0.} \fitem{real}{prod}{int[] \farg{x}}{ The product of the elements in \farg{x}, \[ \text{product}(x) = \begin{cases} \prod_{n=1}^N x_n & \text{if}  N > 0 \\[4pt] 1 & \text{if} N = 0 \end{cases} \] } \fitem{real}{log_sum_exp}{real[] \farg{x}}{ The natural logarithm of the sum of the exponentials of the elements in \farg{x}, or $-\infty$ if the array is empty.} \end{description}


\subsection{Sample Mean, Variance, and Standard Deviation}


The sample mean, variance, and standard deviation are calculated in the usual way.  For i.i.d. draws from a distribution of finite mean, the sample mean is an unbiased estimate of the mean of the distribution.  Similarly, for i.i.d. draws from a distribution of finite variance, the sample variance is an unbiased estimate of the variance.[^fnvar]  The sample deviation is defined as the square root of the sample deviation, but is not unbiased.

[^fnvar]: Dividing by $N$ rather than $(N-1)$ produces a maximum   likelihood estimate of variance, which is biased to underestimate   variance.


\begin{description} \fitem{real}{mean}{real[] \farg{x}}{ The sample mean of the elements in \farg{x}. For an array $x$ of size $N > 0$, \[ \text{mean}(x) \ = \ \bar{x} \ = \ \frac{1}{N} \sum_{n=1}^N x_n. \] It is an error to the call the mean function with an array of size $0$. } \fitem{real}{variance}{real[] \farg{x}}{ The sample variance of the elements in \farg{x}. For $N > 0$, \[ \text{variance}(x) \ = \ \begin{cases} \frac{1}{N-1} \sum_{n=1}^N (x_n - \bar{x})^2 & \text{if } N > 1 \\[4pt] 0 & \text{if } N = 1 \end{cases} \] It is an error to call the \code{variance} function with an array of size 0. } \fitem{real}{sd}{real[] \farg{x}}{The sample standard deviation of elements in \farg{x}. \[ \text{sd}(x) = \begin{cases} \sqrt{\, \text{variance}(x)} & \text{if } N > 1 \\[4pt] 0 & \text{if } N = 0 \end{cases} \] It is an error to call the \code{sd} function with an array of size 0. } \end{description}


\subsection{Euclidean Distance and Squared Distance}


\begin{description}   \fitem{real}{distance}{vector \farg{x}, vector \farg{y}}{The     Euclidean distance between \farg{x} and \farg{y}, defined by \[ \text{distance}(x,y) \ = \ \sqrt{\textstyle \sum_{n=1}^N (x_n - y_n)^2} \] where \code{N} is the size of \farg{x} and \farg{y}. It is an error to call \code{distance} with arguments of unequal size.} \fitem{real}{distance}{vector \farg{x}, row_vector \farg{y}}{The   Euclidean distance between \farg{x} and \farg{y}} \fitem{real}{distance}{row_vector \farg{x}, vector \farg{y}}{The   Euclidean distance between \farg{x} and \farg{y}} \fitem{real}{distance}{row_vector \farg{x}, row_vector \farg{y}}{The   Euclidean distance between \farg{x} and \farg{y}} \fitem{real}{squared_distance}{vector \farg{x}, vector   \farg{y}}{The squared Euclidean   distance between \farg{x} and \farg{y}, defined by \[ \mathrm{squared\_distance}(x,y) \ = \ \text{distance}(x,y)^2 \ = \ \textstyle \sum_{n=1}^N (x_n - y_n)^2, \] where \code{N} is the size of \farg{x} and \farg{y}.  It is an error to call \code{squared_distance} with arguments of unequal size.} \fitem{real}{squared_distance}{vector \farg{x}, row_vector  [] \farg{y}}{The squared Euclidean distance between \farg{x} and \farg{y}} \fitem{real}{squared_distance}{row_vector \farg{x}, vector  [] \farg{y}}{The squared Euclidean distance between \farg{x} and \farg{y}} \fitem{real}{squared_distance}{row_vector \farg{x}, row_vector[] \farg{y}}{The Euclidean   distance between \farg{x} and \farg{y}} \end{description}


\section{Array Size and Dimension Function}


The size of an array or matrix can be obtained using the \code{dims()} function.  The \code{dims()} function is defined to take an argument consisting of any variable with up to 8 array dimensions (and up to 2 additional matrix dimensions) and returns an array of integers with the dimensions.  For example, if two variables are declared as follows,


```\n real x[7,8,9];\n matrix[8,9] y[7];\n ```


then calling \code{dims(x)} or \code{dims(y)} returns an integer array of size 3 containing the elements 7, 8, and 9 in that order.


The \code{size()} function extracts the number of elements in an array.  This is just the top-level elements, so if the array is declared as


```\n real a[M,N];\n ```


the size of \code{a} is \code{M}.


The function \code{num_elements}, on the other hand, measures all of the elements, so that the array \code{a} above has $M \times N$ elements.


The specialized functions \code{rows()} and \code{cols()} should be used to extract the dimensions of vectors and matrices.


\begin{description} \fitem{int[]}{dims}{\farg{T} \farg{x}}{Return an integer array   containing the dimensions of \farg{x}; the type of the argument   \farg{T} can be any Stan type with up to 8 array dimensions.} \fitem{int}{num_elements}{\farg{T}[] \farg{x}}{Return the total   number of elements in the array \farg{x} including all elements   in contained arrays, vectors, and matrices.   \farg{T} can be any array type.  For example, if \code{x} is of   type \code{real[4,3]} then \code{num_elements(x)} is 12, and if   \code{y} is declared as \code{matrix[3,4] y[5]}, then \code{size(y)}   evaluates to 60.} \fitem{int}{size}{\farg{T}[] \farg{x}}{Return the number of elements   in the array \farg{x}; the type of the array \farg{T} can be any   type, but the size is just the size of the top level array, not the   total number of elements contained.  For example, if \code{x} is of   type \code{real[4,3]} then \code{size(x)} is 4.} \end{description}


\section{Array Broadcasting}\label{array-broadcasting.section}


The following operations create arrays by repeating elements to fill an array of a specified size.  These operations work for all input types \farg{T}, including reals, integers, vectors, row vectors, matrices, or arrays.


\begin{description} \fitem{\farg{T}[]}{rep_array}{\farg{T} \farg{x}, int \farg{n}}{Return   the \farg{n} array with every entry assigned to \farg{x}.} \fitem{\farg{T}[,]}{rep_array}{\farg{T} \farg{x}, int \farg{m}, int   \farg{n}}{Return the \farg{m} by \farg{n} array with every entry   assigned to \farg{x}.} \fitem{\farg{T}[,,]}{rep_array}{\farg{T} \farg{x}, int   \farg{k}, int \farg{m}, int   \farg{n}}{Return the \farg{k} by \farg{m} by \farg{n} array with   every entry assigned to \farg{x}.} \end{description}


For example, \code{rep_array(1.0,5)} produces a real array (type \code{real[]}) of size 5 with all values set to 1.0.  On the other hand, \code{rep_array(1,5)} produces an integer array (type \code{int[]}) of size 5 with all values set to 1.  This distinction is important because it is not possible to assign an integer array to a real array.  For example, the following example contrasts legal with illegal array creation and assignment


```\n real y[5];\n int x[5];\n \n x = rep_array(1,5);     // ok\n y = rep_array(1.0,5);   // ok\n \n x = rep_array(1.0,5);   // illegal\n y = rep_array(1,5);     // illegal\n \n x = y;                  // illegal\n y = x;                  // illegal\n ```


If the value being repeated \code{v} is a vector (i.e., \code{T} is \code{vector}), then \code{rep_array(v,27)} is a size 27 array consisting of 27 copies of the vector \code{v}.


```\n vector[5] v;\n vector[5] a[3];\n \n a = rep_array(v,3);  // fill a with copies of v\n a[2,4] = 9.0;        // v[4], a[1,4], a[2,4] unchanged\n ```


If the type \farg{T} of \farg{x} is itself an array type, then the result will be an array with one, two, or three added dimensions, depending on which of the \code{rep_array} functions is called.  For instance, consider the following legal code snippet.


```\n real a[5,6];\n real b[3,4,5,6];\n \n b = rep_array(a,3,4); //  make (3 x 4) copies of a\n b[1,1,1,1] = 27.9;    //  a[1,1] unchanged\n ```


After the assignment to \code{b}, the value for \code{b[j,k,m,n]} is equal to \code{a[m,n]} where it is defined, for \code{j} in \code{1:3}, \code{k} in \code{1:4}, \code{m} in \code{1:5}, and \code{n} in \code{1:6}.


\section{Array Concatenation}\label{array-concatenation.section}


\begin{description} \fitem{T}{append_array}{T \farg{x}, T \farg{y}}{   Return the concatenation of two arrays in the order of the arguments.   T must be an N-dimensional array of any Stan type (with a maximum N of 7). All   dimensions but the first must match. } \end{description}


For example, the following code appends two three dimensional arrays of matrices together. Note that all dimensions except the first match. Any mismatches will cause an error to be thrown.


```\n matrix[4, 6] x1[2, 1, 7];\n matrix[4, 6] x2[3, 1, 7];\n matrix[4, 6] x3[5, 1, 7];\n \n x3 = append_array(x1, x2);\n ```


\section{Sorting functions}\label{sorting-functions.section}


Sorting can be used to sort values or the indices of those values in either ascending or descending order.  For example, if \code{v} is declared as a real array of size 3, with values \[ \text{v} = (1, -10.3, 20.987), \] then the various sort routines produce  \begin{eqnarray*} \mathrm{sort\_asc(v)} & = &  (-10.3,1,20.987) \\[4pt] \mathrm{sort\_desc(v)} & = &  (20.987,1,-10.3) \\[4pt] \mathrm{sort\_indices\_asc(v)} & = &  (2,1,3) \\[4pt] \text{sort\_indices\_desc(v)} & = &  (3,1,2) \end{eqnarray*}


\begin{description} \fitem{real[]}{sort_asc}{real[] \farg{v}}{ Sort the elements of \farg{v} in ascending order} \fitem{int[]}{sort_asc}{int[] \farg{v}}{ Sort the elements of \farg{v} in ascending order} \fitem{real[]}{sort_desc}{real[] \farg{v}}{ Sort the elements of \farg{v} in descending order} \fitem{int[]}{sort_desc}{int[] \farg{v}}{ Sort the elements of \farg{v} in descending order} \fitem{int[]}{sort_indices_asc}{real[] \farg{v}}{ Return an array of indices between 1 and the size of \farg{v}, sorted to index \farg{v} in ascending order.} \fitem{int[]}{sort_indices_asc}{int[] \farg{v}}{ Return an array of indices between 1 and the size of \farg{v}, sorted to index \farg{v} in ascending order.} \fitem{int[]}{sort_indices_desc}{real[] \farg{v}}{ Return an array of indices between 1 and the size of \farg{v}, sorted to index \farg{v} in descending order.} \fitem{int[]}{sort_indices_desc}{int[] \farg{v}}{ Return an array of indices between 1 and the size of \farg{v}, sorted to index \farg{v} in descending order.} \fitem{int}{rank}{real[] \farg{v}, int \farg{s}}{ Number of components of \farg{v} less than \farg{v[s]}} \fitem{int}{rank}{int[] \farg{v}, int \farg{s}}{ Number of components of \farg{v} less than \farg{v[s]}} \end{description}


\chapter{Matrix Operations}\label{matrix-operations.chapter}


\section{Integer-Valued Matrix Size Functions}


\begin{description} \fitem{int}{num_elements}{vector \farg{x}}{The total   number of elements in the vector   \farg{x} (same as function \code{rows})} \fitem{int}{num_elements}{row_vector \farg{x}}{The total   number of elements in the vector   \farg{x} (same as function \code{cols})} \fitem{int}{num_elements}{matrix \farg{x}}{The total   number of elements in the matrix \farg{x}.   For example, if \code{x} is a $5 \times 3$ matrix,   then \code{num_elements(x)} is 15} \fitem{int}{rows}{vector \farg{x}}{The number of rows in the vector \farg{x}} \fitem{int}{rows}{row_vector \farg{x}}{The number of rows in the row vector \farg{x}, namely 1} \fitem{int}{rows}{matrix \farg{x}}{The number of rows in the matrix \farg{x}} \fitem{int}{cols}{vector \farg{x}}{The number of columns in the vector \farg{x}, namely 1} \fitem{int}{cols}{row_vector \farg{x}}{The number of columns in the row vector \farg{x}} \fitem{int}{cols}{matrix \farg{x}}{The number of columns in the matrix \farg{x}} \end{description}


\section{Matrix Arithmetic Operators}\label{matrix-arithmetic-operators.section}


Stan supports the basic matrix operations using infix, prefix and postfix operations.  This section lists the operations supported by Stan along with their argument and result types.


\subsection{Negation Prefix Operators}


\begin{description} \fitem{vector}{operator-}{vector \farg{x}}{The negation of the vector \farg{x}.} \fitem{row_vector}{operator-}{row_vector \farg{x}}{The negation of the row vector \farg{x}.} \fitem{matrix}{operator-}{matrix \farg{x}}{The negation of the matrix   \farg{x}.} \end{description}


\subsection{Infix Matrix Operators}


\begin{description} \fitem{vector}{operator+}{vector \farg{x}, vector \farg{y}}{The sum of the vectors \farg{x} and \farg{y}.} \fitem{row_vector}{operator+}{row_vector \farg{x}, row_vector \farg{y}}{The sum of the row vectors \farg{x} and \farg{y}.} \fitem{matrix}{operator+}{matrix \farg{x}, matrix \farg{y}}{The sum of the matrices \farg{x} and \farg{y}} \end{description}


\begin{description} \fitem{vector}{operator-}{vector \farg{x}, vector \farg{y}}{The difference between the vectors \farg{x} and \farg{y}.} \fitem{row_vector}{operator-}{row_vector \farg{x}, row_vector \farg{y}}{The   difference between the row vectors \farg{x} and \farg{y}} \fitem{matrix}{operator-}{matrix \farg{x}, matrix \farg{y}}{The difference between   the matrices \farg{x} and \farg{y}} \end{description}


\begin{description} \fitem{vector}{operator*}{real \farg{x}, vector \farg{y}}{The product of the scalar \farg{x} and vector \farg{y}} \fitem{row_vector}{operator*}{real \farg{x}, row_vector \farg{y}}{The product of the scalar \farg{x} and the row vector \farg{y}} \fitem{matrix}{operator*}{real \farg{x}, matrix \farg{y}}{The product of the scalar \farg{x} and the matrix \farg{y}} \fitem{vector}{operator*}{vector \farg{x}, real \farg{y}}{The product of the scalar \farg{y} and vector \farg{x}} \fitem{matrix}{operator*}{vector \farg{x}, row_vector \farg{y}}{The product of the vector \farg{x} and row vector \farg{y}} \fitem{row_vector}{operator*}{row_vector \farg{x}, real \farg{y}}{The product of the scalar \farg{y} and row vector \farg{x}} \fitem{real}{operator*}{row_vector \farg{x}, vector \farg{y}}{The product of the row vector \farg{x} and vector \farg{y}} \fitem{row_vector}{operator*}{row_vector \farg{x}, matrix \farg{y}}{The product of the row vector \farg{x} and matrix \farg{y}} \fitem{matrix}{operator*}{matrix \farg{x}, real \farg{y}}{The product of the scalar \farg{y} and matrix \farg{x}} \fitem{vector}{operator*}{matrix \farg{x}, vector \farg{y}}{The   product of the matrix \farg{x} and vector \farg{y}} \fitem{matrix}{operator*}{matrix \farg{x}, matrix \farg{y}}{The product of   the matrices \farg{x} and \farg{y}} \end{description}


\subsection{Broadcast Infix Operators}


\begin{description} \fitem{vector}{operator+}{vector \farg{x}, real \farg{y}}{The result of adding \farg{y} to every entry in the vector \farg{x}} \fitem{vector}{operator+}{real \farg{x}, vector \farg{y}}{The result of adding \farg{x} to every entry in the vector \farg{y}} \fitem{row_vector}{operator+}{row_vector \farg{x}, real \farg{y}}{The result of adding \farg{y} to every entry in the row vector \farg{x}} \fitem{row_vector}{operator+}{real \farg{x}, row_vector \farg{y}}{The result of adding \farg{x} to every entry in the row vector \farg{y}} \fitem{matrix}{operator+}{matrix \farg{x}, real \farg{y}}{The result of adding \farg{y} to every entry in the matrix \farg{x}} \fitem{matrix}{operator+}{real \farg{x}, matrix \farg{y}}{The result of adding \farg{x} to every entry in the matrix \farg{y}} \end{description}


\begin{description} \fitem{vector}{operator-}{vector \farg{x}, real \farg{y}}{The result of subtracting \farg{y} from every entry in the vector \farg{x}} \fitem{vector}{operator-}{real \farg{x}, vector \farg{y}}{The result of adding \farg{x} to every entry in the negation of the vector \farg{y}} \fitem{row_vector}{operator-}{row_vector \farg{x}, real \farg{y}}{The result of subtracting \farg{y} from every entry in the row vector \farg{x}} \fitem{row_vector}{operator-}{real \farg{x}, row_vector \farg{y}}{The result of adding \farg{x} to every entry in the negation of the row vector \farg{y}} \fitem{matrix}{operator-}{matrix \farg{x}, real \farg{y}}{The result of subtracting \farg{y} from every entry in the matrix \farg{x}} \fitem{matrix}{operator-}{real \farg{x}, matrix \farg{y}}{The result of adding \farg{x} to every entry in negation of the matrix \farg{y}} \end{description}


\begin{description} \fitem{vector}{operator/}{vector \farg{x}, real \farg{y}}{The result of dividing each entry in the vector \farg{x} by \farg{y}} \fitem{row_vector}{operator/}{row_vector \farg{x}, real \farg{y}}{The result of dividing each entry in the row vector \farg{x} by \farg{y}} \fitem{matrix}{operator/}{matrix \farg{x}, real \farg{y}}{The result of dividing each entry in the matrix \farg{x} by \farg{y}} \end{description}


\subsection{Elementwise Arithmetic Operations}


\begin{description} \fitem{vector}{operator.*}{vector \farg{x}, vector \farg{y}}{The elementwise product of \farg{y} and \farg{x}} \fitem{row_vector}{operator.*}{row_vector \farg{x}, row_vector \farg{y}}{The elementwise product of \farg{y} and \farg{x}} \fitem{matrix}{operator.*}{matrix \farg{x}, matrix \farg{y}}{The elementwise product of \farg{y} and \farg{x}} \end{description}


\begin{description} \fitem{vector}{operator./}{vector \farg{x}, vector \farg{y}}{The elementwise quotient of \farg{y} and \farg{x}} \fitem{vector}{operator./}{vector \farg{x}, real \farg{y}}{The elementwise quotient of \farg{y} and \farg{x}} \fitem{vector}{operator./}{real \farg{x}, vector \farg{y}}{The elementwise quotient of \farg{y} and \farg{x}} \fitem{row_vector}{operator./}{row_vector \farg{x}, row_vector \farg{y}}{The elementwise quotient of \farg{y} and \farg{x}} \fitem{row_vector}{operator./}{row_vector \farg{x}, real \farg{y}}{The elementwise quotient of \farg{y} and \farg{x}} \fitem{row_vector}{operator./}{real \farg{x}, row_vector \farg{y}}{The elementwise quotient of \farg{y} and \farg{x}} \fitem{matrix}{operator./}{matrix \farg{x}, matrix \farg{y}}{The elementwise quotient of \farg{y} and \farg{x}} \fitem{matrix}{operator./}{matrix \farg{x}, real \farg{y}}{The elementwise quotient of \farg{y} and \farg{x}} \fitem{matrix}{operator./}{real \farg{x}, matrix \farg{y}}{The elementwise quotient of \farg{y} and \farg{x}} \end{description}


\section{Transposition Operator}


Matrix transposition is represented using a postfix operator.


\begin{description} \fitem{matrix}{operator'}{matrix \farg{x}}{The transpose of the matrix  \farg{x}, written as \code{x'}} \fitem{row_vector}{operator'}{vector \farg{x}}{The transpose of the vector  \farg{x}, written as \code{x'}} \fitem{vector}{operator'}{row_vector \farg{x}}{The transpose of the row vector  \farg{x}, written as \code{x'}} \end{description}


\section{Elementwise Functions}


Elementwise functions apply a function to each element of a vector or matrix, returning a result of the same shape as the argument.  There are many functions that are vectorized in addition to the ad hoc cases listed in this section;  see section \@ref(fun-vectorization)for the general cases.


\section{Dot Products and Specialized Products}


\begin{description} \fitem{real}{dot_product}{vector \farg{x}, vector \farg{y}}{ The dot product of \farg{x} and \farg{y}} \fitem{real}{dot_product}{vector \farg{x}, row_vector \farg{y}}{ The dot product of \farg{x} and \farg{y}} \fitem{real}{dot_product}{row_vector \farg{x}, vector \farg{y}}{ The dot product of \farg{x} and \farg{y}} \fitem{real}{dot_product}{row_vector \farg{x}, row_vector \farg{y}}{ The dot product of \farg{x} and \farg{y}} \fitem{row_vector}{columns_dot_product}{vector \farg{x}, vector \farg{y}}{ The dot product of the columns of \farg{x} and \farg{y}} \fitem{row_vector}{columns_dot_product}{row_vector \farg{x}, row_vector \farg{y}}{ The dot product of the columns of \farg{x} and \farg{y}} \fitem{row_vector}{columns_dot_product}{matrix \farg{x}, matrix \farg{y}}{ The dot product of the columns of \farg{x} and \farg{y}} \fitem{vector}{rows_dot_product}{vector \farg{x}, vector \farg{y}}{ The dot product of the rows of \farg{x} and \farg{y}} \fitem{vector}{rows_dot_product}{row_vector \farg{x}, row_vector \farg{y}}{ The dot product of the rows of \farg{x} and \farg{y}} \fitem{vector}{rows_dot_product}{matrix \farg{x}, matrix \farg{y}}{ The dot product of the rows of \farg{x} and \farg{y}} \fitem{real}{dot_self}{vector \farg{x}}{ The dot product of the vector \farg{x} with itself} \fitem{real}{dot_self}{row_vector \farg{x}}{ The dot product of the row vector \farg{x} with itself} \fitem{row_vector}{columns_dot_self}{vector \farg{x}}{ The dot product of the columns of \farg{x} with themselves} \fitem{row_vector}{columns_dot_self}{row_vector \farg{x}}{ The dot product of the columns of \farg{x} with themselves} \fitem{row_vector}{columns_dot_self}{matrix \farg{x}}{ The dot product of the columns of \farg{x} with themselves} \fitem{vector}{rows_dot_self}{vector \farg{x}}{ The dot product of the rows of \farg{x} with themselves} \fitem{vector}{rows_dot_self}{row_vector \farg{x}}{ The dot product of the rows of \farg{x} with themselves} \fitem{vector}{rows_dot_self}{matrix \farg{x}}{ The dot product of the rows of \farg{x} with themselves} \end{description}


\subsection{Specialized Products}


\begin{description} \fitem{matrix}{tcrossprod}{matrix \farg{x}}{ The product of \farg{x} postmultiplied by its own transpose, similar to the tcrossprod(x) function in R. The result is a symmetric matrix $\text{x}\,\text{x}^{\top}$.} \end{description}


\begin{description} \fitem{matrix}{crossprod}{matrix \farg{x}}{ The product of \farg{x} premultiplied by its own transpose, similar to the crossprod(x) function in R. The result is a symmetric matrix $\text{x}^{\top}\,\text{x}$.} \end{description}


The following functions all provide shorthand forms for common expressions, which are also much more efficient.


\begin{description} \fitem{matrix}{quad_form}{matrix \farg{A}, matrix \farg{B}}{   The quadratic form, i.e., \code{B' * A * B}.} \fitem{real}{quad_form}{matrix \farg{A}, vector \farg{B}}{   The quadratic form, i.e., \code{B' * A * B}.} \fitem{matrix}{quad_form_diag}{matrix \farg{m}, vector \farg{v}}   {The quadratic form using the column vector   \farg{v} as a diagonal matrix, i.e.,   \code{diag_matrix(\farg{v}) * \farg{m} * diag_matrix(\farg{v})}.} \fitem{matrix}{quad_form_diag}{matrix \farg{m}, row_vector \farg{rv}}   {The quadratic form using the row vector   \farg{rv} as a diagonal matrix, i.e.,   \code{diag_matrix(\farg{rv}) * \farg{m} * diag_matrix(\farg{rv})}.} \fitem{matrix}{quad_form_sym}{matrix \farg{A}, matrix \farg{B}}{   Similarly to quad_form, gives \code{B' * A * B}, but additionally   checks if A is symmetric and ensures that the result is also symmetric.} \fitem{real}{quad_form_sym}{matrix \farg{A}, vector \farg{B}}{   Similarly to quad_form, gives \code{B' * A * B}, but additionally   checks if A is symmetric and ensures that the result is also symmetric.} \fitem{real}{trace_quad_form}{matrix \farg{A}, matrix \farg{B}}{ The trace of the quadratic form, i.e., \code{trace(B' * A * B)}.} \fitem{real}{trace_gen_quad_form}{matrix \farg{D},matrix \farg{A}, matrix \farg{B}}{ The trace of a generalized quadratic form, i.e., \code{trace(D * B' * A * B).}} \end{description}


\begin{description} \fitem{matrix}{multiply_lower_tri_self_transpose}{matrix \farg{x}}{ The product of the lower triangular portion of \farg{x} (including the diagonal) times its own transpose;  that is, if \code{L} is a matrix of the same dimensions as \farg{x} with \code{L(m,n)} equal to   \code{\farg{x}(m,n)} for $\text{n} \leq \text{m}$ and \code{L(m,n)} equal to 0 if $\text{n} > \text{m}$, the result is the symmetric matrix $\text{L}\,\text{L}^{\top}$. This is a specialization of tcrossprod(x) for lower-triangular matrices.  The input matrix does not need to be square.} \end{description}


\begin{description}   \fitem{matrix}{diag_pre_multiply}{vector \farg{v}, matrix     \farg{m}}{Return the product of the diagonal matrix formed from     the vector \farg{v} and the matrix \farg{m}, i.e.,     \code{diag_matrix(\farg{v}) * \farg{m}}.}   \fitem{matrix}{diag_pre_multiply}{row_vector \farg{rv}, matrix \farg{m}}{Return     the product of the diagonal matrix formed from the vector     \farg{rv} and the matrix \farg{m}, i.e., \code{diag_matrix(\farg{rv}) * \farg{m}}.}   \fitem{matrix}{diag_post_multiply}{matrix \farg{m}, vector \farg{v}}{Return the     product of the matrix \farg{m} and the diagonal matrix formed from     the vector \farg{v}, i.e., \code{\farg{m} * diag_matrix(\farg{v})}.} \fitem{matrix}{diag_post_multiply}{matrix \farg{m}, row_vector \farg{rv}}{Return the   product of the matrix \code{\farg{m}} and the diagonal matrix formed from   the the row vector \code{\farg{rv}}, i.e., \code{\farg{m} * diag_matrix(\farg{rv})}.} \end{description}


\section{Reductions}


\subsection{Log Sum of Exponents}


\begin{description} \fitem{real}{log_sum_exp}{vector \farg{x}}{ The natural logarithm of the sum of the exponentials of the elements in \farg{x}} \fitem{real}{log_sum_exp}{row_vector \farg{x}}{ The natural logarithm of the sum of the exponentials of the elements in \farg{x}} \fitem{real}{log_sum_exp}{matrix \farg{x}}{ The natural logarithm of the sum of the exponentials of the elements in \farg{x}} \end{description}


\subsection{Minimum and Maximum}


\begin{description} \fitem{real}{min}{vector \farg{x}}{ The minimum value in \farg{x}, or $+\infty$ if \farg{x} is empty} \fitem{real}{min}{row_vector \farg{x}}{ The minimum value in \farg{x}, or $+\infty$ if \farg{x} is empty} \fitem{real}{min}{matrix \farg{x}}{ The minimum value in \farg{x}, or $+\infty$ if \farg{x} is empty} \fitem{real}{max}{vector \farg{x}}{ The maximum value in \farg{x}, or $-\infty$ if \farg{x} is empty} \fitem{real}{max}{row_vector \farg{x}}{ The maximum value in \farg{x}, or $-\infty$ if \farg{x} is empty} \fitem{real}{max}{matrix \farg{x}}{ The maximum value in \farg{x}, or $-\infty$ if \farg{x} is empty} \end{description}


\subsection{Sums and Products}


\begin{description} \fitem{real}{sum}{vector \farg{x}}{ The sum of the values in \farg{x}, or 0 if \farg{x} is empty} \fitem{real}{sum}{row_vector \farg{x}}{ The sum of the values in \farg{x}, or 0 if \farg{x} is empty} \fitem{real}{sum}{matrix \farg{x}}{ The sum of the values in \farg{x}, or 0 if \farg{x} is empty} \fitem{real}{prod}{vector \farg{x}}{ The product of the values in \farg{x}, or 1 if \farg{x} is empty} \fitem{real}{prod}{row_vector \farg{x}}{ The product of the values in \farg{x}, or 1 if \farg{x} is empty} \fitem{real}{prod}{matrix \farg{x}}{ The product of the values in \farg{x}, or 1 if \farg{x} is empty} \end{description}


\subsection{Sample Moments}


Full definitions are provided for sample moments in section \@ref(array-reductions).


\begin{description} \fitem{real}{mean}{vector \farg{x}}{ The sample mean of the values in \farg{x}; see section \@ref(array-reductions) for details.} \fitem{real}{mean}{row_vector \farg{x}}{ The sample mean of the values in \farg{x}; see section \@ref(array-reductions) for details.} \fitem{real}{mean}{matrix \farg{x}}{ The sample mean of the values in \farg{x}; see section \@ref(array-reductions) for details.} \fitem{real}{variance}{vector \farg{x}}{   The sample variance of the values in   \farg{x}; see section \@ref(array-reductions) for details.} \fitem{real}{variance}{row_vector \farg{x}}{   The sample variance of the values in   \farg{x}; see section \@ref(array-reductions) for details.} \fitem{real}{variance}{matrix \farg{x}}{ The sample variance of the values in \farg{x}; see section \@ref(array-reductions) for details.} \fitem{real}{sd}{vector \farg{x}}{ The sample standard deviation of the values in \farg{x};  see section \@ref(array-reductions) for details.} \fitem{real}{sd}{row_vector \farg{x}}{ The sample standard deviation of the values in \farg{x}; see section \@ref(array-reductions) for details.} \fitem{real}{sd}{matrix \farg{x}}{ The sample standard deviation of the values in \farg{x}; see section \@ref(array-reductions) for details.}% \end{description}


\section{Broadcast Functions}\label{matrix-broadcast.section}


The following broadcast functions allow vectors, row vectors and matrices to be created by copying a single element into all of their cells.  Matrices may also be created by stacking copies of row vectors vertically or stacking copies of column vectors horizontally.


\begin{description}   \fitem{vector}{rep_vector}{real \farg{x}, int \farg{m}}{Return the     size \farg{m} (column) vector consisting of copies of \farg{x}.}   \fitem{row_vector}{rep_row_vector}{real \farg{x}, int     \farg{n}}{Return the size \farg{n} row vector consisting of copies of     \farg{x}.}   \fitem{matrix}{rep_matrix}{real \farg{x}, int     \farg{m}, int \farg{n}}{Return the \farg{m} by \farg{n} matrix     consisting of copies of \farg{x}.}   \fitem{matrix}{rep_matrix}{vector \farg{v}, int \farg{n}}{Return     the \farg{m} by \farg{n} matrix consisting of \farg{n}     copies of the (column) vector \farg{v} of     size \farg{m}.}   \fitem{matrix}{rep_matrix}{row_vector \farg{rv}, int     \farg{m}}{Return the \farg{m} by \farg{n} matrix consisting of     \farg{m} copies of the row vector \farg{rv} of size \farg{n}.} \end{description}


Unlike the situation with array broadcasting (see section \@ref(array-broadcasting)), where there is a distinction between integer and real arguments, the following two statements produce the same result for vector broadcasting;  row vector and matrix broadcasting behave similarly.


```\n vector[3] x;\n x = rep_vector(1, 3);\n x = rep_vector(1.0, 3);\n ```


There are no integer vector or matrix types, so integer values are automatically promoted.


\section{Diagonal Matrix Functions}


\begin{description} \fitem{vector}{diagonal}{matrix \farg{x}}{The diagonal of the matrix \farg{x}} \fitem{matrix}{diag_matrix}{vector \farg{x}}{The diagonal matrix with diagonal \farg{x}} \end{description}


Although the \code{diag_matrix} function is available, it is unlikely to ever show up in an efficient Stan program.  For example, rather than converting a diagonal to a full matrix for use as a covariance matrix,


```\n y ~ multi_normal(mu, diag_matrix(square(sigma)));\n ```


it is much more efficient to just use a univariate normal, which produces the same density,


```\n y ~ normal(mu, sigma);\n ```


Rather than writing \code{m * diag_matrix(v)} where \code{m} is a matrix and \code{v} is a vector, it is much more efficient to write \code{diag_post_multiply(m, v)} (and similarly for pre-multiplication). By the same token, it is better to use \code{quad_form_diag(m, v)} rather than \code{quad_form(m, diag_matrix(v))}.


\section{Slicing and Blocking Functions}


Stan provides several functions for generating slices or blocks or diagonal entries for matrices.


\subsection{Columns and Rows}


\begin{description} \fitem{vector}{col}{matrix \farg{x}, int \farg{n}}{The \farg{n}-th column of matrix \farg{x}} \fitem{row_vector}{row}{matrix \farg{x}, int \farg{m}}{The \farg{m}-th row of matrix \farg{x}} \end{description}


The \code{row} function is special in that it may be used as an lvalue in an assignment statement (i.e., something to which a value may be assigned).  The row function is also special in that the indexing notation \code{x[m]} is just an alternative way of writing \code{row(x,m)}.  The \code{col} function may **not**, be used as an lvalue, nor is there an indexing based shorthand for it.


\subsection{Block Operations}


\subsubsection{Matrix Slicing Operations}


Block operations may be used to extract a sub-block of a matrix.


\begin{description} \fitem{matrix}{block}{matrix \farg{x}, int \farg{i}, int \farg{j},   int \farg{n_rows}, int \farg{n_cols}}{Return the submatrix of \farg{x} that   starts at row \farg{i} and column \farg{j} and extends \farg{n_rows}   rows and \farg{n_cols} columns.} \end{description}


The sub-row and sub-column operations may be used to extract a slice of row or column from a matrix


\begin{description} \fitem{vector}{sub_col}{matrix \farg{x}, int \farg{i}, int \farg{j},   int \farg{n_rows}}{Return the sub-column of \farg{x} that   starts at row \farg{i} and column \farg{j} and extends   \farg{n_rows} rows and 1 column.} \fitem{row_vector}{sub_row}{matrix \farg{x}, int \farg{i}, int \farg{j},   int \farg{n_cols}}{Return the sub-row of \farg{x} that   starts at row \farg{i} and column \farg{j} and extends 1   row and \farg{n_cols} columns.} \end{description}


\subsubsection{Vector and Array Slicing Operations}


The head operation extracts the first $n$ elements of a vector and the tail operation the last.  The segment operation extracts an arbitrary subvector.


\begin{description}   \fitem{vector}{head}{vector \farg{v}, int \farg{n}}{Return the     vector consisting of the first \farg{n} elements of \farg{v}.}   \fitem{row_vector}{head}{row_vector \farg{rv}, int \farg{n}}{Return     the row vector consisting of the first \farg{n} elements of     \farg{rv}.}   \fitem{T[]}{head}{T[] \farg{sv}, int \farg{n}}{Return     the array consisting of the first \farg{n} elements of     \farg{sv}; applies to up to three-dimensional arrays containing     any type of elements \code{T}.}   \fitem{vector}{tail}{vector \farg{v}, int \farg{n}}{Return the     vector consisting of the last \farg{n} elements of \farg{v}.}   \fitem{row_vector}{tail}{row_vector \farg{rv}, int \farg{n}}{Return     the row vector consisting of the last \farg{n} elements of     \farg{rv}.}   \fitem{T[]}{tail}{T[] \farg{sv}, int \farg{n}}{Return     the array consisting of the last \farg{n} elements of     \farg{sv}; applies to up to three-dimensional arrays containing     any type of elements \code{T}.}   \fitem{vector}{segment}{vector \farg{v}, int \farg{i}, int     \farg{n}}{Return the vector consisting of the \farg{n} elements of \farg{v}     starting at \farg{i}; i.e., elements \farg{i} through     through \farg{i} + \farg{n} - 1.}   \fitem{row_vector}{segment}{row_vector \farg{rv}, int \farg{i}, int     \farg{n}}{Return the row vector consisting of the \farg{n}     elements of \farg{rv} starting at \farg{i}; i.e., elements     \farg{i} through through \farg{i} + \farg{n} - 1.}   \fitem{T[]}{segment}{T[] \farg{sv}, int \farg{i}, int     \farg{n}}{Return the array consisting of the \farg{n}     elements of \farg{sv} starting at \farg{i}; i.e., elements     \farg{i} through through \farg{i} + \farg{n} - 1. Applies to up to     three-dimensional arrays containing any type of elements     \code{T}.} \end{description}


\section{Matrix Concatenation}\label{matrix-concatenation.section}


Stan's matrix concatenation operations \code{append_col} and \code{append_row} are like the operations \code{cbind} and \code{rbind} in R.


\subsubsection{Horizontal concatenation}


\begin{description}   \fitem{matrix}{append_col}{matrix \farg{x}, matrix     \farg{y}}{Combine matrices \farg{x} and \farg{y} by columns. The     matrices must have the same number of rows.} \fitem{matrix}{append_col}{matrix \farg{x}, vector \farg{y}}{Combine   matrix \farg{x} and vector \farg{y} by columns. The matrix and the   vector must have the same number of rows.} \fitem{matrix}{append_col}{vector \farg{x}, matrix \farg{y}}{Combine   vector \farg{x} and matrix \farg{y} by columns. The vector and the   matrix must have the same number of rows.} \fitem{matrix}{append_col}{vector \farg{x}, vector \farg{y}}{Combine   vectors \farg{x} and \farg{y} by columns. The vectors must have the   same number of rows.} \fitem{row_vector}{append_col}{row_vector \farg{x}, row_vector   \farg{y}}{Combine row vectors \farg{x} and \farg{y} of any size into   another row vector.} \fitem{row_vector}{append_col}{real \farg{x}, row_vector   \farg{y}}{Append \farg{x} to the front of \farg{y}, returning   another row vector.} \fitem{row_vector}{append_col}{row_vector \farg{x}, real \farg{y}}   {Append \farg{y} to the end of \farg{x}, returning   another row vector.} \end{description}


\subsubsection{Vertical concatenation}


\begin{description}   \fitem{matrix}{append_row}{matrix \farg{x}, matrix     \farg{y}}{Combine matrices \farg{x} and \farg{y} by rows. The     matrices must have the same number of columns.}   \fitem{matrix}{append_row}{matrix \farg{x}, row_vector     \farg{y}}{Combine matrix \farg{x} and row vector \farg{y} by     rows. The matrix and the row vector must have the same number     of columns.}   \fitem{matrix}{append_row}{row_vector \farg{x}, matrix     \farg{y}}{Combine row vector \farg{x} and matrix \farg{y} by     rows. The row vector and the matrix must have the same number     of columns.}   \fitem{matrix}{append_row}{row_vector \farg{x}, row_vector     \farg{y}}{Combine row vectors \farg{x} and \farg{y} by row. The     row vectors must have the same number of columns.}   \fitem{vector}{append_row}{vector \farg{x}, vector     \farg{y}}{Concatenate vectors \farg{x} and \farg{y} of any size into     another vector.} \fitem{vector}{append_row}{real \farg{x}, vector   \farg{y}}{Append \farg{x} to the top of \farg{y}, returning   another vector.} \fitem{vector}{append_row}{vector \farg{x}, real \farg{y}}   {Append \farg{y} to the bottom of \farg{x}, returning   another vector.} \end{description}


\section{Special Matrix Functions}\label{softmax.section}


\subsection{Softmax}


The softmax function maps[^fnsoftmax] $y \in \mathbb{R}^K$ to the $K$-simplex by \[ \text{softmax}(y)  = \frac{\exp(y)}         {\sum_{k=1}^K \exp(y_k)}, \] where $\exp(y)$ is the componentwise exponentiation of $y$. Softmax is usually calculated on the log scale, \begin{eqnarray*} \log \text{softmax}(y) & = & \ y - \log \sum_{k=1}^K \exp(y_k) \\[4pt] & = & y - \mathrm{log\_sum\_exp}(y). \end{eqnarray*} where the vector $y$ minus the scalar $\mathrm{log\_sum\_exp}(y)$ subtracts the scalar from each component of $y$.

[^fnsoftmax]: The softmax function is so called because in the limit as   $y_n \rightarrow \infty$ with $y_m$ for $m \neq n$ held constant,   the result tends toward the "one-hot" vector $\theta$ with   $\theta_n = 1$ and $\theta_m = 0$ for $m \neq n$, thus providing a   "soft" version of the maximum function.

Stan provides the following functions for softmax and its log.

\begin{description} \fitem{vector}{softmax}{vector \farg{x}}{ The softmax of \farg{x}} \fitem{vector}{log_softmax}{vector \farg{x}}{ The natural logarithm of the softmax of \farg{x}} \end{description}

\subsection{Cumulative Sums}

The cumulative sum of a sequence $x_1,\ldots,x_N$ is the sequence $y_1,\ldots,y_N$, where \[ y_n = \sum_{m = 1}^{n} x_m. \] 

\begin{description} \fitem{real[]}{cumulative_sum}{real[] \farg{x}}{ The cumulative sum of \farg{x}} \fitem{vector}{cumulative_sum}{vector \farg{v}}{ The cumulative sum of \farg{v}} \fitem{row_vector}{cumulative_sum}{row_vector \farg{rv}}{ The cumulative sum of \farg{rv}} \end{description}


\section{Covariance Functions}\label{covariance.section}


\subsection{Exponentiated quadratic covariance function}


The exponentiated quadratic kernel defines the covariance between $f(x_i)$ and $f(x_j)$ where $f\colon \mathbb{R}^D \mapsto \mathbb{R}$ as a function of the squared Euclidian distance between $x_i \in \mathbb{R}^D$ and $x_j \in \mathbb{R}^D$: \[   \text{cov}(f(x_i), f(x_j)) = k(x_i, x_j) = \alpha^2 \exp \left( 	- \dfrac{1}{2\rho^2} \sum_{d=1}^D (x_{i,d} - x_{j,d})^2 \right) \] with $\alpha$ and $\rho$ constrained to be positive.


There are two variants of the exponentiated quadratic covariance function in Stan. One builds a covariance matrix, $K \in \mathbb{R}^{N \times N}$ for $x_1, \dots, x_N$, where $K_{i,j} = k(x_i, x_j)$, which is necessarily symmetric and positive semidefinite by construction. There is a second variant of the exponentiated quadratic covariance function that builds a $K \in \mathbb{R}^{N \times M}$ covariance matrix for $x_1, \dots, x_N$ and $x^\prime_1, \dots, x^\prime_M$, where $x_i \in \mathbb{R}^D$ and $x^\prime_i \in \mathbb{R}^D$ and $K_{i,j} = k(x_i, x^\prime_j)$.


\begin{description} \fitem{matrix}{cov_exp_quad}{row_vectors \farg{x}, real \farg{alpha}, real \farg{rho}}{ The covariance matrix with an exponentiated quadratic kernel of \farg{x}.} \fitem{matrix}{cov_exp_quad}{vectors \farg{x}, real \farg{alpha}, real \farg{rho}}{ The covariance matrix with an exponentiated quadratic kernel of \farg{x}.} \fitem{matrix}{cov_exp_quad}{real[] \farg{x}, real \farg{alpha}, real \farg{rho}}{ The covariance matrix with an exponentiated quadratic kernel of \farg{x}.} \fitemtwolines{matrix}{cov_exp_quad}{row_vectors \farg{x1}, row_vectors \farg{x2}}{real \farg{alpha}, real \farg{rho}}{ The covariance matrix with an exponentiated quadratic kernel of \farg{x1} and \farg{x2}.} \fitemtwolines{matrix}{cov_exp_quad}{vectors \farg{x1}, vectors \farg{x2}}{real \farg{alpha}, real \farg{rho}}{ The covariance matrix with an exponentiated quadratic kernel of \farg{x1} and \farg{x2}.} \fitemtwolines{matrix}{cov_exp_quad}{real[] \farg{x1}, real[] \farg{x2}}{real \farg{alpha}, real \farg{rho}}{ The covariance matrix with an exponentiated quadratic kernel of \farg{x1} and \farg{x2}.} \end{description}


\section{Linear Algebra Functions and Solvers}


\subsection{Matrix Division Operators and Functions}


In general, it is much more efficient and also more arithmetically stable to use matrix division than to multiply by an inverse.  There are specialized forms for lower triangular matrices and for symmetric, positive-definite matrices.


\subsubsection{Matrix division operators}


\begin{description} \fitem{row_vector}{operator/}{row_vector \farg{b}, matrix \farg{A}}{ The right division of \farg{b} by \farg{A}; equivalently \code{\farg{b} * inverse(\farg{A})}} \fitem{matrix}{operator/}{matrix \farg{B}, matrix \farg{A}}{ The right division of \farg{B} by \farg{A}; equivalently \code{\farg{B} * inverse(\farg{A})}} \fitem{vector}{operator\}{matrix \farg{A}, vector \farg{b}} The left division of \farg{b} by \farg{A}; equivalently \code{inverse(\farg{A}) * \farg{b}} \fitem{matrix}{operator\}{matrix \farg{A}, matrix \farg{B}} The left division of \farg{B} by \farg{A}; equivalently \code{inverse(\farg{A}) * \farg{B}} \end{description}


\subsubsection{Lower-triangular matrix division functions}


There are four division functions which use lower triangular views of a matrix.  The lower triangular view of a matrix $\text{tri}(A)$ is used in the definitions and defined by \[ \text{tri}(A)[m,n] = \left\{ \begin{array}{ll} A[m,n] & \text{if } m \geq n, \text{ and} \\[4pt] 0 & \text{otherwise}. \end{array} \right. \] When a lower triangular view of a matrix is used, the elements above the diagonal are ignored.


\begin{description} \fitem{vector}{mdivide_left_tri_low}{matrix \farg{A}, vector \farg{b}}{ The left division of \farg{b} by a lower-triangular view of \farg{A}; algebraically equivalent to the less efficient and stable form \code{inverse(tri(\farg{A})) * \farg{b}}, where \code{tri(\farg{A})} is the lower-triangular portion of \farg{A} with the above-diagonal entries set to zero.} \fitem{matrix}{mdivide_left_tri_low}{matrix \farg{A}, matrix \farg{B}}{ The left division of \farg{B} by a triangular view of \farg{A}; algebraically equivalent to the less efficient and stable form \code{inverse(tri(\farg{A})) * \farg{B}}, where \code{tri(\farg{A})} is the lower-triangular portion of \farg{A} with the above-diagonal entries set to zero.} \fitem{row_vector}{mdivide_right_tri_low}{row_vector \farg{b},   matrix \farg{A}}{ The right division of \farg{b} by a triangular view of \farg{A};  algebraically equivalent to the less efficient and stable form \code{\farg{b} * inverse(tri(\farg{A}))},  where \code{tri(\farg{A})} is the lower-triangular portion of \farg{A} with the above-diagonal entries set to zero.} \fitem{matrix}{mdivide_right_tri_low}{matrix \farg{B}, matrix   \farg{A}}{ The right division of \farg{B} by a triangular view of \farg{A};  algebraically equivalent to the less efficient and stable form \code{\farg{B} * inverse(tri(\farg{A}))},  where \code{tri(\farg{A})} is the lower-triangular portion of \farg{A} with the above-diagonal entries set to zero.} \end{description}


\subsection{Symmetric positive-definite matrix division functions}


There are four division functions which are specialized for efficiency and stability for symmetric positive-definite matrix dividends.  If the matrix dividend argument is not symmetric and positive definite, these will reject and print warnings.


\begin{description} \fitem{matrix}{mdivide_left_spd}{matrix \farg{A}, vector \farg{b}}{ The left division of \farg{b} by the symmetric, positive-definite matrix \farg{A}; algebraically equivalent to the less efficient and stable form \code{inverse(\farg{A}) * \farg{b}}.} \fitem{vector}{mdivide_left_spd}{matrix \farg{A}, matrix \farg{B}}{ The left division of \farg{B} by the symmetric, positive-definite matrix \farg{A}; algebraically equivalent to the less efficient and stable form \code{inverse(\farg{A}) * B}.} \fitem{row_vector}{mdivide_right_spd}{row_vector \farg{b},   matrix \farg{A}}{ The right division of \farg{b} by the symmetric, positive-definite matrix \farg{A};  algebraically equivalent to the less efficient and stable form \code{\farg{b} * inverse(\farg{A})}.} \fitem{matrix}{mdivide_right_spd}{matrix \farg{B}, matrix \farg{A}}{ The right division of \farg{B} by the symmetric, positive-definite matrix \farg{A};  algebraically equivalent to the less efficient and stable form \code{\farg{B} * inverse(\farg{A})}.} \end{description}


\subsection{Matrix Exponential}


The exponential of the matrix $A$ is formally defined by the convergent power series: \[ e^A = \sum_{n=0}^{\infty} \dfrac{A^n}{n!} \] 

\begin{description} \fitem{matrix}{matrix_exp}{matrix \farg{A}}{ The matrix exponential of \farg{A}} \fitem{matrix}{matrix_exp_multiply}{matrix \farg{A}, matrix \farg{B}}{ The multiplication of matrix exponential of \farg{A} and matrix \farg{B}; algebraically equivalent to the less efficient form \code{matrix_exp(\farg{A}) * \farg{B}}.} \fitem{matrix}{scale_matrix_exp_multiply}{real \farg{t}, matrix \farg{A}, matrix \farg{B}}{ The multiplication of matrix exponential of \farg{t}\farg{A} and matrix \farg{B}; algebraically equivalent to the less efficient form \code{matrix_exp(\farg{t}\farg{A}) * \farg{B}}.} \end{description}


\subsection{Linear Algebra Functions}


\subsubsection{Trace}


\begin{description} \fitem{real}{trace}{matrix \farg{A}}{ The trace of \farg{A}, or 0 if \farg{A} is empty;  \farg{A} is not required to be diagonal} \end{description}


\subsubsection{Determinants}


\begin{description} \fitem{real}{determinant}{matrix \farg{A}}{ The determinant of \farg{A}} \fitem{real}{log_determinant}{matrix \farg{A}}{ The log of the absolute value of the determinant of \farg{A}} \end{description}


\subsubsection{Inverses}


It is almost never a good idea to use matrix inverses directly because they are both inefficient and arithmetically unstable compared to the alternatives.  Rather than inverting a matrix \code{m} and post-multiplying by a vector or matrix \code{a}, as in \code{inverse(m) * a}, it is better to code this using matrix division, as in \code{m \ a}.  The pre-multiplication case is similar, with \code{b * inverse(m)} being more efficiently coded as as \code{b / m}.  There are also useful special cases for triangular and symmetric, positive-definite matrices that use more efficient solvers.


 _**Warning:**_   The function \code{inv(m)} is the elementwise inverse function, which returns \code{1 / m[i, j]} for each element.


\begin{description} \fitem{matrix}{inverse}{matrix \farg{A}}{ The inverse of \farg{A}} \fitem{matrix}{inverse_spd}{matrix \farg{A}}{ The inverse of \farg{A} where A is symmetric, positive definite.  This version is faster and more arithmetically stable when the input is symmetric and positive definite.} \end{description}


\subsubsection{Eigendecomposition}


\begin{description} \fitem{vector}{eigenvalues_sym}{matrix \farg{A}}{ The vector of eigenvalues of a symmetric matrix \farg{A} in ascending order} \fitem{matrix}{eigenvectors_sym}{matrix \farg{A}}{ The matrix with   the (column) eigenvectors of symmetric matrix \farg{A} in the same   order as returned by the function \code{eigenvalues_sym}} \end{description}


Because multiplying an eigenvector by $-1$ results in an eigenvector, eigenvectors returned by a decomposition are only identified up to a sign change.  In order to compare the eigenvectors produced by Stan's eigendecomposition to others, signs may need to be normalized in some way, such as by fixing the sign of a component, or doing comparisons allowing a multiplication by $-1$.


The condition number of a symmetric matrix is defined to be the ratio of the largest eigenvalue to the smallest eigenvalue.  Large condition numbers lead to difficulty in numerical algorithms such as computing inverses, and thus known as "ill conditioned."  The ratio can even be infinite in the case of singular matrices (i.e., those with eigenvalues of 0).


\subsubsection{QR Decomposition}\label{QR-decomposition}


\begin{description} \fitem{matrix}{qr_thin_Q}{matrix \farg{A}}{ The orthogonal matrix in the thin QR decomposition of \farg{A}, which implies that the resulting matrix has the same dimensions as \farg{A}} \fitem{matrix}{qr_thin_R}{matrix \farg{A}}{   The upper triangular matrix in the thin QR decomposition of \farg{A},   which implies that the resulting matrix is square with the same   number of columns as \farg{A}} \fitem{matrix}{qr_Q}{matrix \farg{A}}{ The orthogonal matrix in the fat QR decomposition of \farg{A}, which implies that the resulting matrix is square with the same number of rows as \farg{A}} \fitem{matrix}{qr_R}{matrix \farg{A}}{   The upper trapezoidal matrix in the fat QR decomposition of \farg{A},   which implies that the resulting matrix will be square with the same    number of rows as \farg{A}} \end{description}


The thin QR decomposition is always preferable because it will consume much less memory when the input matrix is large than will the fat QR  decomposition. Both versions of the decomposition represent the input matrix as \[ A = Q \, R. \] Multiplying a column of an orthogonal matrix by $-1$ still results in an orthogonal matrix, and you can multiply the corresponding row of the upper trapezoidal matrix by $-1$ without changing the product. Thus, Stan adopts the normalization that the diagonal elements of the upper trapezoidal matrix are strictly positive and the columns of the orthogonal matrix are reflected if necessary. Also, these QR  decomposition algorithms do not utilize pivoting and thus may be  numerically unstable on input matrices that have less than full rank.


\subsubsection{Cholesky Decomposition}


Every symmetric, positive-definite matrix (such as a correlation or covariance matrix) has a Cholesky decomposition.  If $\Sigma$ is a symmetric, positive-definite matrix, its Cholesky decomposition is the lower-triangular vector $L$ such that \[ \Sigma = L \, L^{\top}. \] 

\begin{description} \fitem{matrix}{cholesky_decompose}{matrix \farg{A}}{ The lower-triangular Cholesky factor of the symmetric positive-definite matrix \farg{A}} \end{description}


\subsubsection{Singular Value Decomposition}


Stan only provides functions for the singular values, not for the singular vectors involved in a singular value decomposition (SVD).


\begin{description} \fitem{vector}{singular_values}{matrix \farg{A}}{ The singular values of \farg{A} in descending order} \end{description}


\section{Sort Functions}


see section \@ref(sorting-functions) for examples of how the functions work.


\begin{description} \fitem{vector}{sort_asc}{vector \farg{v}}{ Sort the elements of \farg{v} in ascending order} \fitem{row_vector}{sort_asc}{row_vector \farg{v}}{ Sort the elements of \farg{v} in ascending order} \fitem{vector}{sort_desc}{vector \farg{v}}{ Sort the elements of \farg{v} in descending order} \fitem{row_vector}{sort_desc}{row_vector \farg{v}}{ Sort the elements of \farg{v} in descending order} \fitem{int[]}{sort_indices_asc}{vector \farg{v}}{ Return an array of indices between 1 and the size of \farg{v}, sorted to index \farg{v} in ascending order.} \fitem{int[]}{sort_indices_asc}{row_vector \farg{v}}{ Return an array of indices between 1 and the size of \farg{v}, sorted to index \farg{v} in ascending order.} \fitem{int[]}{sort_indices_desc}{vector \farg{v}}{ Return an array of indices between 1 and the size of \farg{v}, sorted to index \farg{v} in descending order.} \fitem{int[]}{sort_indices_desc}{row_vector \farg{v}}{ Return an array of indices between 1 and the size of \farg{v}, sorted to index \farg{v} in descending order.} \fitem{int}{rank}{vector \farg{v}, int \farg{s}}{ Number of components of \farg{v} less than \farg{v[s]}} \fitem{int}{rank}{row_vector \farg{v}, int \farg{s}}{ Number of components of \farg{v} less than \farg{v[s]}} \end{description}


\chapter{Sparse Matrix Operations}\label{sparse-matrices.chapter}


For sparse matrices, for which many elements are zero, it is more efficient to use specialized representations to save memory and speed up matrix arithmetic (including derivative calculations).  Given Stan's implementation, there is substantial space (memory) savings by using sparse matrices.  Because of the ease of optimizing dense matrix operations, speed improvements only arise at 90\% or even greater sparsity; below that level, dense matrices are faster but use more memory.


Because of this speedup and space savings, it may even be useful to read in a dense matrix and convert it to a sparse matrix before multiplying it by a vector.  This chapter covers a very specific form of sparsity consisting of a sparse matrix multiplied by a dense vector.


\section{Compressed Row Storage}\label{CSR.section}


Sparse matrices are represented in Stan using compressed row storage (CSR).  For example, the matrix \[ A = \begin{bmatrix} 19 & 27 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 52 \\ 81 & 0 & 95 & 33 \end{bmatrix} \] is translated into a vector of the non-zero real values, read by row from the matrix $A$, \[ w(A) = \begin{bmatrix} 19 & 27 & 52 & 81 & 95 & 33 \end{bmatrix}^{\top} \! \! \! , \] an array of integer column indices for the values, \[ v(A) = \begin{bmatrix} 1 & 2 & 4 & 1 & 3 & 4 \end{bmatrix} \! , \] and an array of integer indices indicating where in $w(A)$ a given row's values start, \[ u(A) = \begin{bmatrix} 1 & 3 & 3 & 4 & 7 \end{bmatrix} \! , \] with a padded value at the end to guarantee that \[ u(A)[n+1] - u(A)[n] \] is the number of non-zero elements in row $n$ of the matrix (here $2$, $0$, $1$, and $3$). Note that because the second row has no non-zero elements both the second and third elements of $u(A)$ correspond to the third element of $w(A)$, which is $52$. The values $(w(A), \, v(A), \, u(A))$ are sufficient to reconstruct $A$.


The values are structured so that there is a real value and integer column index for each non-zero entry in the array, plus one integer for each row of the matrix, plus one for padding.  There is also underlying storage for internal container pointers and sizes.  The total memory usage is roughly $12 K + M$ bytes plus a small constant overhead, which is often considerably fewer bytes than the $M \times N$ required to store a dense matrix.  Even more importantly, zero values do not introduce derivatives under multiplication or addition, so many storage and evaluation steps are saved when sparse matrices are multiplied.


\section{Conversion Functions}


Conversion functions between dense and sparse matrices are provided.


\subsection{Dense to Sparse Conversion}


Converting a dense matrix $m$ to a sparse representation produces a vector $w$ and two integer arrays, $u$ and $v$.


\begin{description}   \fitem{vector}{csr_extract_w}{matrix \farg{a}}{ Return non-zero     values in matrix \farg{a}; see section \@ref(CSR).}   \fitem{int[]}{csr_extract_v}{matrix \farg{a}}{Return column     indices for values in \code{csr_extract_w(\farg{a})}; see     \@ref(CSR).}   \fitem{int[]}{csr_extract_u}{matrix \farg{a}}{Return array of row     starting indices for entries in \code{csr_extract_w(\farg{a})}     followed by the size of \code{csr_extract_w(\farg{a})} plus one;     see section \@ref(CSR).} \end{description}


\subsection{Sparse to Dense Conversion}


To convert a sparse matrix representation to a dense matrix, there is a single function.


\begin{description}   \fitem{matrix}{csr_to_dense_matrix}{int \farg{m}, int \farg{n},     vector \farg{w}, int[] \farg{v}, int[] \farg{u}}{Return dense     $\text{\farg{m}} \times \text{\farg{n}}$ matrix with non-zero     matrix entries \farg{w}, column indices \farg{v}, and row starting     indices \farg{u}; the vector \farg{w} and arrays \farg{v} and     \farg{u} must all be the same size, and the arrays \farg{v} and     \farg{u} must have index values bounded by \farg{m} and \farg{n}.     see section \@ref(CSR) for more details.} \end{description}


\section{Sparse Matrix Arithmetic}


\subsection{Sparse Matrix Multiplication}


The only supported operation is the multiplication of a sparse matrix $A$ and a dense vector $b$ to produce a dense vector $A\,b$. Multiplying a dense row vector $b$ and a sparse matrix $A$ can be coded using transposition as \[ b \, A = (A^{\top} \, b^{\top})^{\top}, \] but care must be taken to represent $A^{\top}$ rather than $A$ as a sparse matrix.


\begin{description}   \fitemtwolines{vector}{csr_matrix_times_vector}{int \farg{m}, int     \farg{n}, vector \farg{w}}{int[] \farg{v}, int[] \farg{u}, vector     \farg{b}}{ Multiply the $\text{\farg{m}} \times \text{\farg{n}}$     matrix represented by values \farg{w}, column indices \farg{v},     and row start indices \farg{u} by the vector \farg{b}; see     \@ref(CSR).} \end{description}


\chapter{Mixed Operations}\label{mixed-operations.chapter}


These functions perform conversions between Stan containers matrix, vector, row vector and arrays.


\begin{description} \fitem{matrix}{to_matrix}{matrix \farg{m}}{ Return the matrix \farg{m} itself.} \fitem{matrix}{to_matrix}{vector \farg{v}}{ Convert the column vector \farg{v} to a \code{size(\farg{v})} by 1 matrix.} \fitem{matrix}{to_matrix}{row_vector \farg{v}}{ Convert the row vector \farg{v} to a 1 by \code{size(\farg{v})} matrix.} \fitem{matrix}{to_matrix}{matrix \farg{m}, int \farg{m}, int \farg{n}}{ Convert a matrix \farg{m} to a matrix with \farg{m} rows and \farg{n} columns filled in column-major order. } \fitem{matrix}{to_matrix}{vector \farg{v}, int \farg{m}, int \farg{n}}{ Convert a vector \farg{v} to a matrix with \farg{m} rows and \farg{n} columns filled in column-major order. } \fitem{matrix}{to_matrix}{row_vector \farg{v}, int \farg{m},                            int \farg{n}}{ Convert a row_vector \farg{a} to a matrix with \farg{m} rows and \farg{n} columns filled in column-major order. } \fitem{matrix}{to_matrix}{matrix \farg{m}, int \farg{m}, int \farg{n},                            int col_major}{ Convert a matrix \farg{m} to a matrix with \farg{m} rows and \farg{n} columns filled in row-major order if col_major equals 0 (otherwise, they get filled in column-major order). } \fitem{matrix}{to_matrix}{vector \farg{v}, int \farg{m}, int \farg{n},                            int col_major}{ Convert a vector \farg{v} to a matrix with \farg{m} rows and \farg{n} columns filled in row-major order if col_major equals 0 (otherwise, they get filled in column-major order). } \fitem{matrix}{to_matrix}{row_vector \farg{v}, int \farg{m},                            int \farg{n}, int col_major}{ Convert a row_vector \farg{a} to a matrix with \farg{m} rows and \farg{n} columns filled in row-major order if col_major equals 0 (otherwise, they get filled in column-major order). } \fitem{matrix}{to_matrix}{real[] \farg{a}, int \farg{m}, int \farg{n}}{ Convert a one-dimensional array \farg{a} to a matrix with \farg{m} rows and \farg{n} columns filled in column-major order. } \fitem{matrix}{to_matrix}{int[] \farg{a}, int \farg{m}, int \farg{n}}{ Convert a one-dimensional array \farg{a} to a matrix with \farg{m} rows and \farg{n} columns filled in column-major order. } \fitem{matrix}{to_matrix}{real[] \farg{a}, int \farg{m}, int \farg{n},                            int col_major}{ Convert a one-dimensional array \farg{a} to a matrix with \farg{m} rows and \farg{n} columns filled in row-major order if col_major equals 0 (otherwise, they get filled in column-major order). } \fitem{matrix}{to_matrix}{int[] \farg{a}, int \farg{m}, int \farg{n},                            int col_major}{ Convert a one-dimensional array \farg{a} to a matrix with \farg{m} rows and \farg{n} columns filled in row-major order if col_major equals 0 (otherwise, they get filled in column-major order). } \fitem{matrix}{to_matrix}{real[,] \farg{a}}{ Convert the two dimensional array \farg{a} to a matrix with the same dimensions and indexing order.} \fitem{matrix}{to_matrix}{int[,] \farg{a}}{ Convert the two   dimensional array \farg{a} to a matrix with the same dimensions and   indexing order.  If any of the dimensions of \farg{a} are zero, the   result will be a $0 \times 0$ matrix.} \fitem{vector}{to_vector}{matrix \farg{m}}{ Convert the matrix \farg{m} to a column vector in column-major order.} \fitem{vector}{to_vector}{vector \farg{v}}{ Return the column vector \farg{v} itself.} \fitem{vector}{to_vector}{row_vector \farg{v}}{ Convert the row vector \farg{v} to a column vector.} \fitem{vector}{to_vector}{real[] \farg{a}}{ Convert the one-dimensional array \farg{a} to a column vector.} \fitem{vector}{to_vector}{int[] \farg{a}}{ Convert the one-dimensional integer array \farg{a} to a column vector.} \fitem{row_vector}{to_row_vector}{matrix \farg{m}}{ Convert the matrix \farg{m} to a row vector in column-major order.} \fitem{row_vector}{to_row_vector}{vector \farg{v}}{ Convert the column vector \farg{v} to a row vector.} \fitem{row_vector}{to_row_vector}{row_vector \farg{v}}{ Return the row vector \farg{v} itself.} \fitem{row_vector}{to_row_vector}{real[] \farg{a}}{ Convert the one-dimensional array \farg{a} to a row vector.} \fitem{row_vector}{to_row_vector}{int[] \farg{a}}{ Convert the one-dimensional array \farg{a} to a row vector.} \fitem{real[,]}{to_array_2d}{matrix \farg{m}}{ Convert the matrix \farg{m} to a two dimensional array with the same dimensions and indexing order.} \fitem{real[]}{to_array_1d}{vector \farg{v}}{ Convert the column vector \farg{v} to a one-dimensional array.} \fitem{real[]}{to_array_1d}{row_vector \farg{v}}{ Convert the row vector \farg{v} to a one-dimensional array.} \fitem{real[]}{to_array_1d}{matrix \farg{m}}{ Convert the matrix \farg{m} to a one-dimensional array in column-major order.} \fitem{real[]}{to_array_1d}{real[...] \farg{a}}{ Convert the array \farg{a} (of any dimension up to 10) to a one-dimensional array in row-major order.} \fitem{int[]}{to_array_1d}{int[...] \farg{a}}{ Convert the array \farg{a} (of any dimension up to 10) to a one-dimensional array in row-major order.} \end{description}


\chapter{Compound Arithmetic and Assignment}


Compound arithmetic and assignment statements combine an arithmetic operation and assignment,


```\n x = x op y;\n ```


replacing them with the compound form


```\n x op= y;\n ```


For example, \code{x = x + 1} may be replaced with \code{x += 1}.


The signatures of the supported compound arithmetic and assignment operations are as follows.


\section{Compound Addition and Assignment}


\begin{description} \fitem{void}{operator+=}{int \farg{x}, int \farg{y}}{\code{\farg{x} +=     y} is equivalent to \code{\farg{x} = \farg{x} + \farg{y}}.} \fitem{void}{operator+=}{real \farg{x}, real \farg{y}}{\code{\farg{x} +=     y} is equivalent to \code{\farg{x} = \farg{x} + \farg{y}}.} \fitem{void}{operator+=}{vector \farg{x}, real \farg{y}}{\code{\farg{x} +=     y} is equivalent to \code{\farg{x} = \farg{x} + \farg{y}}.} \fitem{void}{operator+=}{row_vector \farg{x}, real \farg{y}}{\code{\farg{x} +=     y} is equivalent to \code{\farg{x} = \farg{x} + \farg{y}}.} \fitem{void}{operator+=}{matrix \farg{x}, real \farg{y}}{\code{\farg{x} +=     y} is equivalent to \code{\farg{x} = \farg{x} + \farg{y}}.} \fitem{void}{operator+=}{vector \farg{x}, vector \farg{y}}{\code{\farg{x} +=     y} is equivalent to \code{\farg{x} = \farg{x} + \farg{y}}.} \fitem{void}{operator+=}{row_vector \farg{x}, row_vector \farg{y}}{\code{\farg{x} +=     y} is equivalent to \code{\farg{x} = \farg{x} + \farg{y}}.} \fitem{void}{operator+=}{matrix \farg{x}, matrix \farg{y}}{\code{\farg{x} +=     y} is equivalent to \code{\farg{x} = \farg{x} + \farg{y}}.} \end{description}


\section{Compound Subtraction and Assignment}


\begin{description} \fitem{void}{operator-=}{int \farg{x}, int \farg{y}}{\code{\farg{x} -=     y} is equivalent to \code{\farg{x} = \farg{x} - \farg{y}}.} \fitem{void}{operator-=}{real \farg{x}, real \farg{y}}{\code{\farg{x} -=     y} is equivalent to \code{\farg{x} = \farg{x} - \farg{y}}.} \fitem{void}{operator-=}{vector \farg{x}, real \farg{y}}{\code{\farg{x} -=     y} is equivalent to \code{\farg{x} = \farg{x} - \farg{y}}.} \fitem{void}{operator-=}{row_vector \farg{x}, real \farg{y}}{\code{\farg{x} -=     y} is equivalent to \code{\farg{x} = \farg{x} - \farg{y}}.} \fitem{void}{operator-=}{matrix \farg{x}, real \farg{y}}{\code{\farg{x} -=     y} is equivalent to \code{\farg{x} = \farg{x} - \farg{y}}.} \fitem{void}{operator-=}{vector \farg{x}, vector \farg{y}}{\code{\farg{x} -=     y} is equivalent to \code{\farg{x} = \farg{x} - \farg{y}}.} \fitem{void}{operator-=}{row_vector \farg{x}, row_vector \farg{y}}{\code{\farg{x} -=     y} is equivalent to \code{\farg{x} = \farg{x} - \farg{y}}.} \fitem{void}{operator-=}{matrix \farg{x}, matrix \farg{y}}{\code{\farg{x} -=     y} is equivalent to \code{\farg{x} = \farg{x} - \farg{y}}.} \end{description}


\section{Compound Multiplication and Assignment}


\begin{description} \fitem{void}{operator*=}{int \farg{x}, int \farg{y}}{\code{\farg{x} *=     y} is equivalent to \code{\farg{x} = \farg{x} * \farg{y}}.} \fitem{void}{operator*=}{real \farg{x}, real \farg{y}}{\code{\farg{x} *=     y} is equivalent to \code{\farg{x} = \farg{x} * \farg{y}}.} \fitem{void}{operator*=}{vector \farg{x}, real \farg{y}}{\code{\farg{x} *=     y} is equivalent to \code{\farg{x} = \farg{x} * \farg{y}}.} \fitem{void}{operator*=}{row_vector \farg{x}, real \farg{y}}{\code{\farg{x} *=     y} is equivalent to \code{\farg{x} = \farg{x} * \farg{y}}.} \fitem{void}{operator*=}{matrix \farg{x}, real \farg{y}}{\code{\farg{x} *=     y} is equivalent to \code{\farg{x} = \farg{x} * \farg{y}}.} \fitem{void}{operator*=}{row_vector \farg{x}, matrix \farg{y}}{\code{\farg{x} *=     y} is equivalent to \code{\farg{x} = \farg{x} * \farg{y}}.} \fitem{void}{operator*=}{matrix \farg{x}, matrix \farg{y}}{\code{\farg{x} *=     y} is equivalent to \code{\farg{x} = \farg{x} * \farg{y}}.} \end{description}


\section{Compound Division and Assignment}


\begin{description} \fitem{void}{operator/=}{int \farg{x}, int \farg{y}}{\code{\farg{x} /=     y} is equivalent to \code{\farg{x} = \farg{x} / \farg{y}}.} \fitem{void}{operator/=}{real \farg{x}, real \farg{y}}{\code{\farg{x} /=     y} is equivalent to \code{\farg{x} = \farg{x} / \farg{y}}.} \fitem{void}{operator/=}{vector \farg{x}, real \farg{y}}{\code{\farg{x} /=     y} is equivalent to \code{\farg{x} = \farg{x} / \farg{y}}.} \fitem{void}{operator/=}{row_vector \farg{x}, real \farg{y}}{\code{\farg{x} /=     y} is equivalent to \code{\farg{x} = \farg{x} / \farg{y}}.} \fitem{void}{operator/=}{matrix \farg{x}, real \farg{y}}{\code{\farg{x} /=     y} is equivalent to \code{\farg{x} = \farg{x} / \farg{y}}.} \end{description}


\section{Compound Elementwise Multiplication and Assignment}


\begin{description} \fitem{void}{operator.*=}{vector \farg{x}, vector \farg{y}}{\code{\farg{x} .*=     y} is equivalent to \code{\farg{x} = \farg{x} .* \farg{y}}.} \fitem{void}{operator.*=}{row_vector \farg{x}, row_vector \farg{y}}{\code{\farg{x} .*=     y} is equivalent to \code{\farg{x} = \farg{x} .* \farg{y}}.} \fitem{void}{operator.*=}{matrix \farg{x}, matrix \farg{y}}{\code{\farg{x} .*=     y} is equivalent to \code{\farg{x} = \farg{x} .* \farg{y}}.} \end{description}


\section{Compound Elementwise Division and Assignment}


\begin{description} \fitem{void}{operator./=}{vector \farg{x}, vector \farg{y}}{\code{\farg{x} ./=     y} is equivalent to \code{\farg{x} = \farg{x} ./ \farg{y}}.} \fitem{void}{operator./=}{row_vector \farg{x}, row_vector \farg{y}}{\code{\farg{x} ./=     y} is equivalent to \code{\farg{x} = \farg{x} ./ \farg{y}}.} \fitem{void}{operator./=}{matrix \farg{x}, matrix \farg{y}}{\code{\farg{x} ./=     y} is equivalent to \code{\farg{x} = \farg{x} ./ \farg{y}}.} \fitem{void}{operator./=}{vector \farg{x}, real \farg{y}}{\code{\farg{x} ./=     y} is equivalent to \code{\farg{x} = \farg{x} ./ \farg{y}}.} \fitem{void}{operator./=}{row_vector \farg{x}, real \farg{y}}{\code{\farg{x} ./=     y} is equivalent to \code{\farg{x} = \farg{x} ./ \farg{y}}.} \fitem{void}{operator./=}{matrix \farg{x}, real \farg{y}}{\code{\farg{x} ./=     y} is equivalent to \code{\farg{x} = \farg{x} ./ \farg{y}}.} \end{description}


\chapter{Higher-Order Functions}


Stan provides a few higher-order functions that act on other functions.  In all cases, the function arguments to the higher-order functions are defined as functions within the Stan language and passed by name to the higher-order functions.


\section{Algebraic Equation Solver}\label{functions-algebraic-solver.section}


Stan provides a built-in algebraic equation solver. Although it looks like other function applications, the algebraic solver is special in two ways.


First, the algebraic solver is a higher-order function, i.e. it takes another function as one of its arguments. The only other functions in Stan which share this feature are the ordinary differential equation solvers (see section \@ref(functions-ode-solver)). Ordinary Stan functions do not allow functions as arguments.


Second, some of the arguments of the algebraic solvers are restricted to data only expressions. These expressions must not contain variables other than those declared in the data or transformed data blocks. Ordinary Stan functions place no restriction on the origin of variables in their argument expressions.


\subsection{Specifying an Algebraic Equation as a Function}


An algebraic system is specified as an ordinary function in Stan within the function block. The algebraic system function must have this signature:


```\n vector algebra_system(vector y, vector theta,\n                              real[] x_r, int[] x_i)\n ```


The algebraic system function should return the value of the algebraic function which goes to 0, when we plug in the solution to the algebraic system.


The argument of this function are:

*   *`y`*, the unknowns we wish to solve for
*   *`theta`*, parameter values used to evaluate the algebraic system
*   *`x_r`*, data values used to evaluate the algebraic system
*   *`x_i`*, integer data used to evaluate the algebraic system


The algebraic system function separates parameter values, *`theta`*, from data values, *`x_r`*, for efficiency in computing the gradients of the algebraic system.


\subsection{Call to the Algebraic Solver}


\begin{description}    \fitemthreelines{vector}{algebra_solver}                   {function \farg{algebra_system}}                   {vector \farg{y_guess}}                   {vector \farg{theta}, real[] \farg{x_r}, int[] \farg{x_i}}                   {Solves the algebraic system, given an initial guess,                     using the Powell hybrid algorithm.} \fitemfourlines{vector}{algebra_solver}                   {function \farg{algebra_system}}                   {vector \farg{y_guess}}                   {vector \farg{theta}, real[] \farg{x_r}, int[] \farg{x_i}}                   {real \farg{rel_tol}, real \farg{f_tol}, int \farg{max_steps}}                   {Solves the algebraic system, given an initial guess,                     using the Powell hybrid algorithm with additional control                     parameters for the solver.} \end{description}


\subsubsection{Arguments to the Algebraic Solver}


The arguments to the algebraic solver are as follows:


* *`algebra_system`*: function literal referring to a function   specifying the system of algebraic equations with signature   `(vector, vector, real[], int[]):vector`.   The arguments represent (1) unknowns, (2) parameters, (3) real data, and   (4) integer data, and the return value contains the value of the algebraic   function, which goes to 0 when we plug in the solution to the algebraic system, 
*   *`y_guess`*: initial guess for the solution, type \code{vector}, 
*   *`theta`*: parameters only, type \code{vector}, 
*   *`x_r`*: real data only, type \code{real[]}, and 
*   *`x_i`*: integer data only, type \code{int[]}.


For more fine-grained control of the algebraic solver, these parameters can also be provided:


*   *`rel_tol`*: relative tolerance for the algebraic solver, type \code{real}, data only, 
*   *`function_tol`*: function tolerance for the algebraic solver, type \code{real}, data only, 
*   *`max_num_steps`*: maximum number of steps to take in the algebraic solver, type \code{int}, data only.


\subsubsection{Return value}


The return value for the algebraic solver is an object of type \code{vector}, with values which, when plugged in as \code{y} make the algebraic function go to 0.


\subsubsection{Sizes and parallel arrays}


Certain sizes have to be consistent. The initial guess, return value of the solver, and return value of the algebraic function must all be the same size.


The parameters, real data, and integer data will be passed from the solver directly to the system function.


\subsubsection{Algorithmic details}


The algebraic solver uses the Powell hybrid method [@Powell:1970], which in turn uses first-order derivatives. The Stan code builds on the implementation of the hybrid solver in the unsupported module for nonlinear optimization problems of the Eigen library [@Eigen:2013]. This solver is in turn based on the algorithm developed for the package MINPACK-1 [@minpack:1980].


The Jacobian of the solution with respect to auxiliary parameters is computed using the implicit function theorem. Intermediate Jacobians (of the the algebraic function's output with respect to the unknowns \farg{y} and with respect to the auxiliary parameters \farg{theta}) are computed using Stan's automatic differentiation.


\section{Ordinary Differential Equation Solvers}\label{functions-ode-solver.section}


Stan provides built-in ordinary differential equation (ODE) solvers. Although they look like function applications, the ODE solvers are special in two ways.


First, the first argument to each of the solvers is a function specifying the ODE system as an argument, like PKBugs [@LunnEtAl:1999]. Ordinary Stan functions do not allow functions as arguments.


Second, some of the arguments to the ODE solvers are restricted to data only expressions. These expressions must not contain variables other than those declared in the data or transformed data blocks. Ordinary Stan functions place no restriction on the origin of variables in their argument expressions.


\subsection{Specifying an Ordinary Differential Equation as a Function}\label{functions-ode-function.section}


A system of ODEs is specified as an ordinary function in Stan within the functions block. The ODE system function must have this function signature:


```\n real[] ode(real time, real[] state, real[] theta,\n            real[] x_r, int[] x_i)\n ```


The ODE system function should return the derivative of the state with respect to time at the time provided. The length of the returned real array must match the length of the state input into the function.


The arguments to this function are:


*   *`time`*, the time to evaluate the ODE system 
*   *`state`*, the state of the ODE system at the time specified 
*   *`theta`*, parameter values used to evaluate the ODE system 
*   *`x_r`*, data values used to evaluate the ODE system 
*   *`x_i`*, integer data values used to evaluate the ODE system.


The ODE system function separates parameter values, *`theta`*, from data values, *`x_r`*, for efficiency in computing the gradients of the ODE.


\subsection{Non-Stiff Solver}


\begin{description}   \fitemthreelines{real[ , ]}{integrate_ode_rk45}                   {function \farg{ode}, real[] \farg{initial_state}}                   {real \farg{initial_time}, real[] \farg{times}}                   {real[] \farg{theta}, real[] \farg{x_r}, int[] \farg{x_i}}                   {Solves the ODE system for the times provided                     using the Runge Kutta Dopri algorithm with the implementation                     from Boost.}  \fitemfourlines{real[ , ]}{integrate_ode_rk45}                 {function \farg{ode}, real[] \farg{initial_state}}                 {real \farg{initial_time}, real[] \farg{times}}                 {real[] \farg{theta}, real[] \farg{x_r}, int[] \farg{x_i}}                 {real \farg{rel_tol}, real \farg{abs_tol}, int \farg{max_num_steps}}                 {Solves the ODE system for the times provided using the Runge Kutta                   Dopri algorithm with the implementation from Boost with                   additional control parameters for the solver.}  \fitemthreelines{real[ , ]}{integrate_ode}                  {function \farg{ode}, real[] \farg{initial_state}}                  {real \farg{initial_time}, real[] \farg{times}}                  {real[] \farg{theta}, real[] \farg{x_r}, int[] \farg{x_i}}                  {Deprecated. Solves the ODE system for the times provided with a                    non-stiff solver.  This calls the Runge Kutta Dopri algorithm.} \end{description}


\subsection{Stiff Solver}


\begin{description}   \fitemthreelines{real[]}{integrate_ode_bdf}                   {function \farg{ode}, real[] \farg{initial_state}}                   {real \farg{initial_time}, real[] \farg{times}}                   {real[] \farg{theta}, data real[] \farg{x_r}, data int[] \farg{x_i}}                   {Solves the ODE system for the times provided using the backward                     differentiation formula (BDF) method with the implementation from                     CVODES.}  \fitemfourlines{real[]}{integrate_ode_bdf}                 {function \farg{ode}, real[] \farg{initial_state}}                 {real \farg{initial_time}, real[] \farg{times}}                 {real[] \farg{theta}, data real[] \farg{x_r}, data int[] \farg{x_i}}                 {data real \farg{rel_tol}, data real \farg{abs_tol},                   dta int \farg{max_num_steps}}                 {Solves the ODE system for the times provided using the backward                   differentiation formula (BDF) method with the implementation from                   CVODES with additional control parameters for the CVODES solver.} \end{description}


\subsection{Arguments to the ODE solvers}


The arguments to the ODE solvers in both the stiff and non-stiff cases are as follows.


*   *`ode`*: function literal referring to a function specifying   the system of differential equations with signature described in   \@ref(functions-ode-function):

```\n (real, real[], real[], data real[], data int[]):real[]\n```

The arguments represent (1) time, (2) system state, (3) parameters,   (4) real data, and (5) integer data, and the return value contains the   derivatives with respect to time of the state, 

*   *`initial_state`*: initial state, type \code{real[]}, 
*   *`initial_time`*: initial time, type \code{int}  or \code{real}, 
*   *`times`*: solution times, type \code{real[]}, 
*   *`theta`*: parameters, type \code{real[]}, 
*   \code{data}   *`x_r`*: real data, type \code{real[]}, data only, and 
*   \code{data}   *`x_i`*: integer data, type \code{int[]}, data only.

For more fine-grained control of the ODE solvers, these parameters can also be provided:

*   \code{data}   *`rel_tol`*: relative tolerance for the ODE  solver, type \code{real}, data only, 
*   \code{data}   *`abs_tol`*: absolute tolerance for the ODE  solver, type \code{real}, data only, and 
*   \code{data}   *`max_num_steps`*: maximum number of steps to take in the  ODE solver, type \code{int}, data only.


\subsubsection{Return values}


The return value for the ODE solvers is an array of type \code{real[,]}, with values consisting of solutions at the specified times.


\subsubsection{Sizes and parallel arrays}


The sizes must match, and in particular, the following groups are of the same size:


*   state variables passed into the system function,  derivatives returned by the system function, initial state passed  into the solver, and rows of the return value of the solver, 
*   solution times and number of rows of the return value of the solver, 
*   parameters, real data and integer data passed to the solver will be passed to the system function


\section{Higher-Order Map}\label{functions-map.section}


Stan provides a higher-order map function.  This allows map-reduce functionality to be coded in Stan as described in the user's guide.


\subsection{Specifying the Mapped Function}


The function being mapped must have a signature identical to that of the function \code{f} in the following declaration.


```\n vector f(vector phi, vector theta,\n          data real[] x_r, data int[] x_i);\n ```


The map function returns the sequence of results for the particular shard being evaluated.  The arguments to the mapped function are:

*   *`phi`*, the sequence of parameters shared across shards 
*   *`theta`*, the sequence of parameters specific to this shard 
*   *`x_r`*, sequence of real-valued data 
*   *`x_i`*, sequence of integer data


All input for the mapped function must be packed into these sequences and all output from the mapped function must be packed into a single vector.  The vector of output from each mapped function is concatenated into the final result.


\subsection{Rectangular Map}


The rectangular map function operates on rectangular (not ragged) data structures, with parallel data structures for job-specific parameters, job-specific real data, and job-specific integer data.


\begin{description} \fitemtwolines{vector}{map_rect}{F \farg{f}, vector \farg{phi}, vector[] \farg{theta},   data real[,] \farg{x_r}}{data int[,] \farg{x_i}}{Return the concatenation of   the results of applying the function \farg{f}, of type   \code{(vector, vector, real[], int[]):vector} elementwise, i.e.,   \code{\farg{f}(\farg{phi}, \farg{theta}[n], \farg{x_r}[n],     \farg{x_i}[n])} for each \code{n} in \code{1:N}, where \code{N}   is the size of the parallel arrays of job-specific/local parameters   \code{theta}, real data \code{x_r}, and integer data \code{x_r}.   The shared/global parameters \code{phi} are passed to each   invocation of \code{f}.} \end{description}


\part{Discrete Distributions}\label{discrete-prob-functions.part}


\chapter{Conventions for Probability Functions}


Functions associated with distributions are set up to follow the same naming conventions for both built-in distributions and for user-defined distributions.


\section{Suffix Marks Type of Function}


The suffix is determined by the type of function according to the following table.

| function   | outcome  | suffix  |
|:---------|:--------|:------|
| log probability mass function  | discrete | `_lpmf` |
| log probability density function | continuous | `_lpdf` | 
| log cumulative distribution function | any | `_lcdf` |
| log complementary cumulative distribution function | any | `_lccdf`
| random number generator | any | `_rng` |

For example, \code{normal_lpdf} is the log of the normal probability density function (pdf) and \code{bernoulli_lpmf} is the log of the bernoulli probability mass function (pmf).  The log of the corresponding cumulative distribution functions (cdf) use the same suffix, \code{normal_lcdf} and \code{bernoulli_lcdf}.


\section{Argument Order and the Vertical Bar}


Each probability function has a specific outcome value and a number of parameters.  Following conditional probability notation, probability density and mass functions use a vertical bar to separate the outcome from the parameters of the distribution.  For example, \code{normal_lpdf(y | mu, sigma)} returns the value of mathematical formula $log \text{Normal}(y \, | \, \mu, \sigma)$. Cumulative distribution functions separate the outcome from the parameters in the same way (e.g., \code{normal_lcdf(y_low | mu, sigma)}


\section{Sampling Notation}


The notation


```\n y ~ normal(mu, sigma);\n ```


provides the same (proportional) contribution to the model log density as the explicit target density increment,


```\n target += normal_lpdf(y | mu, sigma);\n ```


In both cases, the effect is to add terms to the target log density. The only difference is that the example with the sampling (`~`) notation drops all additive constants in the log density;  the constants are not necessary for any of Stan's sampling, approximation, or optimization algorithms.


\section{Finite Inputs}


All of the distribution functions are configured to throw exceptions (effectively rejecting samples or optimization steps) when they are supplied with non-finite arguments.  The two cases of non-finite arguments are the infinite values and not-a-number value---these are standard in floating-point arithmetic.


\section{Boundary Conditions}


Many distributions are defined with support or constraints on parameters forming an open interval.  For example, the normal density function accepts a scale parameter $\sigma > 0$.  If $\sigma = 0$, the probability function will throw an exception.


This is true even for (complementary) cumulative distribution functions, which will throw exceptions when given input that is out of the support.


\section{Pseudorandom Number Generators}\label{distributions-prng.section}


For most of the probability functions, there is a matching pseudorandom number generator (PRNG) with the suffix \code{_rng}. For example, the function \code{normal_rng(real, real)} accepts two real arguments, an unconstrained location $\mu$ and positive scale $\sigma > 0$, and returns an unconstrained pseudorandom value drawn from $\text{Normal}(\mu,\sigma)$.  There are also vectorized forms of random number generators which return more than one random variate at a time.


\subsection{Restricted to Generated Quantities}


Unlike regular functions, the PRNG functions may only be used in the generated quantities block.


\subsection{Limited Vectorization}


Unlike the probability functions, only some of the PRNG functions are vectorized.


\section{Cumulative Distribution Functions}


For most of the univariate probability functions, there is a corresponding cumulative distribution function, log cumulative distribution function, and log complementary cumulative distribution function.


For a univariate random variable $Y$ with probability function $p_Y(y \, | \, \theta)$, the cumulative distribution function (CDF) $F_Y$ is defined by \[ F_Y(y) \ = \ \text{Pr}[Y < y] \ = \ \int_{-\infty}^y p(y \, | \, \theta) \ \text{d}y. \] The complementary cumulative distribution function (CCDF) is defined as \[ \text{Pr}[Y \geq y] \ = \ 1 - F_Y(y). \] The reason to use CCDFs instead of CDFs in floating-point arithmetic is that it is possible to represent numbers very close to 0 (the closest you can get is roughly $10^{-300}$), but not numbers very close to 1 (the closest you can get is roughly $1 - 10^{-15}$).


In Stan, there is a cumulative distribution function for each probability function.  For instance, \code{normal_cdf(y, mu, sigma)} is defined by \[ \int_{-\infty}^y \text{Normal}(y \, | \, \mu, \sigma) \ \text{d}y. \] There are also log forms of the CDF and CCDF for most univariate distributions.  For example, \code{normal_lcdf(y | mu, sigma)} is defined by \[ \log \left( \int_{-\infty}^y \text{Normal}(y \, | \, \mu, \sigma) \   \text{d}y \right) \] and \code{normal_lccdf(y | mu, sigma)} is defined by \[ \log \left( 1 - \int_{-\infty}^y \text{Normal}(y \, | \, \mu, \sigma) \   \text{d}y \right). \] 

\section{Vectorization}\label{vectorization.section}


Stan's univariate log probability functions, including the log density functions, log mass functions, log CDFs, and log CCDFs, all support vectorized function application, with results defined to be the sum of the elementwise application of the function.  Some of the PRNG functions support vectorization, see section \@ref(prng-vectorization) for more details.


In all cases, matrix operations are at least as fast and usually faster than loops and vectorized log probability functions are faster than their equivalent form defined with loops.  This isn't because loops are slow in Stan, but because more efficient automatic differentiation can be used.  The efficiency comes from the fact that a vectorized log probably function only introduces one new node into the expression graph, thus reducing the number of virtual function calls required to compute gradients in  C++, as well as from allowing caching of repeated computations.


Stan also overloads the multivariate normal distribution, including the Cholesky-factor form, allowing arrays of row vectors or vectors for the variate and location parameter.  This is a huge savings in speed because the work required to solve the linear system for the covariance matrix is only done once.


Stan also overloads some scalar functions, such as \code{log} and \code{exp}, to apply to vectors (arrays) and return vectors (arrays). These vectorizations are defined elementwise and unlike the probability functions, provide only minimal efficiency speedups over repeated application and assignment in a loop.


\subsection{Vectorized Function Signatures}\label{prob-vectorization.section}


\subsubsection{Vectorized Scalar Arguments}


The normal probability function is specified with the signature


```\n normal_lpdf(reals | reals, reals);\n ```


The pseudotype \code{reals} is used to indicate that an argument position may be vectorized.  Argument positions declared as \code{reals} may be filled with a real, a one-dimensional array, a vector, or a row-vector.  If there is more than one array or vector argument, their types can be anything but their size must match.  For instance, it is legal to use \code{normal_lpdf(row_vector | vector, real)} as long as the vector and row vector have the same size.


\subsubsection{Vectorized Vector and Row Vector Arguments}


The multivariate normal distribution accepting vector or array of vector arguments is written as


```\n multi_normal_lpdf(vectors | vectors, matrix);\n ```


These arguments may be row vectors, column vectors, or arrays of row vectors or column vectors.


\subsubsection{Vectorized Integer Arguments}


The pseudotype \code{ints} is used for vectorized integer arguments. Where it appears either an integer or array of integers may be used.


\subsection{Evaluating Vectorized Log Probability Functions}


The result of a vectorized log probability function is equivalent to the sum of the evaluations on each element.  Any non-vector argument, namely \code{real} or \code{int}, is repeated.  For instance, if \code{y} is a vector of size \code{N}, \code{mu} is a vector of size \code{N}, and \code{sigma} is a scalar, then


```\n ll = normal_lpdf(y | mu, sigma);\n ```


is just a more efficient way to write


```\n ll = 0;\n for (n in 1:N)\n   ll = ll + normal_lpdf(y[n] | mu[n], sigma);\n ```


With the same arguments, the vectorized sampling statement


```\n y ~ normal(mu, sigma);\n ```


has the same effect on the total log probability as


```\n for (n in 1:N)\n   y[n] ~ normal(mu[n], sigma);\n ```


\subsection{Evaluating Vectorized PRNG Functions}\label{prng-vectorization.section}


Some PRNG functions accept sequences as well as scalars as arguments. Such functions are indicated by argument pseudotypes \code{reals} or \code{ints}.  In cases of sequence arguments, the output will also be a sequence.  For example, the following is allowed in the generated quantities block.


```\n vector[3] mu = ...;\n real x[3] = normal_rng(mu, 3);\n ```


\subsubsection{Argument types}


In the case of PRNG functions, arguments marked \code{ints} may be integers or integer arrays, whereas arguments marked \code{reals} may be integers or reals, integer or real arrays, vectors, or row vectors.


| pseudotype | allowable PRNG arguments |
|:-----------|:----------------------|
|\code{ints} | \code{int},  \code{int[]}  |
|\code{reals} | \code{int},  \code{int[]},  \code{real},  \code{real[]},  \code{vector},  \code{row_vector} |


\subsubsection{Dimension matching}


In general, if there are multiple non-scalar arguments, they must all have the same dimensions, but need not have the same type.  For example, the \code{normal_rng} function may be called with one vector argument and one real array argument as long as they have the same number of elements.


```\n vector[3] mu = ...;\n real sigma[3] = ...;\n real x[3] = normal_rng(mu, sigma);\n ```


\subsubsection{Return type}


The result of a vectorized PRNG function depends on the size of the arguments and the distribution's support.  If all arguments are scalars, then the return type is a scalar.  For a continuous distribution, if there are any non-scalar arguments, the return type is a real array (\code{real[]}) matching the size of any of the non-scalar arguments, as all non-scalar arguments must have matching size.  Discrete distributions return \code{ints} and continuous distributions return \code{reals}, each of appropriate size.  The symbol \code{R} denotes such a return type.


\chapter{Binary Distributions}


Binary probability distributions have support on $\{0,1\}$, where 1 represents the value true and 0 the value false.


\section{Bernoulli Distribution}


\subsection{Probability Mass Function}


If $\theta \in [0,1]$, then for $y \in \{0,1\}$, \[ \text{Bernoulli}(y~|~\theta) = \left\{ \begin{array}{ll} \theta & \text{if } y = 1, \text{ and} \\ 1 - \theta & \text{if } y = 0. \end{array} \right. \] 

\pitemdisc{y}{bernoulli}{theta}


\subsection{Stan Functions}


\begin{description} \fitem{real}{bernoulli_lpmf}{ints \farg{y} | reals \farg{theta}}{The   log Bernoulli probability mass of \farg{y} given chance of success   \farg{theta}} \end{description}


\begin{description} \fitem{real}{bernoulli_cdf}{ints \farg{y}, reals \farg{theta}}{The   Bernoulli cumulative distribution function of \farg{y} given chance of success   \farg{theta}} \fitem{real}{bernoulli_lcdf}{ints \farg{y} | reals \farg{theta}}{The   log of the Bernoulli cumulative distribution function of \farg{y} given   chance of success \farg{theta}} \fitem{real}{bernoulli_lccdf}{ints \farg{y} | reals \farg{theta}}{The   log of the Bernoulli complementary cumulative distribution function of   \farg{y} given chance of success \farg{theta}} \end{description}


\begin{description}   \fitem{R}{bernoulli_rng}{reals \farg{theta}}{Generate a Bernoulli     variate with chance of success \farg{theta}; may only be used in     generated quantities block.  For a description of argument and     return types, see section \@ref(prng-vectorization).} \end{description}


\section{Bernoulli Distribution, Logit Parameterization}\label{bernoulli-logit-distribution.section}


Stan also supplies a direct parameterization in terms of a logit-transformed chance-of-success parameter.  This parameterization is more numerically stable if the chance-of-success parameter is on the logit scale, as with the linear predictor in a logistic regression.


\subsection{Probability Mass Function}


If $\alpha \in \mathbb{R}$, then for $y \in \{0,1\}$, \[ \text{BernoulliLogit}(y~|~\alpha) = \text{Bernoulli}(y | \text{logit}^{-1}(\alpha)) = \left\{ \begin{array}{ll} \text{logit}^{-1}(\alpha) & \text{if } y = 1, \text{ and} \\ 1 - \text{logit}^{-1}(\alpha) &  \text{if } y = 0. \end{array} \right. \] 

\pitemdisc{y}{bernoulli_logit}{alpha}


\subsection{Stan Functions}


\begin{description} \fitem{real}{bernoulli_logit_lpmf}{ints \farg{y} | reals \farg{alpha}}{The   log Bernoulli probability mass of \farg{y} given chance of success   \code{inv_logit(\farg{alpha})}} \end{description}


\begin{description}   \fitem{R}{bernoulli_logit_rng}{reals \farg{alpha}}{Generate a     Bernoulli variate with chance of success     $\text{logit}^{-1}(\alpha)$; may only be used in generated     quantities block.  For a description of argument and return types,     see section \@ref(prng-vectorization).} \end{description}


\section{Bernoulli-Logit Generalised Linear Model (Logistic Regression)}\label{bernoulli-logit-glm.section}


Stan also supplies a single primitive for a Generalised Linear Model with Bernoulli likelihood and logit link function, i.e. a primitive for a logistic regression. This should provide a more efficient implementation of logistic regression than a manually written regression in terms of a Bernoulli likelihood and matrix multiplication.


\subsection{Probability Mass Function}


If $x\in \mathbb{R}^{n\cdot m}, \alpha \in \mathbb{R}^n, \beta\in \mathbb{R}^m$, then for $y \in {\{0,1\}}^n$, \begin{align*} &\text{BernoulliLogitGLM}(y~|~x, \alpha, \beta) = \prod_{1\leq i \leq n}\text{Bernoulli}(y_i~|~\text{logit}^{-1}(\alpha_i + x_i\cdot \beta))\\ &= \prod_{1\leq i \leq n} \left\{ \begin{array}{ll} \text{logit}^{-1}(\alpha_i + \sum_{1\leq j\leq m}x_{ij}\cdot \beta_j) & \text{if } y_i = 1, \text{ and} \\ 1 - \text{logit}^{-1}(\alpha_i + \sum_{1\leq j\leq m}x_{ij}\cdot \beta_j) & \text{if } y_i = 0. \end{array} \right. \end{align*}


\pitemdisc{y}{bernoulli_logit_glm}{x, alpha, beta}


\subsection{Stan Functions}


\begin{description} \fitem{real}{bernoulli_logit_glm_lpmf}{int[] \farg{y} | matrix \farg{x}, real \farg{alpha}, vector \farg{beta}}{The   log Bernoulli probability mass of \farg{y} given chance of success   \code{inv_logit(\farg{alpha}+\farg{x}*\farg{beta})}, where a   constant intercept \code{\farg{alpha}} is used for all observations.   The number of rows of the independent variable matrix \code{\farg{x}} needs to   match the length of the dependent variable vector \code{\farg{y}} and   the number of columns of \code{\farg{x}} needs to match the length of   the weight vector \code{\farg{beta}}.} \end{description}


\begin{description} \fitem{real}{bernoulli_logit_glm_lpmf}{int[] \farg{y} | matrix   \farg{x}, vector \farg{alpha}, vector \farg{beta}}{The   log Bernoulli probability mass of \farg{y} given chance of success   \code{inv_logit(\farg{alpha}+\farg{x}*\farg{beta})}, where an   intercept \code{\farg{alpha}} is used that is allowed to vary with the different   observations.   The number of rows of the independent variable matrix \code{\farg{x}} needs   to match the length of the dependent variable vector   \code{\farg{y}} and \code{\farg{alpha}}  and   the number of columns of \code{\farg{x}} needs to match the length of   the weight vector \code{\farg{beta}}.} \end{description}


\chapter{Bounded Discrete Distributions}


Bounded discrete probability functions have support on $\{ 0, \ldots, N \}$ for some upper bound $N$.


\section{Binomial Distribution}


\subsection{Probability Mass Function}


Suppose $N \in \mathbb{N}$ and $\theta \in [0,1]$, and $n \in \{0,\ldots,N\}$. \[ \text{Binomial}(n~|~N,\theta) = \binom{N}{n} \theta^n (1 - \theta)^{N - n}. \] 

\subsection{Log Probability Mass Function}


\begin{eqnarray*} \log \text{Binomial}(n~|~N,\theta) & = & \log \Gamma(N+1) - \log \Gamma(n + 1) - \log \Gamma(N- n + 1) \\[4pt] & & { } + n \log \theta + (N - n) \log (1 - \theta), \end{eqnarray*}



\subsection{Gradient of Log Probability Mass Function}

 \[ \frac{\partial}{\partial \theta} \log \text{Binomial}(n~|~N,\theta) = \frac{n}{\theta} - \frac{N - n}{1 - \theta} \] 

\pitemdisc{n}{binomial}{N, theta}


\subsection{Stan Functions}


\begin{description}   \fitem{real}{binomial_lpmf}{ints \farg{n} | ints \farg{N}, reals     \farg{theta}}{The log binomial probability mass of \farg{n}     successes in \farg{N} trials given chance of success \farg{theta}}   \fitem{real}{binomial_cdf}{ints \farg{n}, ints \farg{N}, reals     \farg{theta}}{The binomial cumulative distribution function of \farg{n}     successes in \farg{N} trials given chance of success \farg{theta}}   \fitem{real}{binomial_lcdf}{ints \farg{n} | ints \farg{N}, reals     \farg{theta}}{The log of the binomial cumulative distribution function of     \farg{n} successes in \farg{N} trials given chance of success \farg{theta}}   \fitem{real}{binomial_lccdf}{ints \farg{n} | ints \farg{N}, reals     \farg{theta}}{The log of the binomial complementary cumulative distribution      function of \farg{n} successes in \farg{N} trials given chance of success     \farg{theta}} \end{description}


\begin{description}   \fitem{R}{binomial_rng}{ints \farg{N}, reals \farg{theta}}{Generate     a binomial variate with \farg{N} trials and chance of success     \farg{theta}; may only be used in generated quantities block.  For     a description of argument and return types, see     section \@ref(prng-vectorization).  } \end{description}


\section{Binomial Distribution, Logit Parameterization}


Stan also provides a version of the binomial probability mass function distribution with the chance of success parameterized on the unconstrained logistic scale.


\subsection{Probability Mass Function}


Suppose $N \in \mathbb{N}$, $\alpha \in \mathbb{R}$, and $n \in \{0,\ldots,N\}$.  Then  \begin{eqnarray*} \text{BinomialLogit}(n~|~N,\alpha) & = & \text{Binomial}(n~|~N,\text{logit}^{-1}(\alpha)) \\[6pt] & = & \binom{N}{n} \left( \text{logit}^{-1}(\alpha) \right)^{n}  \left( 1 - \text{logit}^{-1}(\alpha) \right)^{N - n}.  \end{eqnarray*}


\subsection{Log Probability Mass Function}

\begin{eqnarray*} \log \text{BinomialLogit}(n~|~N,\alpha) & = & \log \Gamma(N+1) - \log \Gamma(n + 1) - \log \Gamma(N- n + 1) \\[4pt]   & &  { } + n \log \text{logit}^{-1}(\alpha) + (N - n) \log \left( 1 -   \text{logit}^{-1}(\alpha) \right), \end{eqnarray*}


\subsection{Gradient of Log Probability Mass Function}

 \[ \frac{\partial}{\partial \alpha} \log \text{BinomialLogit}(n~|~N,\alpha) = \frac{n}{\text{logit}^{-1}(-\alpha)} - \frac{N - n}{\text{logit}^{-1}(\alpha)} \] 

\pitemdisc{n}{binomial_logit}{N, alpha}


\subsection{Stan Functions}


\begin{description}   \fitem{real}{binomial_logit_lpmf}{ints \farg{n} | ints \farg{N}, reals     \farg{alpha}}{The log binomial probability mass of \farg{n}     successes in \farg{N} trials given logit-scaled chance of success \farg{alpha}} \end{description}


\section{Beta-Binomial Distribution}


\subsection{Probability Mass Function}


If $N \in \mathbb{N}$, $\alpha \in \mathbb{R}^+$, and $\beta \in \mathbb{R}^+$, then for $n \in {0,\ldots,N}$, \[ \text{BetaBinomial}(n~|~N,\alpha,\beta) = \binom{N}{n} \frac{\mathrm{B}(n+\alpha, N -n +   \beta)}{\mathrm{B}(\alpha,\beta)}, \] where the beta function $\mathrm{B}(u,v)$ is defined for $u \in \mathbb{R}^+$ and $v \in \mathbb{R}^+$ by \[ \mathrm{B}(u,v) = \frac{\Gamma(u) \ \Gamma(v)}{\Gamma(u + v)}. \] 

\pitemdisc{n}{beta_binomial}{N, alpha, beta}


\subsection{Stan Functions}


\begin{description} \fitem{real}{beta_binomial_lpmf}{ints \farg{n} | ints \farg{N}, reals   \farg{alpha}, reals \farg{beta}}{The log beta-binomial probability mass   of \farg{n} successes in \farg{N} trials given prior success count   (plus one) of \farg{alpha} and prior failure count (plus one) of   \farg{beta}} \fitem{real}{beta_binomial_cdf}{ints \farg{n}, ints \farg{N}, reals   \farg{alpha}, reals \farg{beta}}{The beta-binomial cumulative   distribution function   of \farg{n} successes in \farg{N} trials given prior success count   (plus one) of \farg{alpha} and prior failure count (plus one) of   \farg{beta}} \fitem{real}{beta_binomial_lcdf}{ints \farg{n} | ints \farg{N}, reals   \farg{alpha}, reals \farg{beta}}{The log of the beta-binomial cumulative   distribution function   of \farg{n} successes in \farg{N} trials given prior success count   (plus one) of \farg{alpha} and prior failure count (plus one) of   \farg{beta}} \fitem{real}{beta_binomial_lccdf}{ints \farg{n} | ints \farg{N}, reals   \farg{alpha}, reals \farg{beta}}{The log of the beta-binomial complementary    cumulative distribution function of \farg{n} successes in \farg{N} trials    given prior success count (plus one) of \farg{alpha} and prior failure count   (plus one) of \farg{beta}} \end{description}


\begin{description}   \fitem{R}{beta_binomial_rng}{ints \farg{N}, reals \farg{alpha},     reals \farg{beta}}{Generate a beta-binomial variate with \farg{N}     trials, prior success count (plus one) of \farg{alpha}, and prior     failure count (plus one) of \farg{beta}; may only be used in     generated quantities block.  For a description of argument and     return types, see section \@ref(prng-vectorization).} \end{description}


\section{Hypergeometric Distribution}


\subsection{Probability Mass Function}


If $a \in \mathbb{N}$, $b \in \mathbb{N}$, and $N \in \{0,\ldots,a+b\}$, then for $n \in \{\max(0,N-b),\ldots,\min(a,N)\}$, \[ \text{Hypergeometric}(n~|~N,a,b) = \frac{\normalsize{\binom{a}{n} \binom{b}{N - n}}}      {\normalsize{\binom{a + b}{N}}}. \] 

\pitemdisc{n}{hypergeometric}{N, a, b}


\subsection{Stan Functions}


\begin{description}   \fitem{real}{hypergeometric_lpmf}{int \farg{n} ~|~ int \farg{N}, int     \farg{a}, int \farg{b}}{The log hypergeometric probability mass of     \farg{n} successes in \farg{N} trials given total success count of     \farg{a} and total failure count of \farg{b}} \end{description}


\begin{description} \fitem{int}{hypergeometric_rng}{int \farg{N}, int \farg{a}, int2    \farg{b}}{Generate a hypergeometric variate with \farg{N} trials, total     success count of \farg{a}, and total failure count of \farg{b}; may only     be used in generated quantities block} \end{description}


\section{Categorical Distribution}\label{categorical-distribution.section}


\subsection{Probability Mass Functions}


If $N \in \mathbb{N}$, $N > 0$, and if $\theta \in \mathbb{R}^N$ forms an $N$-simplex (i.e., has nonnegative entries summing to one), then for $y \in \{1,\ldots,N\}$, \[ \text{Categorical}(y~|~\theta) = \theta_y. \] In addition, Stan provides a log-odds scaled categorical distribution, \[ \text{CategoricalLogit}(y~|~\beta) = \text{Categorical}(y~|~\text{softmax}(\beta)). \] See section \@ref(softmax) for the definition of the softmax function.


\pitemdisc{y}{categorical}{theta}
\pitemdisc{y}{categorical_logit}{beta}


\subsection{Stan Functions}


All of the categorical distributions are vectorized so that the outcome \farg{y} can be a single integer (type \code{int}) or an array of integers (type \code{int[]}).


\begin{description}   \fitem{real}{categorical_lpmf}{ints \farg{y} | vector     \farg{theta}}{The log categorical probability mass function with     outcome(s) \farg{y} in $1:N$ given $N$-vector of outcome     probabilities \farg{theta}.  The parameter \farg{theta} must have     non-negative entries that sum to one, but it need not be a     variable declared as a simplex.} \fitem{real}{categorical_logit_lpmf}{ints \farg{y} | vector   \farg{beta}}{The log categorical probability mass function with   outcome(s) \farg{y} in $1:N$ given log-odds of outcomes \farg{beta}.} \end{description}


\begin{description} \fitem{int}{categorical_rng}{vector \farg{theta}}{Generate a   categorical variate with $N$-simplex distribution parameter \farg{theta}; may only be used in generated quantities block} \end{description}


\begin{description} \fitem{int}{categorical_logit_rng}{vector \farg{beta}}{Generate a   categorical variate with outcome in range $1:N$ from log-odds vector \farg{beta}; may only be used in generated quantities block} \end{description}


\section{Ordered Logistic Distribution}


\subsection{Probability Mass Function}


If $K \in \mathbb{N}$ with $K > 2$, $c \in \mathbb{R}^{K-1}$ such that $c_k < c_{k+1}$ for $k \in \{1,\ldots,K-2\}$, and $\eta \in \mathbb{R}$, then for $k \in \{1,\ldots,K\}$, \[ \text{OrderedLogistic}(k~|~\eta,c) = \left\{ \begin{array}{ll} 1 - \text{logit}^{-1}(\eta - c_1)  &  \text{if } k = 1, \\[4pt] \text{logit}^{-1}(\eta - c_{k-1}) - \text{logit}^{-1}(\eta - c_{k})  &  \text{if } 1 < k < K, \text{and} \\[4pt] \text{logit}^{-1}(\eta - c_{K-1}) - 0  &  \text{if } k = K. \end{array} \right. \] The $k=K$ case is written with the redundant subtraction of zero to illustrate the parallelism of the cases; the $k=1$ and $k=K$ edge cases can be subsumed into the general definition by setting $c_0 = -\infty$ and $c_K = +\infty$ with $\text{logit}^{-1}(-\infty) = 0$ and $\text{logit}^{-1}(\infty) = 1$.


\pitemdisc{k}{ordered_logistic}{eta, c}


\subsection{Stan Functions}


\begin{description}   \fitem{real}{ordered_logistic_lpmf}{ints \farg{k} | vector \farg{eta},     vectors \farg{c}}{The log ordered logistic probability mass of     \farg{k} given linear predictors \farg{eta}, and cutpoints \farg{c}.} \fitem{int}{ordered_logistic_rng}{real \farg{eta}, vector \farg{c}}{Generate   an ordered logistic variate with linear predictor \farg{eta} and cutpoints     \farg{c}; may only be used in generated quantities block} \end{description}


\section{Ordered Probit Distribution}


\subsection{Probability Mass Function}


If $K \in \mathbb{N}$ with $K > 2$, $c \in \mathbb{R}^{K-1}$ such that $c_k < c_{k+1}$ for $k \in \{1,\ldots,K-2\}$, and $\eta \in \mathbb{R}$, then for $k \in \{1,\ldots,K\}$, \[ \text{OrderedProbit}(k~|~\eta,c) = \left\{ \begin{array}{ll} 1 - \Phi(\eta - c_1) & \text{if } k = 1, \\[4pt] \Phi(\eta - c_{k-1}) - \Phi(\eta - c_{k})  & \text{if } 1 < k < K, \text{and} \\[4pt] \Phi(\eta - c_{K-1}) - 0 & \text{if } k = K. \end{array} \right. \] The $k=K$ case is written with the redundant subtraction of zero to illustrate the parallelism of the cases; the $k=1$ and $k=K$ edge cases can be subsumed into the general definition by setting $c_0 = -\infty$ and $c_K = +\infty$ with $\Phi(-\infty) = 0$ and $\Phi(\infty) = 1$.


\pitemdisc{k}{ordered_probit}{eta, c}


\subsection{Stan Functions}


\begin{description}   \fitem{real}{ordered_probit_lpmf}{ints \farg{k} | vector \farg{eta},     vectors \farg{c}}{The log ordered probit probability mass of     \farg{k} given linear predictors \farg{eta}, and cutpoints \farg{c}.} \fitem{int}{ordered_probit_rng}{real \farg{eta}, vector \farg{c}}{Generate   an ordered probit variate with linear predictor \farg{eta} and cutpoints     \farg{c}; may only be used in generated quantities block} \end{description}


\chapter{Unbounded Discrete Distributions}


The unbounded discrete distributions have support over the natural numbers (i.e., the non-negative integers).


\section{Negative Binomial Distribution}


For the negative binomial distribution Stan uses the parameterization described in @GelmanEtAl:2013.  For alternative parameterizations, see section \@ref(neg-binom-2-log).


\subsection{Probability Mass Function}


If $\alpha \in \mathbb{R}^+$ and $\beta \in \mathbb{R}^+$, then for $y \in \mathbb{N}$, \[ \text{NegBinomial}(y~|~\alpha,\beta)  = \binom{y + \alpha - 1}{\alpha - 1} \, \left( \frac{\beta}{\beta+1} \right)^{\!\alpha} \, \left( \frac{1}{\beta + 1} \right)^{\!y} \!. \] 

The mean and variance of a random variable $y \sim \text{NegBinomial}(\alpha,\beta)$ are given by \[ \mathbb{E}[y] = \frac{\alpha}{\beta} \ \ \text{ and } \ \ \text{Var}[Y] = \frac{\alpha}{\beta^2} (\beta + 1). \] 

\pitemdisc{n}{neg_binomial}{alpha, beta}


\subsection{Stan Functions}


\begin{description}  \fitem{real}{neg_binomial_lpmf}{ints \farg{n} | reals    \farg{alpha}, reals \farg{beta}}{The log negative binomial probability    mass of \farg{n} given shape \farg{alpha} and inverse scale \farg{beta}}  \fitem{real}{neg_binomial_cdf}{ints \farg{n}, reals    \farg{alpha}, reals \farg{beta}}{The negative binomial cumulative    distribution function    of \farg{n} given shape \farg{alpha} and inverse scale \farg{beta}}  \fitem{real}{neg_binomial_lcdf}{ints \farg{n} | reals    \farg{alpha}, reals \farg{beta}}{The log of the negative binomial cumulative    distribution function    of \farg{n} given shape \farg{alpha} and inverse scale \farg{beta}}  \fitem{real}{neg_binomial_lccdf}{ints \farg{n} | reals    \farg{alpha}, reals \farg{beta}}{The log of the negative binomial    complementary cumulative distribution function of \farg{n}    given shape \farg{alpha} and inverse scale \farg{beta}}  \fitem{R}{neg_binomial_rng}{reals \farg{alpha}, reals    \farg{beta}}{Generate a negative binomial variate with shape    \farg{alpha} and inverse scale \farg{beta}; may only be used in    generated quantities block. \farg{alpha} $/$ \farg{beta} must be    less than $2 ^ {29}$.  For a description of argument and return    types, see section \@ref(prob-vectorization).} \end{description}


\section{Negative Binomial Distribution (alternative parameterization)}\label{nbalt.section}


Stan also provides an alternative parameterization of the negative binomial distribution directly using a mean (i.e., location) parameter and a parameter that controls overdispersion relative to the square of the mean.  Section \@ref(betafun), below, provides a second alternative parameterization directly in terms of the log mean.


\subsection{Probability Mass Function}


The first parameterization is for $\mu \in \mathbb{R}^+$ and $\phi \in \mathbb{R}^+$, which for $y \in \mathbb{N}$ is defined as \[ \text{NegBinomial2}(y \, | \, \mu, \phi)  = \binom{y + \phi - 1}{y} \, \left( \frac{\mu}{\mu+\phi} \right)^{\!y} \, \left( \frac{\phi}{\mu+\phi} \right)^{\!\phi} \!. \] 

The mean and variance of a random variable $y \sim \text{NegBinomial2}(y~|~\mu,\phi)$ are \[ \mathbb{E}[Y] = \mu \ \ \ \text{ and } \ \ \ \text{Var}[Y] = \mu + \frac{\mu^2}{\phi}. \] Recall that $\text{Poisson}(\mu)$ has variance $\mu$, so $\mu^2 / \phi > 0$ is the additional variance of the negative binomial above that of the Poisson with mean $\mu$.  So the inverse of parameter $\phi$ controls the overdispersion, scaled by the square of the mean, $\mu^2$.


\pitemdisc{y}{neg_binomial_2}{mu, phi}


\subsection{Stan Functions}


\begin{description}  \fitem{real}{neg_binomial_2_lpmf}{ints \farg{y} | reals    \farg{mu}, reals \farg{phi}}{The negative binomial probability    mass of \farg{n} given location \farg{mu} and precision    \farg{phi}.}  \fitem{real}{neg_binomial_2_cdf}{ints \farg{n}, reals    \farg{mu}, reals \farg{phi}}{The negative binomial cumulative    distribution function of \farg{n}    given location \farg{mu} and precision \farg{phi}.}  \fitem{real}{neg_binomial_2_lcdf}{ints \farg{n} | reals    \farg{mu}, reals \farg{phi}}{The log of the negative binomial    cumulative distribution function of \farg{n}    given location \farg{mu} and precision  \farg{phi}.}  \fitem{real}{neg_binomial_2_lccdf}{ints \farg{n} | reals    \farg{mu}, reals \farg{phi}}{The log of the negative binomial    complementary cumulative distribution function of \farg{n}    given location \farg{mu} and precision \farg{phi}.}  \fitem{R}{neg_binomial_2_rng}{reals \farg{mu}, reals \farg{phi}}{    Generate a negative binomial variate with location \farg{mu} and    precision \farg{phi}; may only be used in generated quantities    block.  \farg{mu} must be less than $2 ^ {29}$.  For a description    of argument and return types, see section \@ref(prob-vectorization).} \end{description}


\section{Negative Binomial Distribution (log alternative parameterization)}\label{neg-binom-2-log.section}


Related to the parameterization in section \@ref(nbalt), the following parameterization uses a log mean parameter $\eta = \log(\mu)$, defined for $\eta \in \mathbb{R}$, $\phi \in \mathbb{R}^+$, so that for $y \in \mathbb{N}$, \[ \text{NegBinomial2Log}(y \, | \, \eta, \phi) = \text{NegBinomial2}(y | \exp(\eta), \phi). \] This alternative may be used for sampling, as a function, and for random number generation, but as of yet, there are no CDFs implemented for it.


\pitemdisc{y}{neg_binomial_2_log}{eta, phi}


\subsection{Stan Functions}


\begin{description}   \fitem{real}{neg_binomial_2_log_lpmf}{ints \farg{y} | reals     \farg{eta}, reals \farg{phi}}{The log negative binomial     probability mass of \farg{n} given log-location \farg{eta} and     inverse overdispersion control \farg{phi}. This is especially     useful for log-linear negative binomial regressions.}   \fitem{R}{neg_binomial_2_log_rng}{reals \farg{eta}, reals     \farg{phi}}{ Generate a negative binomial variate with     log-location \farg{eta} and inverse overdispersion control     \farg{phi}; may only be used in generated quantities     block. \farg{eta} must be less than $29 \log 2$.  For a     description of argument and return types, see     section \@ref(prob-vectorization).} \end{description}


\section{Negative-Binomial-2-Log Generalised Linear Model (Negative Binomial Regression)}\label{neg-binom-2-log-glm.section}


Stan also supplies a single primitive for a Generalised Linear Model with negative binomial likelihood and log link function, i.e. a primitive for a negative binomial regression. This should provide a more efficient implementation of negative binomial regression than a manually written regression in terms of a negative binomial likelihood and matrix multiplication.


\subsection{Probability Mass Function}


If $x\in \mathbb{R}^{n\cdot m}, \alpha \in \mathbb{R}^n, \beta\in \mathbb{R}^m, \phi\in \mathbb{R}^+$, then for $y \in \mathbb{N}^n$, \[ \text{NegBinomial2LogGLM}(y~|~x, \alpha, \beta, \phi) = \prod_{1\leq i \leq n}\text{NegBinomial2}(y_i~|~\exp(\alpha_i + x_i\cdot \beta), \phi). \] 

\pitemdisc{y}{neg_binomial_2_log_glm}{x, alpha, beta, phi}


\subsection{Stan Functions}


\begin{description} \fitem{real}{neg_binomial_2_log_glm_lpmf}{int[] \farg{y} | matrix \farg{x}, real \farg{alpha}, vector \farg{beta}, real \farg{phi}}{The   log negative binomial probability mass of \farg{y} given log-location   \code{\farg{alpha}+\farg{x}*\farg{beta}} and inverse overdispersion   control \code{\farg{phi}}, where a   constant intercept \code{\farg{alpha}} and \code{\farg{phi}} is used for all observations.   The number of rows of the independent variable matrix \code{\farg{x}} needs to   match the length of the dependent variable vector \code{\farg{y}} and   the number of columns of \code{\farg{x}} needs to match the length of   the weight vector \code{\farg{beta}}.} \end{description}


\begin{description} \fitem{real}{neg_binomial_2_log_glm_lpmf}{int[] \farg{y} | matrix \farg{x}, vector \farg{alpha}, vector \farg{beta}, real \farg{phi}}{The   log negative binomial probability mass of \farg{y} given log-location   \code{\farg{alpha}+\farg{x}*\farg{beta}} and inverse overdispersion   control \code{\farg{phi}}, where a   constant \code{\farg{phi}} is used for all observations and an intercept   \code{\farg{alpha}} is used that is allowed to vary with the different   observations.   The number of rows of the independent variable matrix \code{\farg{x}} needs to   match the length of the dependent variable vector \code{\farg{y}} and   \code{\farg{alpha}} and   the number of columns of \code{\farg{x}} needs to match the length of   the weight vector \code{\farg{beta}}.} \end{description}


\section{Poisson Distribution}\label{poisson.section}


\subsection{Probability Mass Function}


If $\lambda \in \mathbb{R}^+$, then for $n \in \mathbb{N}$, \[ \text{Poisson}(n|\lambda) = \frac{1}{n!} \, \lambda^n \, \exp(-\lambda). \] 

\pitemdisc{n}{poisson}{lambda}


\subsection{Stan Functions}


\begin{description} \fitem{real}{poisson_lpmf}{ints \farg{n} | reals \farg{lambda}}{The   log Poisson probability mass of \farg{n} given rate \farg{lambda}} \fitem{real}{poisson_cdf}{ints \farg{n}, reals \farg{lambda}}{The   Poisson cumulative distribution function of \farg{n} given rate \farg{lambda}} \fitem{real}{poisson_lcdf}{ints \farg{n} | reals \farg{lambda}}{The log of the   Poisson cumulative distribution function of \farg{n} given rate \farg{lambda}} \fitem{real}{poisson_lccdf}{ints \farg{n} | reals \farg{lambda}}{The log of the   Poisson complementary cumulative distribution function of \farg{n} given rate \farg{lambda}} \fitem{R}{poisson_rng}{reals \farg{lambda}}{Generate a Poisson   variate with rate \farg{lambda}; may only be used in generated   quantities block. \farg{lambda} must be less than $2^{30}$.  For a   description of argument and return types, see   section \@ref(prob-vectorization).} \end{description}


\section{Poisson Distribution, Log Parameterization}


Stan also provides a parameterization of the Poisson using the log rate $\alpha = \log \lambda$ as a parameter.  This is useful for log-linear Poisson regressions so that the predictor does not need to be exponentiated and passed into the standard Poisson probability function.


\subsection{Probability Mass Function}


If $\alpha \in \mathbb{R}$, then for $n \in \mathbb{N}$, \[ \text{PoissonLog}(n|\alpha) = \frac{1}{n!} \, \exp \left(n\alpha - \exp(\alpha) \right). \] 

\pitemdisc{n}{poisson_log}{alpha}


\subsection{Stan Functions}


\begin{description} \fitem{real}{poisson_log_lpmf}{ints \farg{n} | reals \farg{alpha}}{The   log Poisson probability mass of \farg{n} given log rate \farg{alpha}} \fitem{R}{poisson_log_rng}{reals \farg{alpha}}{Generate a Poisson   variate with log rate \farg{alpha}; may only be used in generated   quantities block. \farg{alpha} must be less than $30 \log 2$.  For a   description of argument and return types, see   section \@ref(prob-vectorization).} \end{description}


\section{Poisson-Log Generalised Linear Model (Poisson Regression)}\label{poisson-log-glm.section}


Stan also supplies a single primitive for a Generalised Linear Model with poisson likelihood and log link function, i.e. a primitive for a poisson regression. This should provide a more efficient implementation of poisson regression than a manually written regression in terms of a poisson likelihood and matrix multiplication.


\subsection{Probability Mass Function}


If $x\in \mathbb{R}^{n\cdot m}, \alpha \in \mathbb{R}^n, \beta\in \mathbb{R}^m$, then for $y \in \mathbb{N}^n$, \[ \text{PoisonLogGLM}(y|x, \alpha, \beta) = \prod_{1\leq i \leq n}\text{Poisson}(y_i|\exp(\alpha_i + x_i\cdot \beta)). \] 

\pitemdisc{y}{poisson_log_glm}{x, alpha, beta}


\subsection{Stan Functions}


\begin{description} \fitem{real}{poisson_log_glm_lpmf}{int[] \farg{y} | matrix \farg{x}, real \farg{alpha}, vector \farg{beta}}{The   log poisson probability mass of \farg{y} given log-rate   \code{\farg{alpha}+\farg{x}*\farg{beta}}, where a   constant intercept \code{\farg{alpha}} is used for all observations.   The number of rows of the independent variable matrix \code{\farg{x}} needs to   match the length of the dependent variable vector \code{\farg{y}} and   the number of columns of \code{\farg{x}} needs to match the length of   the weight vector \code{\farg{beta}}.} \end{description}


\begin{description} \fitem{real}{poisson_log_glm_lpmf}{int[] \farg{y} | matrix \farg{x}, vector \farg{alpha}, vector \farg{beta}}{The   log poisson probability mass of \farg{y} given log-rate   \code{\farg{alpha}+\farg{x}*\farg{beta}}, where an intercept   \code{\farg{alpha}} is used that is allowed to vary with the different   observations.   The number of rows of the independent variable matrix \code{\farg{x}} needs to   match the length of the dependent variable vector \code{\farg{y}} and   the number of columns of \code{\farg{x}} needs to match the length of   the weight vector \code{\farg{beta}}.} \end{description}


\chapter{Multivariate Discrete Distributions}


The multivariate discrete distributions are over multiple integer values, which are expressed in Stan as arrays.


\section{Multinomial Distribution}


\subsection{Probability Mass Function}


If $K \in \mathbb{N}$, $N \in \mathbb{N}$, and $\theta \in \text{$K$-simplex}$, then for $y \in \mathbb{N}^K$ such that $\sum_{k=1}^K y_k = N$, \[ \text{Multinomial}(y|\theta) = \binom{N}{y_1,\ldots,y_K} \prod_{k=1}^K \theta_k^{y_k}, \] where the multinomial coefficient is defined by \[ \binom{N}{y_1,\ldots,y_k} = \frac{N!}{\prod_{k=1}^K y_k!}. \] 

\pitemdisc{y}{multinomial}{theta}


\subsection{Stan Functions}


\begin{description}  \fitem{real}{multinomial_lpmf}{int[] \farg{y} | vector     \farg{theta}}{The log multinomial probability mass function with     outcome array \code{y} of size $K$ given the $K$-simplex     distribution parameter \farg{theta} and (implicit) total count     \code{N = sum(\farg{y})}} \fitem{int[]}{multinomial_rng}{vector \farg{theta}, int \farg{N}}{Generate a   multinomial variate with simplex distribution parameter \farg{theta} and   total count \farg{N}; may only be used in generated quantities block} \end{description}


\part{Continuous Distributions}\label{continuous-prob-functions.part}


\chapter{Unbounded Continuous Distributions}


The unbounded univariate continuous probability distributions have support on all real numbers.


\section{Normal Distribution}\label{normal-distribution.section}


\subsection{Probability Density Function}


If $\mu \in \mathbb{R}$ and $\sigma \in \mathbb{R}^+$, then for $y \in \mathbb{R}$, \[ \text{Normal}(y|\mu,\sigma) = \frac{1}{\sqrt{2 \pi} \ \sigma} \exp\left( - \, \frac{1}{2}            \left(  \frac{y - \mu}{\sigma} \right)^2     \right) \!. \] 

\pitem{y}{normal}{mu, sigma}


\subsection{Stan Functions}


\begin{description}   \fitem{real}{normal_lpdf}{reals \farg{y} | reals \farg{mu}, reals     \farg{sigma}}{The log of the normal density of \farg{y} given     location \farg{mu} and scale \farg{sigma}} \fitem{real}{normal_cdf}{reals \farg{y}, reals \farg{mu}, reals   \farg{sigma}}{The cumulative normal distribution of \farg{y}   given location \farg{mu} and scale \farg{sigma}; normal_cdf will   underflow to 0 for $\frac{{y}-{\mu}}{{\sigma}}$ below -37.5 and   overflow to 1 for $\frac{{y}-{\mu}}{{\sigma}}$ above 8.25;  the   function \code{Phi_approx} is more robust in the tails, but must be   scaled and translated for anything other than a standard normal.} \fitem{real}{normal_lcdf}{reals \farg{y} | reals \farg{mu}, reals   \farg{sigma}}{The log of the cumulative normal distribution of \farg{y}   given location \farg{mu} and scale \farg{sigma}; normal_lcdf   will underflow to $-\infty$ for $\frac{{y}-{\mu}}{{\sigma}}$ below   -37.5 and overflow to 0 for $\frac{{y}-{\mu}}{{\sigma}}$ above 8.25;  see above for discussion of \code{Phi_approx} as an alternative.} \fitem{real}{normal_lccdf}{reals \farg{y} | reals \farg{mu}, reals   \farg{sigma}}{The log of the complementary cumulative normal distribution   of \farg{y} given location \farg{mu} and scale \farg{sigma};   normal_lccdf will overflow to 0 for   $\frac{{y}-{\mu}}{{\sigma}}$ below -37.5 and underflow to $-\infty$   for $\frac{{y}-{\mu}}{{\sigma}}$ above 8.25;   see above for discussion of \code{Phi_approx} as an alternative.} \end{description}


\begin{description}   \fitem{R}{normal_rng}{reals \farg{mu}, reals \farg{sigma}}{Generate     a normal variate with location \farg{mu} and scale \farg{sigma};     may only be used in generated quantities block.  For a description     of argument and return types, see     section \@ref(prng-vectorization).} \end{description}


\subsection{Standard Normal Distribution}


The standard normal distribution is so-called because its parameters are the units for their respective operations---the location (mean) is zero and the scale (standard deviation) one.  The standard normal is parameter free and the unit parameters allow considerable simplification of the expression for the density. \[ \text{StdNormal}(y) \ = \ \text{Normal}(y \mid 0, 1) \ = \ \frac{1}{\sqrt{2 \pi}} \, \exp \left( \frac{-y^2}{2} \right)\!. \] Up to a proportion on the log scale, where Stan computes, \[ \log \text{Normal}(y \mid 0, 1) \ = \ \frac{-y^2}{2} + \text{const}. \] With no logarithm, no subtraction, and no division by a parameter, the standard normal log density is much more efficient to compute than the normal log density with constant location $0$ and scale $1$.


\subsection{Stan Functions}


Only the log probabilty density function is available for the standard normal distribution; for other functions, use the \code{normal_} versions with parameters $\mu = 0$ and $\sigma = 1$.


\begin{description} \fitem{real}{std_normal_lpdf}{reals \farg{y}}{The standard normal   (location zero, scale one) log probability density of \farg{y}.} \end{description}


\pitemTwo{y}{std_normal}


\section{Normal-Id Generalised Linear Model (Linear Regression)}\label{normal-id-glm.section}


Stan also supplies a single primitive for a Generalised Linear Model with normal likelihood and identity link function, i.e. a primitive for a linear regression. This should provide a more efficient implementation of linear regression than a manually written regression in terms of a normal likelihood and matrix multiplication.


\subsection{Probability Mass Function}


If $x\in \mathbb{R}^{n\cdot m}, \alpha \in \mathbb{R}^n, \beta\in \mathbb{R}^m, \sigma\in \mathbb{R}^+$, then for $y \in \mathbb{R}^n$, \[ \text{NormalIdGLM}(y|x, \alpha, \beta, \sigma) = \prod_{1\leq i \leq n}\text{Normal}(y_i|\alpha_i + x_i\cdot \beta, \sigma). \] 

\pitemdisc{y}{normal_id_glm}{x, alpha, beta, sigma}


\subsection{Stan Functions}


\begin{description} \fitem{real}{normal_id_glm_lpmf}{vector \farg{y} | matrix \farg{x}, real \farg{alpha}, vector \farg{beta}, real \farg{sigma}}{The   log normal probability density of \farg{y} given location   \code{\farg{alpha}+\farg{x}*\farg{beta}} and scale \code{\farg{sigma}}, where a   constant intercept \code{\farg{alpha}} and \code{\farg{sigma}} is used for all observations.   The number of rows of the independent variable matrix \code{\farg{x}} needs to   match the length of the dependent variable vector \code{\farg{y}} and   the number of columns of \code{\farg{x}} needs to match the length of   the weight vector \code{\farg{beta}}.} \end{description}


\begin{description} \fitem{real}{normal_id_glm_lpmf}{vector \farg{y} | matrix \farg{x}, vector \farg{alpha}, vector \farg{beta}, real \farg{sigma}}{The   log normal probability density of \farg{y} given location   \code{\farg{alpha}+\farg{x}*\farg{beta}} and scale \code{\farg{sigma}}, where a   constant \code{\farg{sigma}} is used for all observations and an intercept   \code{\farg{alpha}} is used that is allowed to vary with the different   observations.   The number of rows of the independent variable matrix \code{\farg{x}} needs to   match the length of the dependent variable vector \code{\farg{y}} and   the number of columns of \code{\farg{x}} needs to match the length of   the weight vector \code{\farg{beta}}.} \end{description}


\section{Exponentially Modified Normal Distribution}


\subsection{Probability Density Function}


If $\mu \in \mathbb{R}$, $\sigma \in \mathbb{R}^+$, and $\lambda \in \mathbb{R}^+$, then for $y \in \mathbb{R}$, \[ \text{ExpModNormal}(y|\mu,\sigma,\lambda) = \frac{\lambda}{2} \ \exp \left(\frac{\lambda}{2} \left(2\mu + \lambda \sigma^2 - 2y\right)\right) \text{erfc}\left(\frac{\mu + \lambda\sigma^2 - y}{\sqrt{2}\sigma}\right) . \] 

\pitem{y}{exp_mod_normal}{mu, sigma, lambda}


\subsection{Stan Functions}


\begin{description}   \fitemtwolines{real}{exp_mod_normal_lpdf}{reals \farg{y} | reals     \farg{mu}, reals \farg{sigma}}{reals \farg{lambda}}{The     log of the exponentially modified normal density of \farg{y} given     location \farg{mu}, scale \farg{sigma}, and shape \farg{lambda}}   \fitemtwolines{real}{exp_mod_normal_cdf}{reals \farg{y}, reals \farg{mu},     reals \farg{sigma}}{reals \farg{lambda}}{The     exponentially modified normal cumulative distribution function of \farg{y}     given location \farg{mu}, scale \farg{sigma}, and shape \farg{lambda}}   \fitemtwolines{real}{exp_mod_normal_lcdf}{reals \farg{y} | reals \farg{mu},     reals \farg{sigma}}{reals \farg{lambda}}{The log of the     exponentially modified normal cumulative distribution function of \farg{y}     given location \farg{mu}, scale \farg{sigma}, and shape \farg{lambda}}   \fitemtwolines{real}{exp_mod_normal_lccdf}{reals \farg{y} | reals \farg{mu},     reals \farg{sigma}}{reals \farg{lambda}}{The log of the     exponentially modified normal complementary cumulative distribution     function of \farg{y} given location \farg{mu}, scale \farg{sigma}, and shape \farg{lambda}} \end{description}


\begin{description}   \fitem{R}{exp_mod_normal_rng}{reals \farg{mu}, reals     \farg{sigma}, reals \farg{lambda}}{Generate a exponentially     modified normal variate with location \farg{mu}, scale     \farg{sigma}, and shape \farg{lambda}; may only be used in     generated quantities block.  For a description of argument and     return types, see section \@ref(prng-vectorization).} \end{description}


\section{Skew Normal Distribution}


\subsection{Probability Density Function}


If $\xi \in \mathbb{R}$, $\omega \in \mathbb{R}^+$, and $\alpha \in \mathbb{R}$, then for $y \in \mathbb{R}$, \[ \text{SkewNormal}(y \mid \xi, \omega, \alpha) = \frac{1}{\omega\sqrt{2\pi}} \ \exp\left( - \, \frac{1}{2}            \left(  \frac{y - \xi}{\omega} \right)^2     \right) \ \left(1 + \text{erf}\left( \alpha\left(\frac{y - \xi}{\omega\sqrt{2}}\right)\right)\right) . \] 

\pitem{y}{skew_normal}{xi, omega, alpha}


\subsection{Stan Functions}


\begin{description}   \fitem{real}{skew_normal_lpdf}{reals \farg{y} | reals \farg{xi}, reals     \farg{omega}, reals \farg{alpha}}{The log of the skew normal density 	of \farg{y} given location \farg{xi}, scale \farg{omega}, and 	shape \farg{alpha}} \fitem{real}{skew_normal_cdf}{reals \farg{y}, reals \farg{xi}, reals   \farg{omega}, reals \farg{alpha}}{The skew normal   distribution function of \farg{y} given location \farg{xi}, scale   \farg{omega}, and shape \farg{alpha}} \fitemtwolines{real}{skew_normal_lcdf}{reals \farg{y} | reals \farg{xi}, reals   \farg{omega}}{reals \farg{alpha}}{The log of the skew normal cumulative   distribution function of \farg{y} given location \farg{xi}, scale   \farg{omega}, and shape \farg{alpha}} \fitemtwolines{real}{skew_normal_lccdf}{reals \farg{y} | reals \farg{xi}, reals   \farg{omega}}{reals \farg{alpha}}{The log of the skew normal complementary    cumulative distribution function of \farg{y} given location \farg{xi}, scale   \farg{omega}, and shape \farg{alpha}} \end{description}


\begin{description}   \fitem{R}{skew_normal_rng}{reals \farg{xi}, reals \farg{omega},     real \farg{alpha}}{Generate a skew normal variate with location     \farg{xi}, scale \farg{omega}, and shape \farg{alpha}; may only be     used in generated quantities block.  For a description of argument     and return types, see section \@ref(prng-vectorization).} \end{description}


\section{Student-T Distribution}


\subsection{Probability Density Function}


If $\nu \in \mathbb{R}^+$, $\mu \in \mathbb{R}$, and $\sigma \in \mathbb{R}^+$, then for $y \in \mathbb{R}$, \[ \text{StudentT}(y|\nu,\mu,\sigma) = \frac{\Gamma\left((\nu + 1)/2\right)}      {\Gamma(\nu/2)} \ \frac{1}{\sqrt{\nu \pi} \ \sigma} \ \left( 1 + \frac{1}{\nu} \left(\frac{y - \mu}{\sigma}\right)^2 \right)^{-(\nu + 1)/2} \! . \] 

\pitem{y}{student_t}{nu, mu, sigma}


\subsection{Stan Functions}


\begin{description}   \fitem{real}{student_t_lpdf}{reals \farg{y} | reals \farg{nu}, reals     \farg{mu}, reals \farg{sigma}}{The log of the Student-$t$ density of \farg{y}     given degrees of freedom \farg{nu}, location \farg{mu}, and scale     \farg{sigma}}   \fitem{real}{student_t_cdf}{reals \farg{y}, reals \farg{nu}, reals     \farg{mu}, reals \farg{sigma}}{The Student-$t$ cumulative     distribution function of \farg{y}     given degrees of freedom \farg{nu}, location \farg{mu}, and scale     \farg{sigma}}   \fitem{real}{student_t_lcdf}{reals \farg{y} | reals \farg{nu}, reals     \farg{mu}, reals \farg{sigma}}{The log of the Student-$t$ cumulative     distribution function of \farg{y}     given degrees of freedom \farg{nu}, location \farg{mu}, and scale     \farg{sigma}}   \fitem{real}{student_t_lccdf}{reals \farg{y} | reals \farg{nu}, reals     \farg{mu}, reals \farg{sigma}}{The log of the Student-$t$ complementary     cumulative distribution function of \farg{y} given degrees of freedom     \farg{nu}, location \farg{mu}, and scale \farg{sigma}} \end{description}


\begin{description}   \fitem{R}{student_t_rng}{reals \farg{nu}, reals \farg{mu}, reals     \farg{sigma}}{Generate a Student-$t$ variate with degrees of     freedom \farg{nu}, location \farg{mu}, and scale \farg{sigma}; may     only be used in generated quantities block. For a description of     argument and return types, see section \@ref(prng-vectorization).} \end{description}


\section{Cauchy Distribution}


\subsection{Probability Density Function}


If $\mu \in \mathbb{R}$ and $\sigma \in \mathbb{R}^+$, then for $y \in \mathbb{R}$, \[ \text{Cauchy}(y|\mu,\sigma) = \frac{1}{\pi \sigma} \ \frac{1}{1 + \left((y - \mu)/\sigma\right)^2} . \] 

\pitem{y}{cauchy}{mu, sigma}


\subsection{Stan Functions}


\begin{description} \fitem{real}{cauchy_lpdf}{reals \farg{y} | reals \farg{mu}, reals  \farg{sigma}}{ The log of the Cauchy density of \farg{y} given location  \farg{mu} and scale \farg{sigma}} \fitem{real}{cauchy_cdf}{reals \farg{y}, reals \farg{mu}, reals  \farg{sigma}}{ The Cauchy cumulative distribution function of \farg{y} given   location \farg{mu} and scale \farg{sigma}} \fitem{real}{cauchy_lcdf}{reals \farg{y} | reals \farg{mu}, reals  \farg{sigma}}{ The log of the Cauchy cumulative distribution function 	of \farg{y} given location \farg{mu} and scale \farg{sigma}} \fitem{real}{cauchy_lccdf}{reals \farg{y} | reals \farg{mu}, reals  \farg{sigma}}{ The log of the Cauchy complementary cumulative distribution   function of \farg{y} given location \farg{mu} and scale \farg{sigma}} \end{description}


\begin{description}   \fitem{R}{cauchy_rng}{reals \farg{mu}, reals \farg{sigma}}{Generate     a Cauchy variate with location \farg{mu} and scale \farg{sigma};     may only be used in generated quantities block.  For a description     of argument and return types, see     section \@ref(prng-vectorization).} \end{description}


\section{Double Exponential (Laplace) Distribution}


\subsection{Probability Density Function}


If $\mu \in \mathbb{R}$ and $\sigma \in \mathbb{R}^+$, then for $y \in \mathbb{R}$, \[ \text{DoubleExponential}(y|\mu,\sigma) = \frac{1}{2\sigma}   \exp \left( - \, \frac{|y - \mu|}{\sigma} \right) . \] Note that the double exponential distribution is parameterized in terms of the scale, in contrast to the exponential distribution (see section \@ref(exponential-distribution)), which is parameterized in terms of inverse scale.


The double-exponential distribution can be defined as a compound exponential-normal distribution.  Specifically, if \[ \alpha \sim \mathsf{Exponential}\left( \frac{1}{\lambda} \right) \] and \[ \beta \sim \mathsf{Normal}(\mu, \alpha), \] then \[ \beta \sim \mathsf{DoubleExponential}(\mu, \lambda). \] This may be used to code a non-centered parameterization by taking \[ \beta^{\text{raw}} \sim \mathsf{Normal}(0, 1) \] and defining \[ \beta = \mu + \alpha \, \beta^{\text{raw}}. \] 

\pitem{y}{double_exponential}{mu, sigma}


\subsection{Stan Functions}


\begin{description} \fitem{real}{double_exponential_lpdf}{reals \farg{y} | reals \farg{mu}, reals \farg{sigma}}{ The log of the double exponential density of \farg{y} given location \farg{mu} and scale \farg{sigma}} \fitem{real}{double_exponential_cdf}{reals \farg{y}, reals \farg{mu}, reals \farg{sigma}}{ The double exponential cumulative distribution function of \farg{y} given location \farg{mu} and scale \farg{sigma}} \fitem{real}{double_exponential_lcdf}{reals \farg{y} | reals \farg{mu}, reals \farg{sigma}}{ The log of the double exponential cumulative distribution function of \farg{y} given location \farg{mu} and scale \farg{sigma}} \fitem{real}{double_exponential_lccdf}{reals \farg{y} | reals \farg{mu}, reals \farg{sigma}}{ The log of the double exponential complementary cumulative distribution function of \farg{y} given location \farg{mu} and scale \farg{sigma}} \end{description}


\begin{description}   \fitem{R}{double_exponential_rng}{reals \farg{mu}, reals     \farg{sigma}}{Generate a double exponential variate with location     \farg{mu} and scale \farg{sigma}; may only be used in generated     quantities block.  For a description of argument and return types,     see section \@ref(prng-vectorization).} \end{description}


\section{Logistic Distribution}


\subsection{Probability Density Function}


If $\mu \in \mathbb{R}$ and $\sigma \in \mathbb{R}^+$, then for $y \in \mathbb{R}$, \[ \text{Logistic}(y|\mu,\sigma) = \frac{1}{\sigma} \ \exp\!\left( - \, \frac{y - \mu}{\sigma} \right) \ \left(1 + \exp \!\left( - \, \frac{y - \mu}{\sigma} \right) \right)^{\!-2} \! . \] 

\pitem{y}{logistic}{mu, sigma}


\subsection{Stan Functions}


\begin{description} \fitem{real}{logistic_lpdf}{reals \farg{y} | reals \farg{mu}, reals \farg{sigma}}{ The log of the logistic density of \farg{y} given location \farg{mu} and scale \farg{sigma}} \fitem{real}{logistic_cdf}{reals \farg{y}, reals \farg{mu},   reals \farg{sigma}}{ The logistic cumulative distribution function of   \farg{y} given location \farg{mu} and scale \farg{sigma}} \fitem{real}{logistic_lcdf}{reals \farg{y} | reals \farg{mu},   reals \farg{sigma}}{ The log of the logistic cumulative distribution   function of \farg{y} given location \farg{mu} and scale \farg{sigma}} \fitem{real}{logistic_lccdf}{reals \farg{y} | reals \farg{mu},   reals \farg{sigma}}{ The log of the logistic complementary cumulative   distribution function of \farg{y} given location \farg{mu} and scale   \farg{sigma}} \end{description}


\begin{description}   \fitem{R}{logistic_rng}{reals \farg{mu}, reals     \farg{sigma}}{Generate a logistic variate with location \farg{mu}     and scale \farg{sigma}; may only be used in generated quantities     block.  For a description of argument and return types, see     section \@ref(prng-vectorization).} \end{description}


\section{Gumbel Distribution}


\subsection{Probability Density Function}


If $\mu \in \mathbb{R}$ and $\beta \in \mathbb{R}^+$, then for $y \in \mathbb{R}$, \[ \text{Gumbel}(y|\mu,\beta) = \frac{1}{\beta} \ \exp\left(-\frac{y-\mu}{\beta}-\exp\left(-\frac{y-\mu}{\beta}\right)\right) . \] 

\pitem{y}{gumbel}{mu, beta}


\subsection{Stan Functions}


\begin{description}   \fitem{real}{gumbel_lpdf}{reals \farg{y} | reals \farg{mu}, reals     \farg{beta}}{The log of the gumbel density of \farg{y} given location     \farg{mu} and scale \farg{beta}} \fitem{real}{gumbel_cdf}{reals \farg{y}, reals \farg{mu}, reals   \farg{beta}}{The gumbel cumulative distribution function of \farg{y}   given location \farg{mu} and scale \farg{beta}} \fitem{real}{gumbel_lcdf}{reals \farg{y} | reals \farg{mu}, reals   \farg{beta}}{The log of the gumbel cumulative distribution function of \farg{y}   given location \farg{mu} and scale \farg{beta}} \fitem{real}{gumbel_lccdf}{reals \farg{y} | reals \farg{mu}, reals   \farg{beta}}{The log of the gumbel complementary cumulative distribution    function of \farg{y} given location \farg{mu} and scale \farg{beta}} \end{description}


\begin{description}   \fitem{R}{gumbel_rng}{reals \farg{mu}, reals \farg{beta}}{Generate     a gumbel variate with location \farg{mu} and scale \farg{beta};     may only be used in generated quantities block.  For a description     of argument and return types, see     section \@ref(prng-vectorization).} \end{description}


\chapter{Positive Continuous Distributions}


The positive continuous probability functions have support on the positive real numbers.


\section{Lognormal Distribution}\label{lognormal.section}


\subsection{Probability Density Function}


If $\mu \in \mathbb{R}$ and $\sigma \in \mathbb{R}^+$, then for $y \in \mathbb{R}^+$, \[ \text{LogNormal}(y|\mu,\sigma) = \frac{1}{\sqrt{2 \pi} \ \sigma} \, \frac{1}{y} \ \exp \! \left(        - \, \frac{1}{2}        \, \left( \frac{\log y - \mu}{\sigma} \right)^2      \right) . \] 

\pitem{y}{lognormal}{mu, sigma}


\subsection{Stan Functions}


\begin{description}  \fitem{real}{lognormal_lpdf}{reals \farg{y} | reals \farg{mu}, reals   \farg{sigma}}{The log of the lognormal density of \farg{y} given location   \farg{mu} and scale \farg{sigma}} \fitem{real}{lognormal_cdf}{reals \farg{y}, reals \farg{mu}, reals   \farg{sigma}}{The cumulative lognormal distribution function of \farg{y}   given location \farg{mu} and scale \farg{sigma}} \fitem{real}{lognormal_lcdf}{reals \farg{y} | reals \farg{mu}, reals   \farg{sigma}}{The log of the lognormal cumulative distribution function    of \farg{y} given location \farg{mu} and scale \farg{sigma}} \fitem{real}{lognormal_lccdf}{reals \farg{y} | reals \farg{mu}, reals   \farg{sigma}}{The log of the lognormal complementary cumulative distribution   function of \farg{y} given location \farg{mu} and scale \farg{sigma}} \end{description}


\begin{description}   \fitem{R}{lognormal_rng}{reals \farg{mu}, reals     \farg{beta}}{Generate a lognormal variate with location \farg{mu}     and scale \farg{sigma}; may only be used in generated quantities     block.  For a description of argument and return types, see     section \@ref(prng-vectorization).} \end{description}


\section{Chi-Square Distribution}


\subsection{Probability Density Function}


If $\nu \in \mathbb{R}^+$, then for $y \in \mathbb{R}^+$, \[ \text{ChiSquare}(y|\nu) = \frac{2^{-\nu/2}}     {\Gamma(\nu / 2)} \, y^{\nu/2 - 1} \, \exp \! \left( -\, \frac{1}{2} \, y \right) . \] 

\pitem{y}{chi_square}{nu}


\subsection{Stan Functions}


\begin{description} \fitem{real}{chi_square_lpdf}{reals \farg{y} | reals \farg{nu}}{The   log of the Chi-square density of \farg{y} given degrees of freedom   \farg{nu}} \fitem{real}{chi_square_cdf}{reals \farg{y}, reals \farg{nu}}{The    Chi-square cumulative distribution function of \farg{y} given degrees    of freedom \farg{nu}} \fitem{real}{chi_square_lcdf}{reals \farg{y} | reals \farg{nu}}{The    log of the Chi-square cumulative distribution function of    \farg{y} given degrees of freedom \farg{nu}} \fitem{real}{chi_square_lccdf}{reals \farg{y} | reals \farg{nu}}{The    log of the complementary Chi-square cumulative distribution    function of \farg{y} given degrees of freedom \farg{nu}} \end{description}


\begin{description}   \fitem{R}{chi_square_rng}{reals \farg{nu}}{Generate a Chi-square     variate with degrees of freedom \farg{nu}; may only be used in     generated quantities block. For a description of argument and     return types, see section \@ref(prng-vectorization).} \end{description}


\section{Inverse Chi-Square Distribution}


\subsection{Probability Density Function}


If $\nu \in \mathbb{R}^+$, then for $y \in \mathbb{R}^+$, \[ \text{InvChiSquare}(y \, | \, \nu) = \frac{2^{-\nu/2}}    {\Gamma(\nu / 2)} \, y^{-\nu/2 - 1} \, \exp\! \left( \! - \, \frac{1}{2} \, \frac{1}{y} \right) . \] 

\pitem{y}{inv_chi_square}{nu}


\subsection{Stan Functions}


\begin{description} \fitem{real}{inv_chi_square_lpdf}{reals \farg{y} | reals   \farg{nu}}{The log of the inverse Chi-square density of \farg{y} given   degrees of freedom \farg{nu}} \fitem{real}{inv_chi_square_cdf}{reals \farg{y}, reals   \farg{nu}}{The inverse Chi-squared cumulative distribution function of   \farg{y} given degrees of freedom \farg{nu}} \fitem{real}{inv_chi_square_lcdf}{reals \farg{y} | reals   \farg{nu}}{The log of the inverse Chi-squared cumulative distribution    function of \farg{y} given degrees of freedom \farg{nu}} \fitem{real}{inv_chi_square_lccdf}{reals \farg{y} | reals   \farg{nu}}{The log of the inverse Chi-squared complementary cumulative   distribution function of \farg{y} given degrees of freedom \farg{nu}} \end{description}


\begin{description}   \fitem{R}{inv_chi_square_rng}{reals \farg{nu}}{Generate an     inverse Chi-squared variate with degrees of freedom \farg{nu}; may     only be used in generated quantities block.  For a description of     argument and return types, see section \@ref(prng-vectorization).} \end{description}


\section{Scaled Inverse Chi-Square Distribution}


\subsection{Probability Density Function}


If $\nu \in \mathbb{R}^+$ and $\sigma \in \mathbb{R}^+$, then for $y \in \mathbb{R}^+$, \[ \text{ScaledInvChiSquare}(y|\nu,\sigma) = \frac{(\nu / 2)^{\nu/2}}      {\Gamma(\nu / 2)} \, \sigma^{\nu} \, y^{-(\nu/2 + 1)} \, \exp \! \left( \!    - \, \frac{1}{2} \, \nu \, \sigma^2 \, \frac{1}{y} \right) . \] 

\pitem{y}{scaled_inv_chi_square}{nu, sigma}


\subsection{Stan Functions}


\begin{description}   \fitem{real}{scaled_inv_chi_square_lpdf}{reals \farg{y} | reals     \farg{nu}, reals \farg{sigma}}{The log of the scaled inverse Chi-square     density of \farg{y} given degrees of freedom \farg{nu} and scale     \farg{sigma}} \end{description}


\begin{description}   \fitem{real}{scaled_inv_chi_square_cdf}{reals \farg{y}, reals     \farg{nu}, reals \farg{sigma}}{The scaled inverse Chi-square     cumulative distribution function     of \farg{y} given degrees of freedom \farg{nu} and scale     \farg{sigma}}   \fitem{real}{scaled_inv_chi_square_lcdf}{reals \farg{y} | reals     \farg{nu}, reals \farg{sigma}}{The log of the scaled inverse Chi-square     cumulative distribution function     of \farg{y} given degrees of freedom \farg{nu} and scale     \farg{sigma}}   \fitem{real}{scaled_inv_chi_square_lccdf}{reals \farg{y} | reals     \farg{nu}, reals \farg{sigma}}{The log of the scaled inverse Chi-square     complementary cumulative distribution function     of \farg{y} given degrees of freedom \farg{nu} and scale     \farg{sigma}} \end{description}


\begin{description} \fitem{R}{scaled_inv_chi_square_rng}{reals \farg{nu}, reals   \farg{sigma}}{Generate a scaled inverse Chi-squared variate with degrees of freedom \farg{nu} and scale \farg{sigma}; may only be used in generated quantities block.   For a description of argument and return types, see section \@ref(prng-vectorization).} \end{description}


\section{Exponential Distribution}\label{exponential-distribution.section}


\subsection{Probability Density Function}


If $\beta \in \mathbb{R}^+$, then for $y \in \mathbb{R}^+$, \[ \text{Exponential}(y|\beta) = \beta \, \exp ( - \beta \, y ) . \] 

\pitem{y}{exponential}{beta}


\subsection{Stan Functions}


\begin{description}   \fitem{real}{exponential_lpdf}{reals \farg{y} | reals \farg{beta}}{The     log of the exponential density of \farg{y} given inverse scale \farg{beta}} \fitem{real}{exponential_cdf}{reals \farg{y}, reals \farg{beta}}{The   exponential cumulative distribution function of \farg{y} given inverse scale   \farg{beta}} \fitem{real}{exponential_lcdf}{reals \farg{y} | reals \farg{beta}}{The   log of the exponential cumulative distribution function of \farg{y} given   inverse scale \farg{beta}} \fitem{real}{exponential_lccdf}{reals \farg{y} | reals \farg{beta}}{The   log of the exponential complementary cumulative distribution function   of \farg{y} given inverse scale \farg{beta}} \end{description}


\begin{description}   \fitem{R}{exponential_rng}{reals \farg{beta}}{Generate an     exponential variate with inverse scale \farg{beta}; may only be     used in generated quantities block.  For a description of argument     and return types, see section \@ref(prng-vectorization).} \end{description}


\section{Gamma Distribution}


\subsection{Probability Density Function}


If $\alpha \in \mathbb{R}^+$ and $\beta \in \mathbb{R}^+$, then for $y \in \mathbb{R}^+$, \[ \text{Gamma}(y|\alpha,\beta) = \frac{\beta^{\alpha}}      {\Gamma(\alpha)} \, y^{\alpha - 1} \exp(-\beta \, y) . \] 

\pitem{y}{gamma}{alpha, beta}


\subsection{Stan Functions}


\begin{description}  \fitem{real}{gamma_lpdf}{reals \farg{y} | reals \farg{alpha}, reals   \farg{beta}}{The log of the gamma density of \farg{y} given shape   \farg{alpha} and inverse scale \farg{beta}}  \fitem{real}{gamma_cdf}{reals \farg{y}, reals \farg{alpha}, reals    \farg{beta}}{The cumulative gamma distribution function of \farg{y} given shape    \farg{alpha} and inverse scale \farg{beta}}  \fitem{real}{gamma_lcdf}{reals \farg{y} | reals \farg{alpha}, reals    \farg{beta}}{The log of the cumulative gamma distribution function of \farg{y} given shape    \farg{alpha} and inverse scale \farg{beta}}  \fitem{real}{gamma_lccdf}{reals \farg{y} | reals \farg{alpha}, reals    \farg{beta}}{The log of the complementary cumulative gamma distribution function of    \farg{y} given shape \farg{alpha} and inverse scale \farg{beta}} \end{description}


\begin{description}   \fitem{R}{gamma_rng}{reals \farg{alpha}, reals     \farg{beta}}{Generate a gamma variate with shape \farg{alpha} and     inverse scale \farg{beta}; may only be used in generated     quantities block.  For a description of argument and return types,     see section \@ref(prng-vectorization).} \end{description}


\section{Inverse Gamma Distribution}


\subsection{Probability Density Function}


If $\alpha \in \mathbb{R}^+$ and $\beta \in \mathbb{R}^+$, then for $y \in \mathbb{R}^+$, \[ \text{InvGamma}(y|\alpha,\beta) = \frac{\beta^{\alpha}}      {\Gamma(\alpha)} \ y^{-(\alpha + 1)} \, \exp \! \left( \! - \beta \, \frac{1}{y} \right) . \] 

\pitem{y}{inv_gamma}{alpha, beta}


\subsection{Stan Functions}


\begin{description} \fitem{real}{inv_gamma_lpdf}{reals \farg{y} | reals \farg{alpha}, reals   \farg{beta}}{The log of the inverse gamma density of \farg{y} given shape   \farg{alpha} and scale \farg{beta}} \fitem{real}{inv_gamma_cdf}{reals \farg{y}, reals \farg{alpha}, reals   \farg{beta}}{The inverse gamma cumulative distribution function of \farg{y}   given shape \farg{alpha} and scale \farg{beta}} \fitem{real}{inv_gamma_lcdf}{reals \farg{y} | reals \farg{alpha}, reals   \farg{beta}}{The log of the inverse gamma cumulative distribution function   of \farg{y} given shape \farg{alpha} and scale \farg{beta}} \fitem{real}{inv_gamma_lccdf}{reals \farg{y} | reals \farg{alpha}, reals   \farg{beta}}{The log of the inverse gamma complementary cumulative distribution   function of \farg{y} given shape \farg{alpha} and scale \farg{beta}} \end{description}


\begin{description}   \fitem{R}{inv_gamma_rng}{reals \farg{alpha}, reals     \farg{beta}}{Generate an inverse gamma variate with shape     \farg{alpha} and scale \farg{beta}; may only be used in generated     quantities block.  For a description of argument and return types,     see section \@ref(prng-vectorization).} \end{description}


\section{Weibull Distribution}


\subsection{Probability Density Function}


If $\alpha \in \mathbb{R}^+$ and $\sigma \in \mathbb{R}^+$, then for $y \in [0,\infty)$, \[ \text{Weibull}(y|\alpha,\sigma) = \frac{\alpha}{\sigma} \, \left( \frac{y}{\sigma} \right)^{\alpha - 1} \, \exp \! \left( \! - \left( \frac{y}{\sigma} \right)^{\alpha}  \right) . \] 

Note that if $Y \propto \text{Weibull}(\alpha,\sigma)$, then $Y^{-1} \propto \text{Frechet}(\alpha,\sigma^{-1})$.


\pitem{y}{weibull}{alpha, sigma}


\subsection{Stan Functions}


\begin{description} \fitem{real}{weibull_lpdf}{reals \farg{y} | reals \farg{alpha}, reals   \farg{sigma}}{The log of the Weibull density of \farg{y} given shape   \farg{alpha} and scale \farg{sigma}} \fitem{real}{weibull_cdf}{reals \farg{y}, reals \farg{alpha}, reals   \farg{sigma}}{The Weibull cumulative distribution function of \farg{y} given   shape \farg{alpha} and scale \farg{sigma}} \fitem{real}{weibull_lcdf}{reals \farg{y} | reals \farg{alpha}, reals   \farg{sigma}}{The log of the Weibull cumulative distribution function of   \farg{y} given shape \farg{alpha} and scale \farg{sigma}} \fitem{real}{weibull_lccdf}{reals \farg{y} | reals \farg{alpha}, reals   \farg{sigma}}{The log of the Weibull complementary cumulative   distribution function of \farg{y} given shape \farg{alpha} and scale   \farg{sigma}} \end{description}


\begin{description}   \fitem{R}{weibull_rng}{reals \farg{alpha}, reals     \farg{sigma}}{Generate a weibull variate with shape \farg{alpha}     and scale \farg{sigma}; may only be used in generated quantities     block. For a description of argument and return types, see     section \@ref(prng-vectorization).} \end{description}


\section{Frechet Distribution}


\subsection{Probability Density Function}


If $\alpha \in \mathbb{R}^+$ and $\sigma \in \mathbb{R}^+$, then for $y \in \mathbb{R}^+$, \[ \text{Frechet}(y|\alpha,\sigma) = \frac{\alpha}{\sigma} \, \left( \frac{y}{\sigma} \right)^{-\alpha - 1} \, \exp \! \left( \! - \left( \frac{y}{\sigma} \right)^{-\alpha}  \right) . \] 

Note that if $Y \propto \text{Frechet}(\alpha,\sigma)$, then $Y^{-1} \propto \text{Weibull}(\alpha,\sigma^{-1})$.


\pitem{y}{frechet}{alpha, sigma}


\subsection{Stan Functions}


\begin{description} \fitem{real}{frechet_lpdf}{reals \farg{y} | reals \farg{alpha}, reals   \farg{sigma}}{The log of the Frechet density of \farg{y} given shape   \farg{alpha} and scale \farg{sigma}} \fitem{real}{frechet_cdf}{reals \farg{y}, reals \farg{alpha}, reals   \farg{sigma}}{The Frechet cumulative distribution function of \farg{y} given   shape \farg{alpha} and scale \farg{sigma}} \fitem{real}{frechet_lcdf}{reals \farg{y} | reals \farg{alpha}, reals   \farg{sigma}}{The log of the Frechet cumulative distribution function of   \farg{y} given shape \farg{alpha} and scale \farg{sigma}} \fitem{real}{frechet_lccdf}{reals \farg{y} | reals \farg{alpha}, reals   \farg{sigma}}{The log of the Frechet complementary cumulative   distribution function of \farg{y} given shape \farg{alpha} and scale   \farg{sigma}} \end{description}


\begin{description}   \fitem{R}{frechet_rng}{reals \farg{alpha}, reals     \farg{sigma}}{Generate an Frechet variate with shape     \farg{alpha} and scale \farg{sigma}; may only be used in generated     quantities block.  For a description of argument and return types,     see section \@ref(prng-vectorization).} \end{description}


\chapter{Non-negative Continuous Distributions}


The non-negative continuous probability functions have support on the non-negative real numbers.


\section{Rayleigh Distribution}


\subsection{Probability Density Function}

If $\sigma \in \mathbb{R}^+$, then for $y \in [0,\infty)$, \[ \text{Rayleigh}(y|\sigma) = \frac{y}{\sigma^2} \exp(-y^2 / 2\sigma^2) \!. \] 

\pitem{y}{rayleigh}{sigma}


\subsection{Stan Functions}

\begin{description} \fitem{real}{rayleigh_lpdf}{reals \farg{y} | reals \farg{sigma}}{The log of the Rayleigh  density of \farg{y} given scale \farg{sigma}} \fitem{real}{rayleigh_cdf}{real \farg{y}, real \farg{sigma}}{The Rayleigh cumulative distribution of \farg{y} given scale \farg{sigma}} \fitem{real}{rayleigh_lcdf}{real \farg{y} | real \farg{sigma}}{The log of the  Rayleigh cumulative distribution of \farg{y} given scale \farg{sigma}} \fitem{real}{rayleigh_lccdf}{real \farg{y} | real \farg{sigma}}{The log of the Rayleigh complementary cumulative distribution  of \farg{y} given scale \farg{sigma}} \end{description}


\begin{description}   \fitem{R}{rayleigh_rng}{reals \farg{sigma}}{Generate a Rayleigh     variate with scale \farg{sigma}; may only be used in generated     quantities block. For a description of argument and return types,     see section \@ref(prng-vectorization).} \end{description}


\section{Wiener First Passage Time Distribution}


\subsection{Probability Density Function}


If $\alpha \in \mathbb{R}^+$, $\tau \in \mathbb{R}^+$, $\beta \in [0, 1]$ and $\delta \in \mathbb{R}$, then for $y > \tau$, \[ \text{Wiener}(y|\alpha, \tau, \beta, \delta) = \frac{\alpha^3}{(y-\tau)^{3/2}} \exp \! \left(- \delta \alpha \beta - \frac{\delta^2(y-\tau)}{2}\right) \sum_{k = - \infty}^{\infty} (2k + \beta) \phi \! \left(\frac{2k \alpha + \beta}{\sqrt{y - \tau}}\right) \] where $\phi(x)$ denotes the standard normal density function;  see [@Feller1968], [@NavarroFuss2009].


\pitem{y}{wiener}{alpha, tau, beta, delta}


\subsection{Stan Functions}

\begin{description} \fitemtwolines{real}{wiener_lpdf}{reals \farg{y} | reals \farg{alpha},  reals \farg{tau}, reals \farg{beta}}{reals \farg{delta}}  {The log of the Wiener first passage time  density of \farg{y} given boundary separation \farg{alpha},  non-decision time \farg{tau}, a-priori bias \farg{beta} and  drift rate \farg{delta}} \end{description}


\subsection{Boundaries}


Stan returns the first passage time of the accumulation process over the upper boundary only. To get the result for the lower boundary, use \[ \text{wiener}(y | \alpha, \tau, 1 - \beta, - \delta) \] For more details, see the appendix of @Vandekerckhove-Wabersich:2014.


\chapter{Positive Lower-Bounded Probabilities}


The positive lower-bounded probabilities have support on real values above some positive minimum value.


\section{Pareto Distribution}


\subsection{Probability Density Function}


If $y_{\text{min}} \in \mathbb{R}^+$ and $\alpha \in \mathbb{R}^+$, then for $y \in \mathbb{R}^+$ with $y \geq y_{\text{min}}$, \[ \text{Pareto}(y|y_{\text{min}},\alpha) = \frac{\displaystyle \alpha\,y_{\text{min}}^\alpha}{\displaystyle y^{\alpha+1}}. \] 

\pitem{y}{pareto}{y_min, alpha}


\subsection{Stan Functions}


\begin{description}   \fitem{real}{pareto_lpdf}{reals \farg{y} | reals \farg{y_min}, reals     \farg{alpha}}{The log of the Pareto density of \farg{y} given     positive minimum value \farg{y_min} and shape \farg{alpha}}   \fitem{real}{pareto_cdf}{reals \farg{y}, reals \farg{y_min}, reals     \farg{alpha}}{The Pareto cumulative distribution function of \farg{y} given     positive minimum value \farg{y_min} and shape \farg{alpha}}   \fitem{real}{pareto_lcdf}{reals \farg{y} | reals \farg{y_min}, reals     \farg{alpha}}{The log of the Pareto cumulative distribution function of     \farg{y} given positive minimum value \farg{y_min} and shape \farg{alpha}}   \fitem{real}{pareto_lccdf}{reals \farg{y} | reals \farg{y_min}, reals     \farg{alpha}}{The log of the Pareto complementary cumulative distribution     function of \farg{y} given positive minimum value \farg{y_min} and shape     \farg{alpha}} \end{description}


\begin{description} \fitem{R}{pareto_rng}{reals \farg{y_min}, reals   \farg{alpha}}{Generate a Pareto variate with positive minimum value   \farg{y_min} and shape \farg{alpha}; may only be used in generated   quantities block.  For a description of argument and return types,     see section \@ref(prng-vectorization).} \end{description}


\section{Pareto Type 2 Distribution}


\subsection{Probability Density Function}


If $\mu \in \mathbb{R}$, $\lambda \in \mathbb{R}^+$, and $\alpha \in \mathbb{R}^+$, then for $y \geq \mu$, \[ \mathrm{Pareto\_Type\_2}(y|\mu,\lambda,\alpha) = \ \frac{\alpha}{\lambda} \, \left( 1+\frac{y-\mu}{\lambda} \right)^{-(\alpha+1)} \! . \] 

Note that the Lomax distribution is a Pareto Type 2 distribution with $\mu=0$.


\pitem{y}{pareto_type_2}{mu, lambda, alpha}


\subsection{Stan Functions}


\begin{description}   \fitemtwolines{real}{pareto_type_2_lpdf}{reals \farg{y} | reals \farg{mu}, reals \farg{lambda}}{reals     \farg{alpha}}{The log of the Pareto Type 2 density of \farg{y} given     location \farg{mu}, scale \farg{lambda}, and shape \farg{alpha}}   \fitemtwolines{real}{pareto_type_2_cdf}{reals \farg{y}, reals \farg{mu}, reals \farg{lambda}}{reals     \farg{alpha}}{The Pareto Type 2 cumulative distribution function of \farg{y} given     location \farg{mu}, scale \farg{lambda}, and shape \farg{alpha}}   \fitemtwolines{real}{pareto_type_2_lcdf}{reals \farg{y} | reals \farg{mu}, reals \farg{lambda}}{reals     \farg{alpha}}{The log of the Pareto Type 2 cumulative distribution function of \farg{y} given     location \farg{mu}, scale \farg{lambda}, and shape \farg{alpha}}   \fitemtwolines{real}{pareto_type_2_lccdf}{reals \farg{y} | reals \farg{mu}, reals \farg{lambda}}{reals     \farg{alpha}}{The log of the Pareto Type 2 complementary cumulative distribution function      of \farg{y} given location \farg{mu}, scale \farg{lambda}, and shape \farg{alpha}} \end{description}


\begin{description}   \fitem{R}{pareto_type_2_rng}{reals \farg{mu}, reals     \farg{lambda}, reals \farg{alpha}}{Generate a Pareto Type 2     variate with location \farg{mu}, scale \farg{lambda}, and shape     \farg{alpha}; may only be used in generated quantities block.  For     a description of argument and return types, see     section \@ref(prng-vectorization).} \end{description}


\chapter{Continuous Distributions on [0, 1]}


The continuous distributions with outcomes in the interval $[0,1]$ are used to characterized bounded quantities, including probabilities.


\section{Beta Distribution}


\subsection{Probability Density Function}


If $\alpha \in \mathbb{R}^+$ and $\beta \in \mathbb{R}^+$, then for $\theta \in (0,1)$, \[ \text{Beta}(\theta|\alpha,\beta) = \frac{1}{\mathrm{B}(\alpha,\beta)} \, \theta^{\alpha - 1} \, (1 - \theta)^{\beta - 1} , \] where the beta function $\mathrm{B}()$ is as defined in section \@ref(betafun).


 _**Warning:**_  If $\theta = 0$ or $\theta = 1$, then the probability is 0 and the log probability is $-\infty$.  Similarly, the distribution requires strictly positive parameters, $\alpha, \beta > 0$.


\pitem{theta}{beta}{alpha, beta}


\subsection{Stan Functions}


\begin{description}   \fitem{real}{beta_lpdf}{reals \farg{theta} | reals \farg{alpha}, reals     \farg{beta}}{The log of the beta density of \code{theta} in $[0,1]$     given positive prior successes (plus one)     \farg{alpha} and prior failures (plus one) \farg{beta}}   \fitem{real}{beta_cdf}{reals \farg{theta}, reals \farg{alpha}, reals     \farg{beta}}{The beta cumulative distribution function of \code{theta}     in $[0,1]$ given positive prior successes (plus one)     \farg{alpha} and prior failures (plus one) \farg{beta}}   \fitem{real}{beta_lcdf}{reals \farg{theta} | reals \farg{alpha}, reals     \farg{beta}}{The log of the beta cumulative distribution function of     \code{theta} in $[0,1]$ given positive prior successes (plus one)     \farg{alpha} and prior failures (plus one) \farg{beta}}   \fitem{real}{beta_lccdf}{reals \farg{theta} | reals \farg{alpha}, reals     \farg{beta}}{The log of the beta complementary cumulative distribution     function of \code{theta} in $[0,1]$ given positive prior successes (plus one)     \farg{alpha} and prior failures (plus one) \farg{beta}} \end{description}


\begin{description}   \fitem{R}{beta_rng}{reals \farg{alpha}, reals \farg{beta}}{Generate     a beta variate with positive prior successes (plus one)     \farg{alpha} and prior failures (plus one) \farg{beta}; may only     be used in generated quantities block.  For a description of     argument and return types, see section \@ref(prng-vectorization).} \end{description}


\section{Beta Proportion Distribution}


\subsection{Probability Density Function}


If $\mu \in (0, 1)$ and $\kappa \in \mathbb{R}^+$, then for $\theta \in (0,1)$, \[ \mathrm{Beta\_Proportion}(\theta|\mu,\kappa) = \frac{1}{\mathrm{B}(\mu \kappa, (1 - \mu) \kappa)} \, \theta^{\mu\kappa - 1} \, (1 - \theta)^{(1 - \mu)\kappa- 1} , \] where the beta function $\mathrm{B}()$ is as defined in section \@ref(betafun).


 _**Warning:**_  If $\theta = 0$ or $\theta = 1$, then the probability is 0 and the log probability is $-\infty$.  Similarly, the distribution requires $\mu \in (0, 1)$ and strictly positive parameter, $\kappa > 0$.


\pitem{theta}{beta_proportion}{mu, kappa}


\subsection{Stan Functions}


\begin{description}   \fitem{real}{beta_proportion_lpdf}{reals \farg{theta} |     reals \farg{mu}, reals \farg{kappa}}{The log of the beta_proportion density     of \code{theta} in $(0,1)$ given mean \farg{mu} and     precision \farg{kappa}}   \fitem{real}{beta_proportion_lcdf}{reals \farg{theta} |     reals \farg{mu}, reals \farg{kappa}}{The log of the beta_proportion     cumulative distribution function of \code{theta} in $(0,1)$ given     mean \farg{mu} and precision \farg{kappa}}   \fitem{real}{beta_proportion_lccdf}{reals \farg{theta} | reals     \farg{mu}, reals \farg{kappa}}{The log of the beta_proportion complementary     cumulative distribution function of \code{theta} in $(0,1)$ given     mean \farg{mu} and precision \farg{kappa}} \end{description}


\begin{description}   \fitem{R}{beta_proportion_rng}{reals \farg{mu}, reals     \farg{kappa}}{Generate a beta_proportion variate with mean \farg{mu} and     precision \farg{kappa}; may only be used in generated quantities     block.  For a description of argument and return types, see     section \@ref(prng-vectorization).} \end{description}


\chapter{Circular Distributions}


Circular distributions are defined for finite values \farg{y} in any interval of length $2\pi$.


\section{Von Mises Distribution}


\subsection{Probability Density Function}


If $\mu \in \mathbb{R}$ and $\kappa \in \mathbb{R}^+$, then for $y \in \mathbb{R}$, \[ \text{VonMises}(y|\mu,\kappa) = \frac{\exp(\kappa\cos(y-\mu))}{2\pi I_0(\kappa)} \!. \] In order for this density to properly normalize, $y$ must be restricted to some interval  $(c, c + 2\pi)$ of length $2 \pi$, because \[ \int_{c}^{c + 2\pi} \text{VonMises}(y|\mu,\kappa) dy = 1. \] Similarly, if $\mu$ is a parameter, it will typically be restricted to the same range as $y$.


A von Mises distribution with its $2 \pi$ interval of support centered around its location $\mu$ will have a single mode at $\mu$; for example, restricting $y$ to $(-\pi,\pi)$ and taking $\mu = 0$ leads to a single local optimum at the model $\mu$.  If the location $\mu$ is not in the center of the support, the density is circularly translated and there will be a second local maximum at the boundary furthest from the mode.  Ideally, the parameterization and support will be set up so that the bulk of the probability mass is in a continuous interval around the mean $\mu$.


\pitem{y}{von_mises}{mu, kappa}


\subsection{Stan Functions}


\begin{description}   \fitem{R}{von_mises_lpdf}{reals \farg{y} | reals     \farg{mu}, reals \farg{kappa}}{The log of the von mises density of     \farg{y} given location \farg{mu} and scale \farg{kappa}.  For a     description of argument and return types, see     section \@ref(prob-vectorization).} \end{description}


\begin{description}   \fitem{R}{von_mises_rng}{reals \farg{mu}, reals     \farg{kappa}}{Generate a Von Mises variate with location \farg{mu}     and scale \farg{kappa} (i.e. returns values in the interval     $[(\mu \mod 2\pi)-\pi,(\mu \mod 2\pi)+\pi]$); may only be used in     generated quantities block.  For a description of argument and     return types, see section \@ref(prng-vectorization).} \end{description}


\subsection{Numerical Stability}


Evaluating the Von Mises distribution for $\kappa > 100$ is numerically unstable in the current implementation.  Nathanael I.\ Lichti suggested the following workaround on the Stan users group, based on the fact that as $\kappa \rightarrow \infty$, \[ \text{VonMises}(y|\mu,\kappa) \rightarrow \text{Normal}(\mu, \sqrt{1 / \kappa}). \] The workaround is to replace `y ~ von_mises(mu,kappa)` with


```\n if (kappa < 100)\n   y ~ von_mises(mu, kappa);\n else\n   y ~ normal(mu, sqrt(1 / kappa));\n ```


\chapter{Bounded Continuous Probabilities}


The bounded continuous probabilities have support on a finite interval of real numbers.


\section{Uniform Distribution}


\subsection{Probability Density Function}


If $\alpha \in \mathbb{R}$ and $\beta \in (\alpha,\infty)$, then for $y \in [\alpha,\beta]$, \[ \text{Uniform}(y|\alpha,\beta) = \frac{1}{\beta - \alpha} . \] 

\pitem{y}{uniform}{alpha, beta}


\subsection{Stan Functions}


\begin{description} \fitem{real}{uniform_lpdf}{reals \farg{y} | reals \farg{alpha}, reals   \farg{beta}}{The log of the uniform density of \farg{y} given lower bound   \farg{alpha} and upper bound \farg{beta}} \fitem{real}{uniform_cdf}{reals \farg{y}, reals \farg{alpha}, reals   \farg{beta}}{The uniform cumulative distribution function of \farg{y} given   lower bound \farg{alpha} and upper bound \farg{beta}} \fitem{real}{uniform_lcdf}{reals \farg{y} | reals \farg{alpha}, reals   \farg{beta}}{The log of the uniform cumulative distribution function of   \farg{y} given lower bound \farg{alpha} and upper bound \farg{beta}} \fitem{real}{uniform_lccdf}{reals \farg{y} | reals \farg{alpha}, reals   \farg{beta}}{The log of the uniform complementary cumulative distribution   function of \farg{y} given lower bound \farg{alpha} and upper bound   \farg{beta}} \end{description}


\begin{description}   \fitem{R}{uniform_rng}{reals \farg{alpha}, reals     \farg{beta}}{Generate a uniform variate with lower bound     \farg{alpha} and upper bound \farg{beta}; may only be used in     generated quantities block.  For a description of argument and     return types, see section \@ref(prng-vectorization).} \end{description}


\chapter{Distributions over Unbounded Vectors}


The unbounded vector probability distributions have support on all of $\mathbb{R}^K$ for some fixed $K$.


\section{Multivariate Normal Distribution}


\subsection{Probability Density Function}


If $K \in \mathbb{N}$, $\mu \in \mathbb{R}^K$, and $\Sigma \in \mathbb{R}^{K \times   K}$ is symmetric and positive definite, then for $y \in \mathbb{R}^K$, \[ \text{MultiNormal}(y|\mu,\Sigma) = \frac{1}{\left( 2 \pi \right)^{K/2}} \ \frac{1}{\sqrt{|\Sigma|}} \ \exp \! \left( \! - \frac{1}{2} (y - \mu)^{\top} \, \Sigma^{-1} \, (y - \mu) \right) \! , \] where $|\Sigma|$ is the absolute determinant of $\Sigma$.


\pitem{y}{multi_normal}{mu, Sigma}


\subsection{Stan Functions}


The multivariate normal probability function is overloaded to allow the variate vector $y$ and location vector $\mu$ to be vectors or row vectors (or to mix the two types).  The density function is also vectorized, so it allows arrays of row vectors or vectors as arguments; see section \@ref(prob-vectorization) for a description of vectorization.


\begin{description}   \fitem{real}{multi_normal_lpdf}{vectors \farg{y} | vectors     \farg{mu}, matrix \farg{Sigma}}{The log of the multivariate normal     density of vector(s) \farg{y} given location vector(s) \farg{mu} and     covariance matrix \farg{Sigma}}   \fitem{real}{multi_normal_lpdf}{vectors \farg{y} | row_vectors     \farg{mu}, matrix \farg{Sigma}}{The log of the multivariate normal     density of vector(s) \farg{y} given location row vector(s)     \farg{mu} and covariance matrix \farg{Sigma}}   \fitem{real}{multi_normal_lpdf}{row_vectors \farg{y} | vectors     \farg{mu}, matrix \farg{Sigma}}{The log of the multivariate normal     density of row vector(s) \farg{y} given location vector(s)     \farg{mu} and covariance matrix \farg{Sigma}}   \fitem{real}{multi_normal_lpdf}{row_vectors \farg{y} | row_vectors     \farg{mu}, matrix \farg{Sigma}}{The log of the multivariate normal     density of row vector(s) \farg{y} given location row vector(s)     \farg{mu} and covariance matrix \farg{Sigma}} \end{description}


Although there is a direct multi-normal RNG function, if more than one result is required, it's much more efficient to Cholesky factor the covariance matrix and call \code{multi_normal_cholesky_rng};  see section \@ref(multi-normal-cholesky-fun).


\begin{description} \fitem{vector}{multi_normal_rng}{vector \farg{mu}, matrix   \farg{Sigma}}{Generate a multivariate normal variate with location   \farg{mu} and covariance matrix \farg{Sigma}; may only be used in generated quantities block} \fitem{vector}{multi_normal_rng}{row_vector \farg{mu}, matrix   \farg{Sigma}}{Generate a multivariate normal variate with location   \farg{mu} and covariance matrix \farg{Sigma}; may only be used in generated quantities block} \fitem{vectors}{multi_normal_rng}{vectors \farg{mu}, matrix   \farg{Sigma}}{Generate an array of multivariate normal variates with locations   \farg{mu} and covariance matrix \farg{Sigma}; may only be used in generated quantities block} \fitem{vectors}{multi_normal_rng}{row_vectors \farg{mu}, matrix   \farg{Sigma}}{Generate an array of multivariate normal variates with locations   \farg{mu} and covariance matrix \farg{Sigma}; may only be used in generated quantities block} \end{description}


\section{Multivariate Normal Distribution, Precision Parameterization}


\subsection{Probability Density Function}


If $K \in \mathbb{N}$, $\mu \in \mathbb{R}^K$, and $\Omega \in \mathbb{R}^{K \times   K}$ is symmetric and positive definite, then for $y \in \mathbb{R}^K$, \[ \text{MultiNormalPrecision}(y|\mu,\Omega) = \text{MultiNormal}(y|\mu,\Sigma^{-1}) \] 

\pitem{y}{multi_normal_prec}{mu, Omega}


\subsection{Stan Functions}


\begin{description} \fitemtwolines{real}{multi_normal_prec_lpdf}{vectors \farg{y} | vectors   \farg{mu}}{matrix \farg{Omega}}{The log of the multivariate normal density of vector(s)   \farg{y} given location vector(s) \farg{mu} and positive definite precision   matrix \farg{Omega}} \fitemtwolines{real}{multi_normal_prec_lpdf}{vectors \farg{y} | row_vectors   \farg{mu}}{matrix \farg{Omega}}{The log of the multivariate normal density of vector(s)   \farg{y} given location row vector(s) \farg{mu} and positive definite precision   matrix \farg{Omega}} \fitemtwolines{real}{multi_normal_prec_lpdf}{row_vectors \farg{y} | vectors   \farg{mu}}{matrix \farg{Omega}}{The log of the multivariate normal density of row vector(s)   \farg{y} given location vector(s) \farg{mu} and positive definite precision   matrix \farg{Omega}} \fitemtwolines{real}{multi_normal_prec_lpdf}{row_vectors \farg{y} | row_vectors   \farg{mu}}{matrix \farg{Omega}}{The log of the multivariate normal density of row vector(s)   \farg{y} given location row vector(s) \farg{mu} and positive definite precision   matrix \farg{Omega}} \end{description}


\section{Multivariate Normal Distribution, Cholesky Parameterization}\label{multi-normal-cholesky-fun.section}


\subsection{Probability Density Function}


If $K \in \mathbb{N}$, $\mu \in \mathbb{R}^K$, and $L \in \mathbb{R}^{K \times K}$ is lower triangular and such that $LL^{\top}$ is positive definite, then for $y \in \mathbb{R}^K$, \[ \text{MultiNormalCholesky}(y|\mu,L) = \text{MultiNormal}(y|\mu,LL^{\top}). \] If $L$ is lower triangular and $LL^{top}$ is a $K \times K$ positive definite matrix, then $L_{k,k}$ must be strictly positive for $k \in 1{:}K$.  If an $L$ is provided that is not the Cholesky factor of a positive-definite matrix, the probability functions will raise errors.


\pitem{y}{multi_normal_cholesky}{mu, L}


\subsection{Stan Functions}


\begin{description} \fitemtwolines{real}{multi_normal_cholesky_lpdf}{vectors \farg{y} | vectors   \farg{mu}}{matrix \farg{L}}{The log of the multivariate normal density of vector(s)   \farg{y} given location vector(s) \farg{mu} and lower-triangular Cholesky   factor of the covariance matrix \farg{L}} \fitemtwolines{real}{multi_normal_cholesky_lpdf}{vectors \farg{y} | row_vectors   \farg{mu}}{matrix \farg{L}}{The log of the multivariate normal density of vector(s)   \farg{y} given location row vector(s) \farg{mu} and lower-triangular Cholesky   factor of the covariance matrix \farg{L}} \fitemtwolines{real}{multi_normal_cholesky_lpdf}{row_vectors \farg{y} | vectors   \farg{mu}}{matrix \farg{L}}{The log of the multivariate normal density of row vector(s)   \farg{y} given location vector(s) \farg{mu} and lower-triangular Cholesky   factor of the covariance matrix \farg{L}} \fitemtwolines{real}{multi_normal_cholesky_lpdf}{row_vectors \farg{y} | row_vectors   \farg{mu}}{matrix \farg{L}}{The log of the multivariate normal density of row vector(s)   \farg{y} given location row vector(s) \farg{mu} and lower-triangular Cholesky   factor of the covariance matrix \farg{L}} \end{description}


\begin{description} \fitem{vector}{multi_normal_cholesky_rng}{vector \farg{mu}, matrix   \farg{L}}{Generate a multivariate normal variate with location   \farg{mu} and lower-triangular Cholesky factor of the covariance   matrix \farg{L}; may only be used in generated quantities block} \fitem{vector}{multi_normal_cholesky_rng}{row_vector \farg{mu}, matrix   \farg{L}}{Generate a multivariate normal variate with location   \farg{mu} and lower-triangular Cholesky factor of the covariance   matrix \farg{L}; may only be used in generated quantities block} \fitem{vectors}{multi_normal_cholesky_rng}{vectors \farg{mu}, matrix   \farg{L}}{Generate an array of multivariate normal variates with locations   \farg{mu} and lower-triangular Cholesky factor of the covariance   matrix \farg{L}; may only be used in generated quantities block} \fitem{vectors}{multi_normal_cholesky_rng}{row_vectors \farg{mu}, matrix   \farg{L}}{Generate an array of multivariate normal variates with locations   \farg{mu} and lower-triangular Cholesky factor of the covariance   matrix \farg{L}; may only be used in generated quantities block} \end{description}


\section{Multivariate Gaussian Process Distribution}


\subsection{Probability Density Function}


If $K,N \in \mathbb{N}$, $\Sigma \in \mathbb{R}^{N \times N}$ is symmetric, positive definite kernel matrix and $w \in \mathbb{R}^{K}$ is a vector of positive inverse scales, then for $y \in \mathbb{R}^{K \times N}$, \[ \text{MultiGP}(y|\Sigma,w) = \prod_{i=1}^{K} \text{MultiNormal}(y_i|0,w_i^{-1} \Sigma), \] where $y_i$ is the $i$th row of $y$.  This is used to efficiently handle Gaussian Processes with multi-variate outputs where only the output dimensions share a kernel function but vary based on their scale.  Note that this function does not take into account the mean prediction.


\pitem{y}{multi_gp}{Sigma, w}


\subsection{Stan Functions}


\begin{description} \fitem{real}{multi_gp_lpdf}{matrix \farg{y} | matrix \farg{Sigma},   vector \farg{w}}{The log of the multivariate GP density of matrix   \farg{y} given kernel matrix \farg{Sigma} and inverses scales   \farg{w}} \end{description}


\section{Multivariate Gaussian Process Distribution, Cholesky parameterization}


\subsection{Probability Density Function}


If $K,N \in \mathbb{N}$, $L \in \mathbb{R}^{N \times N}$ is lower triangular and such that $LL^{\top}$ is positive definite kernel matrix (implying $L_{n,n} > 0$ for $n \in 1{:}N$), and $w \in \mathbb{R}^{K}$ is a vector of positive inverse scales, then for $y \in \mathbb{R}^{K \times N}$, \[ \text{MultiGPCholesky}(y \, | \ L,w) = \prod_{i=1}^{K} \text{MultiNormal}(y_i|0,w_i^{-1} LL^{\top}), \] where $y_i$ is the $i$th row of $y$.  This is used to efficiently handle Gaussian Processes with multi-variate outputs where only the output dimensions share a kernel function but vary based on their scale.  If the model allows parameterization in terms of Cholesky factor of the kernel matrix, this distribution is also more efficient than $\text{MultiGP}()$. Note that this function does not take into account the mean prediction.


\pitem{y}{multi_gp_cholesky}{L, w}


\subsection{Stan Functions}


\begin{description} \fitem{real}{multi_gp_cholesky_lpdf}{matrix \farg{y} | matrix \farg{L},   vector \farg{w}}{The log of the multivariate GP density of matrix   \farg{y} given lower-triangular Cholesky factor of the kernel matrix \farg{L} and inverses scales   \farg{w}} \end{description}


\section{Multivariate Student-T Distribution}


\subsection{Probability Density Function}


If $K \in \mathbb{N}$, $\nu \in \mathbb{R}^+$, $\mu \in \mathbb{R}^K$, and $\Sigma \in \mathbb{R}^{K \times K}$ is symmetric and positive definite, then for $y \in \mathbb{R}^K$, \[ \begin{array}{l} \text{MultiStudentT}(y\,|\,\nu,\,\mu,\,\Sigma) \\  = \frac{1}{\pi^{K/2}} \ \frac{1}{\nu^{K/2}} \ \frac{\Gamma\!\left((\nu + K)/2\right)}      {\Gamma(\nu/2)} \ \frac{1}{\sqrt{\left| \Sigma \right|}} \ \left( 1 + \frac{1}{\nu} \, \left(y - \mu\right)^{\top} \, \Sigma^{-1} \, \left(y - \mu\right) \right)^{-(\nu + K)/2} \! . \end{array} \]


\pitem{y}{multi_student_t}{nu, mu, Sigma}


\subsection{Stan Functions}


\begin{description} \fitemtwolines{real}{multi_student_t_lpdf}{vectors \farg{y} | real \farg{nu},   vectors \farg{mu}}{matrix \farg{Sigma}}{The log of the multivariate Student-$t$   density of vector(s) \farg{y} given degrees of freedom \farg{nu},   location vector(s) \farg{mu}, and scale matrix \farg{Sigma}} \fitemtwolines{real}{multi_student_t_lpdf}{vectors \farg{y} | real \farg{nu},   row_vectors \farg{mu}}{matrix \farg{Sigma}}{The log of the multivariate Student-$t$   density of vector(s) \farg{y} given degrees of freedom \farg{nu},   location row vector(s) \farg{mu}, and scale matrix \farg{Sigma}} \fitemtwolines{real}{multi_student_t_lpdf}{row_vectors \farg{y} | real \farg{nu},   vectors \farg{mu}}{matrix \farg{Sigma}}{The log of the multivariate Student-$t$   density of row vector(s) \farg{y} given degrees of freedom \farg{nu},   location vector(s) \farg{mu}, and scale matrix \farg{Sigma}} \fitemtwolines{real}{multi_student_t_lpdf}{row_vectors \farg{y} | real \farg{nu},   row_vectors \farg{mu}}{matrix \farg{Sigma}}{The log of the multivariate Student-$t$   density of row vector(s) \farg{y} given degrees of freedom \farg{nu},   location row vector(s) \farg{mu}, and scale matrix \farg{Sigma}} \end{description}


\begin{description} \fitem{vector}{multi_student_t_rng}{real \farg{nu}, vector \farg{mu}, matrix   \farg{Sigma}}{Generate a multivariate Student-$t$ variate with degrees   of freedom \farg{nu}, location \farg{mu}, and scale matrix \farg{Sigma}; may only be used in generated quantities block} \fitem{vector}{multi_student_t_rng}{real \farg{nu}, row_vector \farg{mu}, matrix   \farg{Sigma}}{Generate a multivariate Student-$t$ variate with degrees   of freedom \farg{nu}, location \farg{mu}, and scale matrix \farg{Sigma}; may only be used in generated quantities block} \fitem{vectors}{multi_student_t_rng}{real \farg{nu}, vectors \farg{mu}, matrix   \farg{Sigma}}{Generate an array of multivariate Student-$t$ variates with degrees   of freedom \farg{nu}, locations \farg{mu}, and scale matrix \farg{Sigma}; may only be used in generated quantities block} \fitem{vectors}{multi_student_t_rng}{real \farg{nu}, row_vectors \farg{mu}, matrix   \farg{Sigma}}{Generate an array of multivariate Student-$t$ variates with degrees   of freedom \farg{nu}, locations \farg{mu}, and scale matrix \farg{Sigma}; may only be used in generated quantities block} \end{description}


\section{Gaussian Dynamic Linear Models}


A Gaussian Dynamic Linear model is defined as follows, For $t \in 1, \dots, T$, \[   \begin{aligned}[t]     y_{t} &\sim N(F' \theta_{t}, V) \\     \theta_{t} &\sim N(G \theta_{t - 1}, W) \\     \theta_{0} &\sim N(m_{0}, C_{0})   \end{aligned} \] where $y$ is $n \times T$ matrix where rows are variables and columns are observations. These functions calculate the log-likelihood of the observations marginalizing over the latent states ($p(y | F, G, V, W, m_{0}, C_{0})$). This log-likelihood is a system that is calculated using the Kalman Filter. If $V$ is diagonal, then a more efficient algorithm which sequentially processes observations and avoids a matrix inversions can be used [@DurbinKoopman:2001 section 6.4].


\pitem{y}{gaussian_dlm_obs}{F, G, V, W, m0, C0}


\subsection{Stan Functions}


The following two functions differ in the type of their \farg{V}, the first taking a full observation covariance matrix \farg{V}\ and the second a vector \farg{V}\ representing the diagonal of the observation covariance matrix.  The sampling statement defined in the previous section works with either type of observation \farg{V}.


\begin{description} \fitemtwolines{real}{gaussian_dlm_obs_lpdf}{matrix \farg{y} | matrix \farg{F},   matrix \farg{G}, matrix \farg{V}}{matrix \farg{W}, vector   \farg{m0}, matrix \farg{C0}}{The log of the density of the   Gaussian Dynamic Linear model with observation matrix \farg{y} in   which rows are variables and columns are observations, design   matrix \farg{F}, transition matrix \farg{G}, observation   covariance matrix \farg{V}, system covariance matrix \farg{W}, and   the initial state is distributed normal with mean \farg{m0} and   covariance \farg{C0}.} \fitemtwolines{real}{gaussian_dlm_obs_lpdf}{matrix \farg{y} | matrix \farg{F},   matrix \farg{G}, vector \farg{V}}{matrix \farg{W}, vector \farg{m0},   matrix \farg{C0}} {The log of the density of the Gaussian Dynamic   Linear model with observation matrix \farg{y} in which rows are   variables and columns are observations, design matrix \farg{F},   transition matrix \farg{G}, observation covariance matrix with   diagonal \farg{V}, system covariance matrix \farg{W}, and the   initial state is distributed normal with mean \farg{m0} and   covariance \farg{C0}.} \end{description}


\chapter{Simplex Distributions}


The simplex probabilities have support on the unit $K$-simplex for a specified $K$.  A $K$-dimensional vector $\theta$ is a unit $K$-simplex if $\theta_k \geq 0$ for $k \in \{1,\ldots,K\}$ and $\sum_{k = 1}^K \theta_k = 1$.


\section{Dirichlet Distribution}


\subsection{Probability Density Function}


If $K \in \mathbb{N}$ and $\alpha \in (\mathbb{R}^+)^{K}$, then for $\theta \in \text{$K$-simplex}$, \[ \text{Dirichlet}(\theta|\alpha) = \frac{\Gamma \! \left( \sum_{k=1}^K \alpha_k \right)}      {\prod_{k=1}^K \Gamma(\alpha_k)} \ \prod_{k=1}^K \theta_k^{\alpha_k - 1} . \] 

 _**Warning:**_  If any of the components of $\theta$ satisfies $\theta_i = 0$ or $\theta_i = 1$, then the probability is 0 and the log probability is $-\infty$.  Similarly, the distribution requires strictly positive parameters, with $\alpha_i > 0$ for each $i$.


\subsection{Meaning of Dirichlet Parameters}


A symmetric Dirichlet prior is $[\alpha, \ldots, \alpha]^{\top}$.  To code this in Stan,


```\n data {\n   int<lower = 1> K;\n   real<lower = 0> alpha;\n }\n generated quantities {\n   vector[K] theta = dirichlet_rng(rep_vector(alpha, K));\n }\n ```


Taking $K = 10$, here are the first five draws for $\alpha = 0.001$. For $\alpha = 1$, the distribution is uniform over simplexes.


```\n 1) 0.17 0.05 0.07 0.17 0.03 0.13 0.03 0.03 0.27 0.05\n 2) 0.08 0.02 0.12 0.07 0.52 0.01 0.07 0.04 0.01 0.06\n 3) 0.02 0.03 0.22 0.29 0.17 0.10 0.09 0.00 0.05 0.03\n 4) 0.04 0.03 0.21 0.13 0.04 0.01 0.10 0.04 0.22 0.18\n 5) 0.11 0.22 0.02 0.01 0.06 0.18 0.33 0.04 0.01 0.01\n ```


That does not mean it's uniform over the marginal probabilities of each element.  As the size of the simplex grows, the marginal draws become more and more concentrated below (not around) $1/K$.  When one component of the simplex is large, the others must all be relatively small to compensate.  For example, in a uniform distribution on $10$-simplexes, the probability that a component is greater than the mean of $1/10$ is only 39\%.  Most of the posterior marginal probability mass for each component is in the interval $(0, 0.1)$.


When the $\alpha$ value is small, the draws gravitate to the corners of the simplex.  Here are the first five draws for $\alpha = 0.001$.


```\n 1) 3e-203 0e+00 2e-298 9e-106 1e+000 0e+00 0e+000 1e-047 0e+00 4e-279\n 2) 1e+000 0e+00 5e-279 2e-014 1e-275 0e+00 3e-285 9e-147 0e+00 0e+000\n 3) 1e-308 0e+00 1e-213 0e+000 0e+000 8e-75 0e+000 1e+000 4e-58 7e-112\n 4) 6e-166 5e-65 3e-068 3e-147 0e+000 1e+00 3e-249 0e+000 0e+00 0e+000\n 5) 2e-091 0e+00 0e+000 0e+000 1e-060 0e+00 4e-312 1e+000 0e+00 0e+000\n ```


Each row denotes a draw.  Each draw has a single value that rounds to one and other values that are very close to zero or rounded down to zero.


As $\alpha$ increases, the draws become increasingly uniform.  For $\alpha = 1000$,


```\n 1) 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10\n 2) 0.10 0.10 0.09 0.10 0.10 0.10 0.11 0.10 0.10 0.10\n 3) 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10\n 4) 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10\n 5) 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10 0.10\n ```


\pitem{theta}{dirichlet}{alpha}


\subsection{Stan Functions}


\begin{description}   \fitem{real}{dirichlet_lpdf}{vector \farg{theta} | vector     \farg{alpha}}{ The log of the Dirichlet density for simplex     \farg{theta} given prior counts (plus one) \farg{alpha}} \end{description}


\begin{description} \fitem{vector}{dirichlet_rng}{vector \farg{alpha}}{Generate a   Dirichlet variate with prior counts (plus one) \farg{alpha}; may only be used in generated quantities block} \end{description}


\chapter{Correlation Matrix Distributions}


The correlation matrix distributions have support on the (Cholesky factors of) correlation matrices.  A Cholesky factor $L$ for a $K \times K$ correlation matrix $\Sigma$ of dimension $K$ has rows of unit length so that the diagonal of $L L^{\top}$ is the unit $K$-vector. Even though models are usually conceptualized in terms of correlation matrices, it is better to operationalize them in terms of their Cholesky factors. If you are interested in the posterior distribution of the correlations, you can recover them in the generated quantities block via


```\n generated quantities {\n   corr_matrix[K] Sigma;\n   Sigma = multiply_lower_tri_self_transpose(L);\n }\n ```


\section{LKJ Correlation Distribution}\label{lkj-correlation.section}


\subsection{Probability Density Function}


For $\eta > 0$, if $\Sigma$ a positive-definite, symmetric matrix with unit diagonal (i.e., a correlation matrix), then \[ \text{LkjCorr}(\Sigma|\eta) \propto \det \left( \Sigma \right)^{(\eta - 1)}. \] The expectation is the identity matrix for any positive value of the shape parameter $\eta$, which can be interpreted like the shape parameter of a symmetric beta distribution:


*   if $\eta = 1$, then the density is uniform over correlation   matrices of order $K$; 
*   if $\eta > 1$, the identity matrix is the modal correlation   matrix, with a sharper peak in the density at the identity matrix   for larger $\eta$; and 
*   for $0 < \eta < 1$, the density has a trough at the identity   matrix. 
*   if $\eta$ were an unknown parameter, the Jeffreys prior is   proportional to $\sqrt{2\sum_{k=1}^{K-1}\left(   \psi_1\left(\eta+\frac{K-k-1}{2}\right) -   2\psi_1\left(2\eta+K-k-1 \right)\right)}$, where $\psi_1()$ is the   trigamma function


See [@LewandowskiKurowickaJoe:2009] for definitions. However, it is much better computationally to work directly with the Cholesky factor of $\Sigma$, so this distribution should never be explicitly used in practice.


\pitem{y}{lkj_corr}{eta}


\subsection{Stan Functions}


\begin{description}   \fitem{real}{lkj_corr_lpdf}{matrix \farg{y} | real     \farg{eta}}{The log of the LKJ density for the correlation matrix     \farg{y} given nonnegative shape \farg{eta}. The only reason to     use this density function is if you want the code to run slower     and consume more memory with more risk of numerical errors.     Use its Cholesky factor as described in the next section.} \end{description}


\begin{description} \fitem{matrix}{lkj_corr_rng}{int \farg{K}, real \farg{eta}}{Generate a   LKJ random correlation matrix of order \farg{K} with shape \farg{eta};   may only be used in generated quantities block} \end{description}


\section{Cholesky LKJ Correlation Distribution}


Stan provides an implicit parameterization of the LKJ correlation matrix density in terms of its Cholesky factor, which you should use rather than the explicit parameterization in the previous section. For example, if \code{L} is a Cholesky factor of a correlation matrix, then


```\n L ~ lkj_corr_cholesky(2.0); # implies L * L' ~ lkj_corr(2.0); \n```


Because Stan requires models to have support on all valid constrained parameters, \code{L} will almost always[^fnlkj] be a parameter declared with the type of a Cholesky factor for a correlation matrix; for example,

[^fnlkj]: It is possible to build up a valid \code{L} within Stan, but that would then require Jacobian adjustments to imply the intended posterior.

```\n parameters {   cholesky_factor_corr[K] L;   # rather than corr_matrix[K] Sigma;   // ... \n```



\subsection{Probability Density Function}


For $\eta > 0$, if $L$ is a $K \times K$ lower-triangular Cholesky factor of a symmetric positive-definite matrix with unit diagonal (i.e., a correlation matrix), then \[ \text{LkjCholesky}(L|\eta) \propto \left|J\right|\det(L L^\top)^{(\eta - 1)} = \prod_{k=2}^K L_{kk}^{K-k+2\eta-2}. \] See the previous section for details on interpreting the shape parameter $\eta$. Note that even if $\eta=1$, it is still essential to evaluate the density function because the density of $L$ is not constant, regardless of the value of $\eta$, even though the density of $LL^\top$ is constant iff $\eta=1$.


A lower triangular $L$ is a Cholesky factor for a correlation matrix if and only if $L_{k,k} > 0$ for $k \in 1{:}K$ and each row $L_k$ has unit Euclidean length.


\pitem{L}{lkj_corr_cholesky}{eta}


\subsection{Stan Functions}


\begin{description}   \fitem{real}{lkj_corr_cholesky_lpdf}{matrix \farg{L} | real     \farg{eta}}{The log of the LKJ density for the lower-triangular     Cholesky factor \farg{L} of a correlation matrix given shape     \farg{eta}.} \end{description}


\begin{description} \fitem{matrix}{lkj_corr_cholesky_rng}{int \farg{K}, real \farg{eta}}{   Generate a random Cholesky factor of a correlation matrix of order   \farg{K} that is distributed LKJ with shape \farg{eta}; may only be   used in generated quantities block} \end{description}


\chapter{Covariance Matrix Distributions}


The covariance matrix distributions have support on symmetric, positive-definite $K \times K$ matrices.


\section{Wishart Distribution}


\subsection{Probability Density Function}


If $K \in \mathbb{N}$, $\nu \in (K-1,\infty)$, and $S \in \mathbb{R}^{K \times K}$ is symmetric and positive definite, then for symmetric and positive-definite $W \in \mathbb{R}^{K \times K}$, \[ \text{Wishart}(W|\nu,S) = \frac{1}{2^{\nu K / 2}} \ \frac{1}{\Gamma_K \! \left( \frac{\nu}{2} \right)} \ \left| S \right|^{-\nu/2} \ \left| W \right|^{(\nu - K - 1)/2} \ \exp \! \left(- \frac{1}{2} \ \text{tr}\left( S^{-1} W \right) \right) \! , \] where $\text{tr}()$ is the matrix trace function, and $\Gamma_K()$ is the multivariate Gamma function, \[ \Gamma_K(x) = \frac{1}{\pi^{K(K-1)/4}} \ \prod_{k=1}^K \Gamma \left( x + \frac{1 - k}{2} \right) \!. \] 

\pitem{W}{wishart}{nu, Sigma}


\subsection{Stan Functions}


\begin{description} \fitem{real}{wishart_lpdf}{matrix \farg{W} | real \farg{nu}, matrix  \farg{Sigma}}{The log of the Wishart density for symmetric and positive-definite matrix  \farg{W} given degrees of freedom \farg{nu} and symmetric and positive-definite scale matrix  \farg{Sigma}} \end{description}


\begin{description} \fitem{matrix}{wishart_rng}{real \farg{nu}, matrix \farg{Sigma}}{Generate a   Wishart variate with degrees of freedom \farg{nu} and symmetric and positive-definite scale matrix  \farg{Sigma}; may only be used in generated quantities block} \end{description}


\section{Inverse Wishart Distribution}


\subsection{Probability Density Function}


If $K \in \mathbb{N}$, $\nu \in (K-1,\infty)$, and $S \in \mathbb{R}^{K \times   K}$ is symmetric and positive definite, then for symmetric and positive-definite $W \in \mathbb{R}^{K \times K}$, \[ \text{InvWishart}(W|\nu,S) = \frac{1}{2^{\nu K / 2}} \ \frac{1}{\Gamma_K \! \left( \frac{\nu}{2} \right)} \ \left| S \right|^{\nu/2} \ \left| W \right|^{-(\nu + K + 1)/2} \ \exp \! \left( - \frac{1}{2} \ \text{tr}(SW^{-1}) \right) \! . \] 

\pitem{W}{inv_wishart}{nu, Sigma}


\subsection{Stan Functions}


\begin{description} \fitem{real}{inv_wishart_lpdf}{matrix \farg{W} | real \farg{nu}, matrix  \farg{Sigma}}{The log of the inverse Wishart density for symmetric and positive-definite matrix  \farg{W} given degrees of freedom \farg{nu} and symmetric and positive-definite scale matrix  \farg{Sigma}} \end{description}


\begin{description} \fitem{matrix}{inv_wishart_rng}{real \farg{nu}, matrix   \farg{Sigma}}{Generate an inverse Wishart variate with degrees of freedom \farg{nu} and symmetric and positive-definite scale matrix  \farg{Sigma}; may only be used in generated quantities block} \end{description}


\part{Appendix}


\chapter{Mathematical Functions}\label{math-functions.appendix}


This appendix provides the definition of several mathematical functions used throughout the manual.


\section{Beta}\label{beta-appendix.section}


The beta function, $\text{B}(\alpha,\beta)$, computes the normalizing constant for the beta distribution, and is defined for $a > 0$ and $b > 0$ by \[ \text{B}(a,b) \ = \ \int_0^1 u^{a - 1} (1 - u)^{b - 1} \, du \ = \ \frac{\Gamma(a) \, \Gamma(b)}{\Gamma(a+b)} \, . \] 

\section{Incomplete Beta}\label{inc-beta-appendix.section}


The incomplete beta function, $\text{B}(x; a, b)$, is defined for $x \in [0, 1]$ and $a, b \geq 0$ such that $a + b \neq 0$ by \[ \text{B}(x; \, a, b) \ = \ \int_0^x u^{a -  1} \, (1 - u)^{b - 1} \, du, `<\] where $\text{B}(a, b)$ is the beta function defined in \@ref(beta-appendix).  If $x = 1$, the incomplete beta function reduces to the beta function, $\text{B}(1; a, b) = \text{B}(a, b)$.


The regularized incomplete beta function divides the incomplete beta function by the beta function, \[ I_x(a, b) \ = \ \frac{\text{B}(x; \, a, b)}{B(a, b)} \, . \] 

\section{Gamma}\label{gamma-appendix.section}


The gamma function, $\Gamma(x)$, is the generalization of the factorial function to continuous variables, defined so that for positive integers $n$, \[ \Gamma(n+1) = n! \] Generalizing to all positive numbers and non-integer negative numbers, \[ \Gamma(x) = \int_0^{\infty} u^{x - 1} \exp(-u) \,n du. \] 

\section{Digamma}\label{digamma-appendix.section}


The digamma function $\Psi$ is the derivative of the $\log \Gamma$ function, \[ \Psi(u) \ = \ \frac{d}{d u} \log \Gamma(u) \ = \ \frac{1}{\Gamma(u)} \ \frac{d}{d u} \Gamma(u). \]
